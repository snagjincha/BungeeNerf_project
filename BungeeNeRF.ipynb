{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"CfwsQHxObkoZ"},"outputs":[],"source":["!git clone https://github.com/city-super/BungeeNeRF.git"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":70149,"status":"ok","timestamp":1731778026625,"user":{"displayName":"Jaeho Jung","userId":"09785872886245681410"},"user_tz":-540},"id":"wFlpntj2aQ1I","outputId":"e038cb5b-845d-4a70-85ae-9fffd91491f2"},"outputs":[{"name":"stdout","output_type":"stream","text":["/bin/bash: line 1: conda: command not found\n","/bin/bash: line 1: conda: command not found\n","/bin/bash: line 1: conda: command not found\n","Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (24.1.2)\n","Collecting pip\n","  Downloading pip-24.3.1-py3-none-any.whl.metadata (3.7 kB)\n","Downloading pip-24.3.1-py3-none-any.whl (1.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m61.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: pip\n","  Attempting uninstall: pip\n","    Found existing installation: pip 24.1.2\n","    Uninstalling pip-24.1.2:\n","      Successfully uninstalled pip-24.1.2\n","Successfully installed pip-24.3.1\n","/content/drive/MyDrive/JBNU/2024-2/FuturisticCar/BungeeNeRF\n","Collecting torch<=1.11.0 (from -r requirements.txt (line 1))\n","  Downloading torch-1.11.0-cp310-cp310-manylinux1_x86_64.whl.metadata (24 kB)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 2)) (0.20.1+cu121)\n","Requirement already satisfied: imageio in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 3)) (2.36.0)\n","Requirement already satisfied: imageio-ffmpeg in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 4)) (0.5.1)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 5)) (3.8.0)\n","Collecting configargparse (from -r requirements.txt (line 6))\n","  Downloading ConfigArgParse-1.7-py3-none-any.whl.metadata (23 kB)\n","Requirement already satisfied: tensorboard>=2.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 7)) (2.17.1)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 8)) (4.66.6)\n","Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 9)) (4.10.0.84)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch<=1.11.0->-r requirements.txt (line 1)) (4.12.2)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->-r requirements.txt (line 2)) (1.26.4)\n","INFO: pip is looking at multiple versions of torchvision to determine which version is compatible with other requirements. This could take a while.\n","Collecting torchvision (from -r requirements.txt (line 2))\n","  Downloading torchvision-0.20.1-cp310-cp310-manylinux1_x86_64.whl.metadata (6.1 kB)\n","  Downloading torchvision-0.20.0-cp310-cp310-manylinux1_x86_64.whl.metadata (6.1 kB)\n","  Downloading torchvision-0.19.1-cp310-cp310-manylinux1_x86_64.whl.metadata (6.0 kB)\n","  Downloading torchvision-0.19.0-cp310-cp310-manylinux1_x86_64.whl.metadata (6.0 kB)\n","  Downloading torchvision-0.18.1-cp310-cp310-manylinux1_x86_64.whl.metadata (6.6 kB)\n","  Downloading torchvision-0.18.0-cp310-cp310-manylinux1_x86_64.whl.metadata (6.6 kB)\n","  Downloading torchvision-0.17.2-cp310-cp310-manylinux1_x86_64.whl.metadata (6.6 kB)\n","INFO: pip is still looking at multiple versions of torchvision to determine which version is compatible with other requirements. This could take a while.\n","  Downloading torchvision-0.17.1-cp310-cp310-manylinux1_x86_64.whl.metadata (6.6 kB)\n","  Downloading torchvision-0.17.0-cp310-cp310-manylinux1_x86_64.whl.metadata (6.6 kB)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision->-r requirements.txt (line 2)) (2.32.3)\n","  Downloading torchvision-0.16.2-cp310-cp310-manylinux1_x86_64.whl.metadata (6.6 kB)\n","  Downloading torchvision-0.16.1-cp310-cp310-manylinux1_x86_64.whl.metadata (6.6 kB)\n","  Downloading torchvision-0.16.0-cp310-cp310-manylinux1_x86_64.whl.metadata (6.6 kB)\n","INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n","  Downloading torchvision-0.15.2-cp310-cp310-manylinux1_x86_64.whl.metadata (11 kB)\n","  Downloading torchvision-0.15.1-cp310-cp310-manylinux1_x86_64.whl.metadata (11 kB)\n","  Downloading torchvision-0.14.1-cp310-cp310-manylinux1_x86_64.whl.metadata (11 kB)\n","  Downloading torchvision-0.14.0-cp310-cp310-manylinux1_x86_64.whl.metadata (11 kB)\n","  Downloading torchvision-0.13.1-cp310-cp310-manylinux1_x86_64.whl.metadata (10 kB)\n","  Downloading torchvision-0.13.0-cp310-cp310-manylinux1_x86_64.whl.metadata (10 kB)\n","  Downloading torchvision-0.12.0-cp310-cp310-manylinux1_x86_64.whl.metadata (10 kB)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->-r requirements.txt (line 2)) (11.0.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from imageio-ffmpeg->-r requirements.txt (line 4)) (75.1.0)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 5)) (1.3.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 5)) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 5)) (4.54.1)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 5)) (1.4.7)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 5)) (24.2)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 5)) (3.2.0)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 5)) (2.8.2)\n","Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.0->-r requirements.txt (line 7)) (1.4.0)\n","Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.0->-r requirements.txt (line 7)) (1.67.1)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.0->-r requirements.txt (line 7)) (3.7)\n","Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.0->-r requirements.txt (line 7)) (4.25.5)\n","Requirement already satisfied: six>1.9 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.0->-r requirements.txt (line 7)) (1.16.0)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.0->-r requirements.txt (line 7)) (0.7.2)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.0->-r requirements.txt (line 7)) (3.1.3)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard>=2.0->-r requirements.txt (line 7)) (3.0.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->-r requirements.txt (line 2)) (3.4.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->-r requirements.txt (line 2)) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->-r requirements.txt (line 2)) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->-r requirements.txt (line 2)) (2024.8.30)\n","Downloading torch-1.11.0-cp310-cp310-manylinux1_x86_64.whl (750.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m750.6/750.6 MB\u001b[0m \u001b[31m31.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading torchvision-0.12.0-cp310-cp310-manylinux1_x86_64.whl (21.0 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m66.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading ConfigArgParse-1.7-py3-none-any.whl (25 kB)\n","Installing collected packages: torch, configargparse, torchvision\n","  Attempting uninstall: torch\n","    Found existing installation: torch 2.5.1+cu121\n","    Uninstalling torch-2.5.1+cu121:\n","      Successfully uninstalled torch-2.5.1+cu121\n","  Attempting uninstall: torchvision\n","    Found existing installation: torchvision 0.20.1+cu121\n","    Uninstalling torchvision-0.20.1+cu121:\n","      Successfully uninstalled torchvision-0.20.1+cu121\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","peft 0.13.2 requires torch>=1.13.0, but you have torch 1.11.0 which is incompatible.\n","torchaudio 2.5.1+cu121 requires torch==2.5.1, but you have torch 1.11.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed configargparse-1.7 torch-1.11.0 torchvision-0.12.0\n"]}],"source":["!conda create --name bungee python=3.7; conda activate bungee\n","!conda install pip; pip install --upgrade pip\n","%cd BungeeNeRF\n","!pip install -r requirements.txt"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":290,"status":"ok","timestamp":1731774607027,"user":{"displayName":"Jaeho Jung","userId":"09785872886245681410"},"user_tz":-540},"id":"yTiz2UXDz14F","outputId":"4ad0cbdc-9372-4e25-b336-e29fdb0e2691"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[0m\u001b[01;34mconfigs\u001b[0m/  GES2pose.py  LICENSE             \u001b[01;34m__pycache__\u001b[0m/  requirements.txt  run_nerf_helpers.py\n","\u001b[01;34mdata\u001b[0m/     \u001b[01;34mimgs\u001b[0m/        load_multiscale.py  README.md     run_bungee.py\n"]}],"source":["%ls"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"degDs7mtaX9j","outputId":"0ee09552-1178-4bdb-c48f-16fcca323f9c"},"outputs":[{"name":"stdout","output_type":"stream","text":["Loaded image data shape: (463, 360, 640, 3)  hwf: [360.        640.        494.5459355]\n","Auto holdout, 16\n","Bungee_NeRF_block(\n","  (baseblock): Bungee_NeRF_baseblock(\n","    (pts_linears): ModuleList(\n","      (0): Linear(in_features=63, out_features=256, bias=True)\n","      (1): Linear(in_features=256, out_features=256, bias=True)\n","      (2): Linear(in_features=256, out_features=256, bias=True)\n","      (3): Linear(in_features=256, out_features=256, bias=True)\n","    )\n","    (views_linear): Linear(in_features=283, out_features=128, bias=True)\n","    (feature_linear): Linear(in_features=256, out_features=256, bias=True)\n","    (alpha_linear): Linear(in_features=256, out_features=1, bias=True)\n","    (rgb_linear): Linear(in_features=128, out_features=3, bias=True)\n","  )\n","  (resblocks): ModuleList()\n",")\n","Found ckpts []\n","shuffle rays\n","done\n","Begin\n","TRAIN views are [  1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  17  18  19\n","  20  21  22  23  24  25  26  27  28  29  30  31  33  34  35  36  37  38\n","  39  40  41  42  43  44  45  46  47  49  50  51  52  53  54  55  56  57\n","  58  59  60  61  62  63  65  66  67  68  69  70  71  72  73  74  75  76\n","  77  78  79  81  82  83  84  85  86  87  88  89  90  91  92  93  94  95\n","  97  98  99 100 101 102 103 104 105 106 107 108 109 110 111 113 114 115\n"," 116 117 118 119 120 121 122 123 124 125 126 127 129 130 131 132 133 134\n"," 135 136 137 138 139 140 141 142 143 145 146 147 148 149 150 151 152 153\n"," 154 155 156 157 158 159 161 162 163 164 165 166 167 168 169 170 171 172\n"," 173 174 175 177 178 179 180 181 182 183 184 185 186 187]\n","TEST views are [  0  16  32  48  64  80  96 112 128 144 160 176]\n","2024-11-16 17:33:27.299817: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-11-16 17:33:27.316075: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-11-16 17:33:27.320787: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2024-11-16 17:33:27.332909: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2024-11-16 17:33:28.288779: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","[TRAIN] Iter: 100 Loss: 0.03989123915214701  PSNR: 17.002378463745117\n","[TRAIN] Iter: 200 Loss: 0.037548407594852215  PSNR: 17.267929077148438\n","[TRAIN] Iter: 300 Loss: 0.03697134824658694  PSNR: 17.33422088623047\n","[TRAIN] Iter: 400 Loss: 0.03430244297745495  PSNR: 17.655719757080078\n","[TRAIN] Iter: 500 Loss: 0.03552536013108887  PSNR: 17.503734588623047\n","[TRAIN] Iter: 600 Loss: 0.03571886208418359  PSNR: 17.476181030273438\n","[TRAIN] Iter: 700 Loss: 0.03576775032756084  PSNR: 17.47651481628418\n","[TRAIN] Iter: 800 Loss: 0.035076350635583925  PSNR: 17.563982009887695\n","[TRAIN] Iter: 900 Loss: 0.033690054366496816  PSNR: 17.73970603942871\n","[TRAIN] Iter: 1000 Loss: 0.03229034179284174  PSNR: 17.91532325744629\n","[TRAIN] Iter: 1100 Loss: 0.03029241932217271  PSNR: 18.18968963623047\n","[TRAIN] Iter: 1200 Loss: 0.03349415192612027  PSNR: 17.76188850402832\n","[TRAIN] Iter: 1300 Loss: 0.03008268268091987  PSNR: 18.22960090637207\n","[TRAIN] Iter: 1400 Loss: 0.029139766840983024  PSNR: 18.353878021240234\n","[TRAIN] Iter: 1500 Loss: 0.027780070027814248  PSNR: 18.56752586364746\n","[TRAIN] Iter: 1600 Loss: 0.029497575107108792  PSNR: 18.32915496826172\n","[TRAIN] Iter: 1700 Loss: 0.02930831381182607  PSNR: 18.3558292388916\n","[TRAIN] Iter: 1800 Loss: 0.027523251602817836  PSNR: 18.626041412353516\n","[TRAIN] Iter: 1900 Loss: 0.028955434779097865  PSNR: 18.407241821289062\n","[TRAIN] Iter: 2000 Loss: 0.02717499273710732  PSNR: 18.677120208740234\n","[TRAIN] Iter: 2100 Loss: 0.028535656834945418  PSNR: 18.464458465576172\n","[TRAIN] Iter: 2200 Loss: 0.026776829551752843  PSNR: 18.724361419677734\n","[TRAIN] Iter: 2300 Loss: 0.027091247550202254  PSNR: 18.692455291748047\n","[TRAIN] Iter: 2400 Loss: 0.026146489648343214  PSNR: 18.859786987304688\n","[TRAIN] Iter: 2500 Loss: 0.027586534416595906  PSNR: 18.631193161010742\n","[TRAIN] Iter: 2600 Loss: 0.026417212259298203  PSNR: 18.807085037231445\n","[TRAIN] Iter: 2700 Loss: 0.025965002379273384  PSNR: 18.877628326416016\n","[TRAIN] Iter: 2800 Loss: 0.02513853854000902  PSNR: 19.0285701751709\n","[TRAIN] Iter: 2900 Loss: 0.026039026564298535  PSNR: 18.875829696655273\n","[TRAIN] Iter: 3000 Loss: 0.026089330766482385  PSNR: 18.8746337890625\n","[TRAIN] Iter: 3100 Loss: 0.02502487370793975  PSNR: 19.055248260498047\n","[TRAIN] Iter: 3200 Loss: 0.02458803923162145  PSNR: 19.132095336914062\n","[TRAIN] Iter: 3300 Loss: 0.024502068116201838  PSNR: 19.134349822998047\n","[TRAIN] Iter: 3400 Loss: 0.024912780276583613  PSNR: 19.05935287475586\n","[TRAIN] Iter: 3500 Loss: 0.02336919120593603  PSNR: 19.37228775024414\n","[TRAIN] Iter: 3600 Loss: 0.025264031380706944  PSNR: 19.036327362060547\n","[TRAIN] Iter: 3700 Loss: 0.024649222433212735  PSNR: 19.091699600219727\n","[TRAIN] Iter: 3800 Loss: 0.023109673337196702  PSNR: 19.388471603393555\n","[TRAIN] Iter: 3900 Loss: 0.023316537983305463  PSNR: 19.363035202026367\n","[TRAIN] Iter: 4000 Loss: 0.022217503705145095  PSNR: 19.577133178710938\n","[TRAIN] Iter: 4100 Loss: 0.02227468401343356  PSNR: 19.560192108154297\n","[TRAIN] Iter: 4200 Loss: 0.025016760855806237  PSNR: 19.052404403686523\n","[TRAIN] Iter: 4300 Loss: 0.023345635961947747  PSNR: 19.37871551513672\n","[TRAIN] Iter: 4400 Loss: 0.02151198805434579  PSNR: 19.748838424682617\n","[TRAIN] Iter: 4500 Loss: 0.02222139860089581  PSNR: 19.5899658203125\n","[TRAIN] Iter: 4600 Loss: 0.023044297715163895  PSNR: 19.437397003173828\n","[TRAIN] Iter: 4700 Loss: 0.02105012582301722  PSNR: 19.812747955322266\n","[TRAIN] Iter: 4800 Loss: 0.021380586479672163  PSNR: 19.732486724853516\n","[TRAIN] Iter: 4900 Loss: 0.02027555446562951  PSNR: 19.95608901977539\n","[TRAIN] Iter: 5000 Loss: 0.02384660008062555  PSNR: 19.274831771850586\n","[TRAIN] Iter: 5100 Loss: 0.02107280285301689  PSNR: 19.830108642578125\n","[TRAIN] Iter: 5200 Loss: 0.022448626585979597  PSNR: 19.56035614013672\n","[TRAIN] Iter: 5300 Loss: 0.020964223871187272  PSNR: 19.827373504638672\n","[TRAIN] Iter: 5400 Loss: 0.02246101511530442  PSNR: 19.53109359741211\n","[TRAIN] Iter: 5500 Loss: 0.02034324536660009  PSNR: 19.96369743347168\n","[TRAIN] Iter: 5600 Loss: 0.021717430741247237  PSNR: 19.674665451049805\n","[TRAIN] Iter: 5700 Loss: 0.021742657970337313  PSNR: 19.680461883544922\n","[TRAIN] Iter: 5800 Loss: 0.02131329073042452  PSNR: 19.803239822387695\n","[TRAIN] Iter: 5900 Loss: 0.0213220229446026  PSNR: 19.791093826293945\n","[TRAIN] Iter: 6000 Loss: 0.02107384504721517  PSNR: 19.77851676940918\n","[TRAIN] Iter: 6100 Loss: 0.02156230783762064  PSNR: 19.774333953857422\n","[TRAIN] Iter: 6200 Loss: 0.022490922856973853  PSNR: 19.521757125854492\n","[TRAIN] Iter: 6300 Loss: 0.020306930839376774  PSNR: 19.962570190429688\n","[TRAIN] Iter: 6400 Loss: 0.02097313040289496  PSNR: 19.834247589111328\n","[TRAIN] Iter: 6500 Loss: 0.020350526468852378  PSNR: 19.966691970825195\n","[TRAIN] Iter: 6600 Loss: 0.02156083506650916  PSNR: 19.680150985717773\n","[TRAIN] Iter: 6700 Loss: 0.02043151097745219  PSNR: 19.943452835083008\n","[TRAIN] Iter: 6800 Loss: 0.0204164602695848  PSNR: 19.96137237548828\n","[TRAIN] Iter: 6900 Loss: 0.020825410651868437  PSNR: 19.874610900878906\n","[TRAIN] Iter: 7000 Loss: 0.019999291906914955  PSNR: 20.06431007385254\n","[TRAIN] Iter: 7100 Loss: 0.021871567495930753  PSNR: 19.626304626464844\n","[TRAIN] Iter: 7200 Loss: 0.020774016137019513  PSNR: 19.892776489257812\n","[TRAIN] Iter: 7300 Loss: 0.019215034733762733  PSNR: 20.212791442871094\n","[TRAIN] Iter: 7400 Loss: 0.018821322639270326  PSNR: 20.336807250976562\n","[TRAIN] Iter: 7500 Loss: 0.01907222323110091  PSNR: 20.278766632080078\n","[TRAIN] Iter: 7600 Loss: 0.019872101879180384  PSNR: 20.08367347717285\n","[TRAIN] Iter: 7700 Loss: 0.02009654472020248  PSNR: 20.024810791015625\n","[TRAIN] Iter: 7800 Loss: 0.01926481717590238  PSNR: 20.215932846069336\n","[TRAIN] Iter: 7900 Loss: 0.01968552968443976  PSNR: 20.068204879760742\n","[TRAIN] Iter: 8000 Loss: 0.020454266826008698  PSNR: 19.951274871826172\n","[TRAIN] Iter: 8100 Loss: 0.01962429297650552  PSNR: 20.125179290771484\n","[TRAIN] Iter: 8200 Loss: 0.020053691190470735  PSNR: 20.08294677734375\n","[TRAIN] Iter: 8300 Loss: 0.020659617530349776  PSNR: 19.892885208129883\n","[TRAIN] Iter: 8400 Loss: 0.019330972417550066  PSNR: 20.257511138916016\n","[TRAIN] Iter: 8500 Loss: 0.019547498862523337  PSNR: 20.14218521118164\n","[TRAIN] Iter: 8600 Loss: 0.01881858954032507  PSNR: 20.293134689331055\n","[TRAIN] Iter: 8700 Loss: 0.01858032375808786  PSNR: 20.410545349121094\n","[TRAIN] Iter: 8800 Loss: 0.018330906695023824  PSNR: 20.48822021484375\n","[TRAIN] Iter: 8900 Loss: 0.018890802869164502  PSNR: 20.29305648803711\n","[TRAIN] Iter: 9000 Loss: 0.019753884930967656  PSNR: 20.066753387451172\n","[TRAIN] Iter: 9100 Loss: 0.018906200398065367  PSNR: 20.321548461914062\n","[TRAIN] Iter: 9200 Loss: 0.020137475147521927  PSNR: 20.05481719970703\n","[TRAIN] Iter: 9300 Loss: 0.01905343100621815  PSNR: 20.328125\n","[TRAIN] Iter: 9400 Loss: 0.019242393942005886  PSNR: 20.240516662597656\n","[TRAIN] Iter: 9500 Loss: 0.018583182035712464  PSNR: 20.41822052001953\n","[TRAIN] Iter: 9600 Loss: 0.018907036167281503  PSNR: 20.315574645996094\n","[TRAIN] Iter: 9700 Loss: 0.020076022151232054  PSNR: 20.05438804626465\n","[TRAIN] Iter: 9800 Loss: 0.019453556769417513  PSNR: 20.231508255004883\n","[TRAIN] Iter: 9900 Loss: 0.019152474140108275  PSNR: 20.24852180480957\n","  5% 9999/200000 [46:37<14:41:49,  3.59it/s]Saved checkpoints at ./logs/56leonard_test_st0/010000.tar\n","[TRAIN] Iter: 10000 Loss: 0.01775106928946464  PSNR: 20.600078582763672\n","[TRAIN] Iter: 10100 Loss: 0.018869686383216507  PSNR: 20.355243682861328\n","[TRAIN] Iter: 10200 Loss: 0.01839033205096155  PSNR: 20.451927185058594\n","[TRAIN] Iter: 10300 Loss: 0.018740879044878082  PSNR: 20.326345443725586\n","[TRAIN] Iter: 10400 Loss: 0.019243061384659102  PSNR: 20.262426376342773\n","[TRAIN] Iter: 10500 Loss: 0.018371836877210772  PSNR: 20.44019889831543\n","[TRAIN] Iter: 10600 Loss: 0.017794813710985895  PSNR: 20.591312408447266\n","[TRAIN] Iter: 10700 Loss: 0.019085631080897072  PSNR: 20.28005027770996\n","[TRAIN] Iter: 10800 Loss: 0.018416236637628956  PSNR: 20.412153244018555\n","[TRAIN] Iter: 10900 Loss: 0.019001470968981818  PSNR: 20.31231689453125\n","[TRAIN] Iter: 11000 Loss: 0.019788235940018162  PSNR: 20.121789932250977\n","[TRAIN] Iter: 11100 Loss: 0.016515922520418794  PSNR: 20.898799896240234\n","[TRAIN] Iter: 11200 Loss: 0.017294314562427082  PSNR: 20.704877853393555\n","[TRAIN] Iter: 11300 Loss: 0.017466721042567246  PSNR: 20.61803436279297\n","[TRAIN] Iter: 11400 Loss: 0.0185309156621667  PSNR: 20.386280059814453\n","[TRAIN] Iter: 11500 Loss: 0.017724915695075484  PSNR: 20.600826263427734\n","[TRAIN] Iter: 11600 Loss: 0.01896462995614826  PSNR: 20.30426597595215\n","[TRAIN] Iter: 11700 Loss: 0.01743690704062567  PSNR: 20.629228591918945\n","[TRAIN] Iter: 11800 Loss: 0.017376691111895288  PSNR: 20.66930389404297\n","[TRAIN] Iter: 11900 Loss: 0.017853864868880426  PSNR: 20.59471893310547\n","[TRAIN] Iter: 12000 Loss: 0.018289700385364305  PSNR: 20.41175651550293\n","[TRAIN] Iter: 12100 Loss: 0.018495540296232783  PSNR: 20.43655776977539\n","[TRAIN] Iter: 12200 Loss: 0.018224949226292232  PSNR: 20.488862991333008\n","[TRAIN] Iter: 12300 Loss: 0.018493773144447945  PSNR: 20.44207000732422\n","[TRAIN] Iter: 12400 Loss: 0.01785142456091056  PSNR: 20.572891235351562\n","[TRAIN] Iter: 12500 Loss: 0.017970766952138967  PSNR: 20.53085708618164\n","[TRAIN] Iter: 12600 Loss: 0.017315202943424823  PSNR: 20.687631607055664\n","[TRAIN] Iter: 12700 Loss: 0.0181325453702212  PSNR: 20.518535614013672\n","[TRAIN] Iter: 12800 Loss: 0.018367174761362355  PSNR: 20.442779541015625\n","[TRAIN] Iter: 12900 Loss: 0.017024691325970242  PSNR: 20.77530288696289\n","[TRAIN] Iter: 13000 Loss: 0.018011340161674456  PSNR: 20.570688247680664\n","[TRAIN] Iter: 13100 Loss: 0.017202490691394616  PSNR: 20.760414123535156\n","[TRAIN] Iter: 13200 Loss: 0.01810209697728948  PSNR: 20.470481872558594\n","[TRAIN] Iter: 13300 Loss: 0.01649957787975848  PSNR: 20.925678253173828\n","[TRAIN] Iter: 13400 Loss: 0.017767541470226442  PSNR: 20.56146240234375\n","[TRAIN] Iter: 13500 Loss: 0.017195798752844858  PSNR: 20.76784324645996\n","[TRAIN] Iter: 13600 Loss: 0.017760271294349355  PSNR: 20.611377716064453\n","[TRAIN] Iter: 13700 Loss: 0.016826091068187138  PSNR: 20.840194702148438\n","[TRAIN] Iter: 13800 Loss: 0.01932390700281889  PSNR: 20.183462142944336\n","[TRAIN] Iter: 13900 Loss: 0.018112216439485178  PSNR: 20.50471305847168\n","[TRAIN] Iter: 14000 Loss: 0.016141392105682398  PSNR: 21.010433197021484\n","[TRAIN] Iter: 14100 Loss: 0.017511353545305773  PSNR: 20.655969619750977\n","[TRAIN] Iter: 14200 Loss: 0.016946427424726847  PSNR: 20.771892547607422\n","[TRAIN] Iter: 14300 Loss: 0.017003116658749456  PSNR: 20.794178009033203\n","[TRAIN] Iter: 14400 Loss: 0.017960982593967674  PSNR: 20.568422317504883\n","[TRAIN] Iter: 14500 Loss: 0.01734926329294957  PSNR: 20.674495697021484\n","[TRAIN] Iter: 14600 Loss: 0.016580135163683427  PSNR: 20.902299880981445\n","[TRAIN] Iter: 14700 Loss: 0.016537689448967233  PSNR: 20.917434692382812\n","[TRAIN] Iter: 14800 Loss: 0.018546760133372517  PSNR: 20.408348083496094\n","[TRAIN] Iter: 14900 Loss: 0.01754239334202015  PSNR: 20.669923782348633\n","[TRAIN] Iter: 15000 Loss: 0.015264477017569542  PSNR: 21.23064613342285\n","[TRAIN] Iter: 15100 Loss: 0.016147071941650444  PSNR: 21.013111114501953\n","[TRAIN] Iter: 15200 Loss: 0.017101729059353813  PSNR: 20.788572311401367\n","[TRAIN] Iter: 15300 Loss: 0.01677229519063927  PSNR: 20.838565826416016\n","[TRAIN] Iter: 15400 Loss: 0.015814035168021948  PSNR: 21.147907257080078\n","[TRAIN] Iter: 15500 Loss: 0.016630172310550105  PSNR: 20.886320114135742\n","[TRAIN] Iter: 15600 Loss: 0.017745999504325673  PSNR: 20.579593658447266\n","[TRAIN] Iter: 15700 Loss: 0.016874239303250452  PSNR: 20.83444595336914\n","[TRAIN] Iter: 15800 Loss: 0.01820849576595457  PSNR: 20.489179611206055\n","[TRAIN] Iter: 15900 Loss: 0.017174762502273785  PSNR: 20.797746658325195\n","[TRAIN] Iter: 16000 Loss: 0.01722805737226442  PSNR: 20.73668670654297\n","[TRAIN] Iter: 16100 Loss: 0.016590488297638643  PSNR: 20.953527450561523\n","[TRAIN] Iter: 16200 Loss: 0.018966900328709266  PSNR: 20.314552307128906\n","[TRAIN] Iter: 16300 Loss: 0.016639929634285316  PSNR: 20.87655258178711\n","[TRAIN] Iter: 16400 Loss: 0.015957224107863828  PSNR: 21.087913513183594\n","[TRAIN] Iter: 16500 Loss: 0.016571596582680412  PSNR: 20.909137725830078\n","[TRAIN] Iter: 16600 Loss: 0.016723473785979676  PSNR: 20.852197647094727\n","[TRAIN] Iter: 16700 Loss: 0.016529472415149987  PSNR: 20.91902732849121\n","[TRAIN] Iter: 16800 Loss: 0.01736538639947757  PSNR: 20.72588348388672\n","[TRAIN] Iter: 16900 Loss: 0.016278293923020465  PSNR: 21.0238094329834\n","[TRAIN] Iter: 17000 Loss: 0.016374531515882196  PSNR: 21.02826690673828\n","[TRAIN] Iter: 17100 Loss: 0.016284672487841947  PSNR: 20.99595832824707\n","[TRAIN] Iter: 17200 Loss: 0.01536407776203724  PSNR: 21.2420654296875\n","[TRAIN] Iter: 17300 Loss: 0.01563927571147125  PSNR: 21.138811111450195\n","[TRAIN] Iter: 17400 Loss: 0.01618720568852381  PSNR: 21.02138328552246\n","[TRAIN] Iter: 17500 Loss: 0.016835376135878807  PSNR: 20.813190460205078\n","[TRAIN] Iter: 17600 Loss: 0.016046937754321873  PSNR: 21.15009307861328\n","[TRAIN] Iter: 17700 Loss: 0.016946436130385486  PSNR: 20.80705451965332\n","[TRAIN] Iter: 17800 Loss: 0.015884053124673912  PSNR: 21.103303909301758\n","[TRAIN] Iter: 17900 Loss: 0.01669037939104672  PSNR: 20.890851974487305\n","[TRAIN] Iter: 18000 Loss: 0.01638799263700087  PSNR: 20.970975875854492\n","[TRAIN] Iter: 18100 Loss: 0.01609665587144904  PSNR: 20.989410400390625\n","[TRAIN] Iter: 18200 Loss: 0.01597644727734373  PSNR: 21.101268768310547\n","[TRAIN] Iter: 18300 Loss: 0.015781312675469762  PSNR: 21.117576599121094\n","[TRAIN] Iter: 18400 Loss: 0.015568445190037038  PSNR: 21.156719207763672\n","[TRAIN] Iter: 18500 Loss: 0.01591114754909976  PSNR: 21.105215072631836\n","[TRAIN] Iter: 18600 Loss: 0.017822995972956356  PSNR: 20.589597702026367\n","[TRAIN] Iter: 18700 Loss: 0.016653706091996855  PSNR: 20.912052154541016\n","[TRAIN] Iter: 18800 Loss: 0.016530443084572174  PSNR: 20.950183868408203\n","[TRAIN] Iter: 18900 Loss: 0.0167825073364261  PSNR: 20.85867691040039\n","[TRAIN] Iter: 19000 Loss: 0.015757307537362455  PSNR: 21.135591506958008\n","[TRAIN] Iter: 19100 Loss: 0.016843501889358257  PSNR: 20.82674217224121\n","[TRAIN] Iter: 19200 Loss: 0.017021777372632566  PSNR: 20.832866668701172\n","[TRAIN] Iter: 19300 Loss: 0.015129836849315764  PSNR: 21.3167781829834\n","[TRAIN] Iter: 19400 Loss: 0.01612218472451578  PSNR: 21.028566360473633\n","[TRAIN] Iter: 19500 Loss: 0.016323749529701517  PSNR: 21.04010581970215\n","[TRAIN] Iter: 19600 Loss: 0.015397178218369516  PSNR: 21.20728874206543\n","[TRAIN] Iter: 19700 Loss: 0.015915230519225405  PSNR: 21.084976196289062\n"," 10% 19799/200000 [1:31:58<13:44:12,  3.64it/s]Shuffle data after an epoch!\n","[TRAIN] Iter: 19800 Loss: 0.01586994012554297  PSNR: 21.103092193603516\n","[TRAIN] Iter: 19900 Loss: 0.015329932506167456  PSNR: 21.230131149291992\n"," 10% 19999/200000 [1:33:00<14:06:00,  3.55it/s]Saved checkpoints at ./logs/56leonard_test_st0/020000.tar\n","[TRAIN] Iter: 20000 Loss: 0.016457481108013372  PSNR: 20.955638885498047\n","[TRAIN] Iter: 20100 Loss: 0.01561569177902638  PSNR: 21.212770462036133\n","[TRAIN] Iter: 20200 Loss: 0.015346614320238186  PSNR: 21.331192016601562\n","[TRAIN] Iter: 20300 Loss: 0.015736901177954482  PSNR: 21.105117797851562\n","[TRAIN] Iter: 20400 Loss: 0.01607583024200867  PSNR: 21.05560874938965\n","[TRAIN] Iter: 20500 Loss: 0.015511843044822504  PSNR: 21.181411743164062\n","[TRAIN] Iter: 20600 Loss: 0.016674428753563057  PSNR: 20.916383743286133\n","[TRAIN] Iter: 20700 Loss: 0.015379825517575378  PSNR: 21.26572608947754\n","[TRAIN] Iter: 20800 Loss: 0.015317750505154264  PSNR: 21.310930252075195\n","[TRAIN] Iter: 20900 Loss: 0.01553243241159669  PSNR: 21.250991821289062\n","[TRAIN] Iter: 21000 Loss: 0.01576566960127484  PSNR: 21.16234016418457\n","[TRAIN] Iter: 21100 Loss: 0.01666013512606991  PSNR: 20.932594299316406\n","[TRAIN] Iter: 21200 Loss: 0.015603402730422856  PSNR: 21.174238204956055\n","[TRAIN] Iter: 21300 Loss: 0.014776249314838624  PSNR: 21.44664192199707\n","[TRAIN] Iter: 21400 Loss: 0.016165773663332064  PSNR: 21.03399085998535\n","[TRAIN] Iter: 21500 Loss: 0.014883554211478533  PSNR: 21.3861083984375\n","[TRAIN] Iter: 21600 Loss: 0.016578971580599684  PSNR: 20.956266403198242\n","[TRAIN] Iter: 21700 Loss: 0.015647014610415934  PSNR: 21.182828903198242\n","[TRAIN] Iter: 21800 Loss: 0.016054828753724826  PSNR: 21.10910415649414\n","[TRAIN] Iter: 21900 Loss: 0.0158383888924417  PSNR: 21.118595123291016\n","[TRAIN] Iter: 22000 Loss: 0.015905518529243115  PSNR: 21.14886474609375\n","[TRAIN] Iter: 22100 Loss: 0.014246703581526434  PSNR: 21.640094757080078\n","[TRAIN] Iter: 22200 Loss: 0.015481220300764566  PSNR: 21.282495498657227\n","[TRAIN] Iter: 22300 Loss: 0.01624503676000805  PSNR: 21.054475784301758\n","[TRAIN] Iter: 22400 Loss: 0.01527452943863334  PSNR: 21.304927825927734\n","[TRAIN] Iter: 22500 Loss: 0.014536821235777098  PSNR: 21.46600341796875\n","[TRAIN] Iter: 22600 Loss: 0.015210805562461873  PSNR: 21.347023010253906\n","[TRAIN] Iter: 22700 Loss: 0.016587931412604656  PSNR: 20.942581176757812\n","[TRAIN] Iter: 22800 Loss: 0.016809141549799386  PSNR: 20.843063354492188\n","[TRAIN] Iter: 22900 Loss: 0.015175055757328413  PSNR: 21.358030319213867\n","[TRAIN] Iter: 23000 Loss: 0.01590050329975355  PSNR: 21.140058517456055\n","[TRAIN] Iter: 23100 Loss: 0.015774441857047516  PSNR: 21.19465446472168\n","[TRAIN] Iter: 23200 Loss: 0.01563393129579333  PSNR: 21.191543579101562\n","[TRAIN] Iter: 23300 Loss: 0.01523970801522581  PSNR: 21.253482818603516\n","[TRAIN] Iter: 23400 Loss: 0.015675317674165395  PSNR: 21.16631317138672\n","[TRAIN] Iter: 23500 Loss: 0.016017085587219936  PSNR: 21.135498046875\n","[TRAIN] Iter: 23600 Loss: 0.015613388974118616  PSNR: 21.201011657714844\n","[TRAIN] Iter: 23700 Loss: 0.015802243639186267  PSNR: 21.144664764404297\n","[TRAIN] Iter: 23800 Loss: 0.016016370873431662  PSNR: 21.05158233642578\n","[TRAIN] Iter: 23900 Loss: 0.015895437152196996  PSNR: 21.13105583190918\n","[TRAIN] Iter: 24000 Loss: 0.015658087359007623  PSNR: 21.144466400146484\n","[TRAIN] Iter: 24100 Loss: 0.014845478171899124  PSNR: 21.4774169921875\n","[TRAIN] Iter: 24200 Loss: 0.015612419472754192  PSNR: 21.205734252929688\n","[TRAIN] Iter: 24300 Loss: 0.014748006471216338  PSNR: 21.446863174438477\n","[TRAIN] Iter: 24400 Loss: 0.01445006745981123  PSNR: 21.504619598388672\n","[TRAIN] Iter: 24500 Loss: 0.01523732654564896  PSNR: 21.26018524169922\n","[TRAIN] Iter: 24600 Loss: 0.015537409677629252  PSNR: 21.202051162719727\n","[TRAIN] Iter: 24700 Loss: 0.015702530792379692  PSNR: 21.19208526611328\n","[TRAIN] Iter: 24800 Loss: 0.014696396516217321  PSNR: 21.53017807006836\n","[TRAIN] Iter: 24900 Loss: 0.01590663824199815  PSNR: 21.09172248840332\n","[TRAIN] Iter: 25000 Loss: 0.015042786143700127  PSNR: 21.364669799804688\n","[TRAIN] Iter: 25100 Loss: 0.01478440638581354  PSNR: 21.43037986755371\n","[TRAIN] Iter: 25200 Loss: 0.015529795147944548  PSNR: 21.21695899963379\n","[TRAIN] Iter: 25300 Loss: 0.015225105528963258  PSNR: 21.237625122070312\n","[TRAIN] Iter: 25400 Loss: 0.01531665746001881  PSNR: 21.265531539916992\n","[TRAIN] Iter: 25500 Loss: 0.015625088911409662  PSNR: 21.22834587097168\n","[TRAIN] Iter: 25600 Loss: 0.015342931837535984  PSNR: 21.253463745117188\n","[TRAIN] Iter: 25700 Loss: 0.014663411321318483  PSNR: 21.46853256225586\n","[TRAIN] Iter: 25800 Loss: 0.01569555085537767  PSNR: 21.193330764770508\n","[TRAIN] Iter: 25900 Loss: 0.015412495513183393  PSNR: 21.230878829956055\n","[TRAIN] Iter: 26000 Loss: 0.014475636064758046  PSNR: 21.54800033569336\n","[TRAIN] Iter: 26100 Loss: 0.01527883864675954  PSNR: 21.338294982910156\n","[TRAIN] Iter: 26200 Loss: 0.014942455982727719  PSNR: 21.428695678710938\n","[TRAIN] Iter: 26300 Loss: 0.015692295232511012  PSNR: 21.24274253845215\n","[TRAIN] Iter: 26400 Loss: 0.014350031416062009  PSNR: 21.608869552612305\n","[TRAIN] Iter: 26500 Loss: 0.015249138328641686  PSNR: 21.252025604248047\n","[TRAIN] Iter: 26600 Loss: 0.01446748678752489  PSNR: 21.57991600036621\n","[TRAIN] Iter: 26700 Loss: 0.014805668869693372  PSNR: 21.44077491760254\n","[TRAIN] Iter: 26800 Loss: 0.015068817684310386  PSNR: 21.35024642944336\n","[TRAIN] Iter: 26900 Loss: 0.015052465672810062  PSNR: 21.347021102905273\n","[TRAIN] Iter: 27000 Loss: 0.014437774370030949  PSNR: 21.533479690551758\n","[TRAIN] Iter: 27100 Loss: 0.014039682225348252  PSNR: 21.634794235229492\n","[TRAIN] Iter: 27200 Loss: 0.014259535357375432  PSNR: 21.60517692565918\n","[TRAIN] Iter: 27300 Loss: 0.014100218920082671  PSNR: 21.672578811645508\n","[TRAIN] Iter: 27400 Loss: 0.015632470027094246  PSNR: 21.22274398803711\n","[TRAIN] Iter: 27500 Loss: 0.013753896248325372  PSNR: 21.712703704833984\n","[TRAIN] Iter: 27600 Loss: 0.015141064967698525  PSNR: 21.312612533569336\n","[TRAIN] Iter: 27700 Loss: 0.0138750779436242  PSNR: 21.709033966064453\n","[TRAIN] Iter: 27800 Loss: 0.015764291417796797  PSNR: 21.176101684570312\n","[TRAIN] Iter: 27900 Loss: 0.013965255490065012  PSNR: 21.707244873046875\n","[TRAIN] Iter: 28000 Loss: 0.015391002235386371  PSNR: 21.2813663482666\n","[TRAIN] Iter: 28100 Loss: 0.01509069071326446  PSNR: 21.332843780517578\n","[TRAIN] Iter: 28200 Loss: 0.014862782425482049  PSNR: 21.474027633666992\n","[TRAIN] Iter: 28300 Loss: 0.014231228653777121  PSNR: 21.62171745300293\n","[TRAIN] Iter: 28400 Loss: 0.014737621070709726  PSNR: 21.451465606689453\n","[TRAIN] Iter: 28500 Loss: 0.014937458040504488  PSNR: 21.396495819091797\n","[TRAIN] Iter: 28600 Loss: 0.014360075088303503  PSNR: 21.57126235961914\n","[TRAIN] Iter: 28700 Loss: 0.015367699894520625  PSNR: 21.255910873413086\n","[TRAIN] Iter: 28800 Loss: 0.014305431771557135  PSNR: 21.60188865661621\n","[TRAIN] Iter: 28900 Loss: 0.015278075302631942  PSNR: 21.341110229492188\n","[TRAIN] Iter: 29000 Loss: 0.013841773110693074  PSNR: 21.733692169189453\n","[TRAIN] Iter: 29100 Loss: 0.01458923624082443  PSNR: 21.421405792236328\n","[TRAIN] Iter: 29200 Loss: 0.015087611755059188  PSNR: 21.36669158935547\n","[TRAIN] Iter: 29300 Loss: 0.014932120145473012  PSNR: 21.357181549072266\n","[TRAIN] Iter: 29400 Loss: 0.015784448717099785  PSNR: 21.138931274414062\n","[TRAIN] Iter: 29500 Loss: 0.01467193977466458  PSNR: 21.401168823242188\n","[TRAIN] Iter: 29600 Loss: 0.015307369262422302  PSNR: 21.351245880126953\n","[TRAIN] Iter: 29700 Loss: 0.01505841089043784  PSNR: 21.43572235107422\n","[TRAIN] Iter: 29800 Loss: 0.014740022385114503  PSNR: 21.468910217285156\n","[TRAIN] Iter: 29900 Loss: 0.014378677850478382  PSNR: 21.56511116027832\n"," 15% 29999/200000 [2:19:11<12:54:29,  3.66it/s]Saved checkpoints at ./logs/56leonard_test_st0/030000.tar\n","[TRAIN] Iter: 30000 Loss: 0.014067852470812634  PSNR: 21.67220115661621\n","[TRAIN] Iter: 30100 Loss: 0.013872751447201723  PSNR: 21.77669334411621\n","[TRAIN] Iter: 30200 Loss: 0.014833154571932496  PSNR: 21.469459533691406\n","[TRAIN] Iter: 30300 Loss: 0.014519088213217795  PSNR: 21.51130485534668\n","[TRAIN] Iter: 30400 Loss: 0.01585214421822646  PSNR: 21.16120719909668\n","[TRAIN] Iter: 30500 Loss: 0.013886042866306077  PSNR: 21.789813995361328\n","[TRAIN] Iter: 30600 Loss: 0.01494782337544234  PSNR: 21.40934181213379\n","[TRAIN] Iter: 30700 Loss: 0.015560548581118355  PSNR: 21.224016189575195\n","[TRAIN] Iter: 30800 Loss: 0.013820149492061246  PSNR: 21.75339698791504\n","[TRAIN] Iter: 30900 Loss: 0.014128025770975241  PSNR: 21.653053283691406\n","[TRAIN] Iter: 31000 Loss: 0.014303508732692555  PSNR: 21.58407974243164\n","[TRAIN] Iter: 31100 Loss: 0.014718264681663703  PSNR: 21.481204986572266\n","[TRAIN] Iter: 31200 Loss: 0.015251685241149449  PSNR: 21.288755416870117\n","[TRAIN] Iter: 31300 Loss: 0.015194695482341838  PSNR: 21.36008644104004\n","[TRAIN] Iter: 31400 Loss: 0.013590203010786388  PSNR: 21.800262451171875\n","[TRAIN] Iter: 31500 Loss: 0.014591049212205148  PSNR: 21.523752212524414\n","[TRAIN] Iter: 31600 Loss: 0.015868713922153648  PSNR: 21.116430282592773\n","[TRAIN] Iter: 31700 Loss: 0.0145095585312244  PSNR: 21.517621994018555\n","[TRAIN] Iter: 31800 Loss: 0.014073531988957059  PSNR: 21.677963256835938\n","[TRAIN] Iter: 31900 Loss: 0.014535443846011804  PSNR: 21.52674102783203\n","[TRAIN] Iter: 32000 Loss: 0.014613482411673782  PSNR: 21.47649383544922\n","[TRAIN] Iter: 32100 Loss: 0.016278553365854136  PSNR: 21.04060935974121\n","[TRAIN] Iter: 32200 Loss: 0.012617432443673188  PSNR: 22.221696853637695\n","[TRAIN] Iter: 32300 Loss: 0.014062655588626988  PSNR: 21.75157356262207\n","[TRAIN] Iter: 32400 Loss: 0.013319115054387968  PSNR: 21.873462677001953\n","[TRAIN] Iter: 32500 Loss: 0.014777007345284815  PSNR: 21.438194274902344\n","[TRAIN] Iter: 32600 Loss: 0.015562419143802745  PSNR: 21.203968048095703\n","[TRAIN] Iter: 32700 Loss: 0.013487190039820686  PSNR: 21.85013771057129\n","[TRAIN] Iter: 32800 Loss: 0.01488488976281314  PSNR: 21.452804565429688\n","[TRAIN] Iter: 32900 Loss: 0.014600378940951312  PSNR: 21.49298095703125\n","[TRAIN] Iter: 33000 Loss: 0.015518807735362402  PSNR: 21.26649284362793\n","[TRAIN] Iter: 33100 Loss: 0.014104397498734081  PSNR: 21.625598907470703\n","[TRAIN] Iter: 33200 Loss: 0.014871804537648818  PSNR: 21.43672752380371\n","[TRAIN] Iter: 33300 Loss: 0.014913506785591824  PSNR: 21.43390655517578\n","[TRAIN] Iter: 33400 Loss: 0.015584965758689017  PSNR: 21.243408203125\n","[TRAIN] Iter: 33500 Loss: 0.014315424505428994  PSNR: 21.56397247314453\n","[TRAIN] Iter: 33600 Loss: 0.0142185039129348  PSNR: 21.638050079345703\n","[TRAIN] Iter: 33700 Loss: 0.013843536783943395  PSNR: 21.761388778686523\n","[TRAIN] Iter: 33800 Loss: 0.013231607329784843  PSNR: 21.905921936035156\n","[TRAIN] Iter: 33900 Loss: 0.015370808737855634  PSNR: 21.291406631469727\n","[TRAIN] Iter: 34000 Loss: 0.014425387500712766  PSNR: 21.61451530456543\n","[TRAIN] Iter: 34100 Loss: 0.013977494862668721  PSNR: 21.708799362182617\n","[TRAIN] Iter: 34200 Loss: 0.013491540707559065  PSNR: 21.864606857299805\n","[TRAIN] Iter: 34300 Loss: 0.014107775748973057  PSNR: 21.714191436767578\n","[TRAIN] Iter: 34400 Loss: 0.014890744924129201  PSNR: 21.394506454467773\n","[TRAIN] Iter: 34500 Loss: 0.015530294057583063  PSNR: 21.2354736328125\n","[TRAIN] Iter: 34600 Loss: 0.014487094277394575  PSNR: 21.496509552001953\n","[TRAIN] Iter: 34700 Loss: 0.014181822653445104  PSNR: 21.726104736328125\n","[TRAIN] Iter: 34800 Loss: 0.014126466817978207  PSNR: 21.651016235351562\n","[TRAIN] Iter: 34900 Loss: 0.014289511896551673  PSNR: 21.544363021850586\n","[TRAIN] Iter: 35000 Loss: 0.013551131890329384  PSNR: 21.8182373046875\n","[TRAIN] Iter: 35100 Loss: 0.014045921001081669  PSNR: 21.660844802856445\n","[TRAIN] Iter: 35200 Loss: 0.01589278140747676  PSNR: 21.152835845947266\n","[TRAIN] Iter: 35300 Loss: 0.013635466549360091  PSNR: 21.788734436035156\n","[TRAIN] Iter: 35400 Loss: 0.014337190109759406  PSNR: 21.53569221496582\n","[TRAIN] Iter: 35500 Loss: 0.013790101009894872  PSNR: 21.722314834594727\n","[TRAIN] Iter: 35600 Loss: 0.014159113384764227  PSNR: 21.714035034179688\n","[TRAIN] Iter: 35700 Loss: 0.014167002191423379  PSNR: 21.746501922607422\n","[TRAIN] Iter: 35800 Loss: 0.015510871104324095  PSNR: 21.283855438232422\n","[TRAIN] Iter: 35900 Loss: 0.014319237580285173  PSNR: 21.62110137939453\n","[TRAIN] Iter: 36000 Loss: 0.013934480997195284  PSNR: 21.74419403076172\n","[TRAIN] Iter: 36100 Loss: 0.013533361416154854  PSNR: 21.90097427368164\n","[TRAIN] Iter: 36200 Loss: 0.014175633625793917  PSNR: 21.66273307800293\n","[TRAIN] Iter: 36300 Loss: 0.013982974642646213  PSNR: 21.632244110107422\n","[TRAIN] Iter: 36400 Loss: 0.014573420913452179  PSNR: 21.51268768310547\n","[TRAIN] Iter: 36500 Loss: 0.013994646082422464  PSNR: 21.711532592773438\n","[TRAIN] Iter: 36600 Loss: 0.013688099420248448  PSNR: 21.81287956237793\n","[TRAIN] Iter: 36700 Loss: 0.014393268906357544  PSNR: 21.634532928466797\n","[TRAIN] Iter: 36800 Loss: 0.013443846429360713  PSNR: 21.84209632873535\n","[TRAIN] Iter: 36900 Loss: 0.014110768769287595  PSNR: 21.71596908569336\n","[TRAIN] Iter: 37000 Loss: 0.013770563040726404  PSNR: 21.805639266967773\n","[TRAIN] Iter: 37100 Loss: 0.014722215227854496  PSNR: 21.5495662689209\n","[TRAIN] Iter: 37200 Loss: 0.013150007432514158  PSNR: 21.994792938232422\n","[TRAIN] Iter: 37300 Loss: 0.01409101137733418  PSNR: 21.597009658813477\n","[TRAIN] Iter: 37400 Loss: 0.013963086650619752  PSNR: 21.786001205444336\n","[TRAIN] Iter: 37500 Loss: 0.014385563274754605  PSNR: 21.547042846679688\n","[TRAIN] Iter: 37600 Loss: 0.014503419159554325  PSNR: 21.558931350708008\n","[TRAIN] Iter: 37700 Loss: 0.014458971191684026  PSNR: 21.57118797302246\n","[TRAIN] Iter: 37800 Loss: 0.014297543654769446  PSNR: 21.636503219604492\n","[TRAIN] Iter: 37900 Loss: 0.013799277166251362  PSNR: 21.77227020263672\n","[TRAIN] Iter: 38000 Loss: 0.013396609888234441  PSNR: 21.82988929748535\n","[TRAIN] Iter: 38100 Loss: 0.01426686624067129  PSNR: 21.691770553588867\n","[TRAIN] Iter: 38200 Loss: 0.013105969537213769  PSNR: 21.981868743896484\n","[TRAIN] Iter: 38300 Loss: 0.0147376980939833  PSNR: 21.5953311920166\n","[TRAIN] Iter: 38400 Loss: 0.014092543500677651  PSNR: 21.708110809326172\n","[TRAIN] Iter: 38500 Loss: 0.014675270284361792  PSNR: 21.5098819732666\n","[TRAIN] Iter: 38600 Loss: 0.01445943294779705  PSNR: 21.621002197265625\n","[TRAIN] Iter: 38700 Loss: 0.014342138723994752  PSNR: 21.621000289916992\n","[TRAIN] Iter: 38800 Loss: 0.014126120100976105  PSNR: 21.619949340820312\n","[TRAIN] Iter: 38900 Loss: 0.01329832888097828  PSNR: 21.958385467529297\n","[TRAIN] Iter: 39000 Loss: 0.012947242806658387  PSNR: 22.03667640686035\n","[TRAIN] Iter: 39100 Loss: 0.014033706603157812  PSNR: 21.727785110473633\n","[TRAIN] Iter: 39200 Loss: 0.01278849157574856  PSNR: 22.104883193969727\n","[TRAIN] Iter: 39300 Loss: 0.013214630634873204  PSNR: 22.111408233642578\n","[TRAIN] Iter: 39400 Loss: 0.013803517320125778  PSNR: 21.826356887817383\n","[TRAIN] Iter: 39500 Loss: 0.013641174518915002  PSNR: 21.807159423828125\n"," 20% 39599/200000 [3:03:59<12:30:27,  3.56it/s]Shuffle data after an epoch!\n","[TRAIN] Iter: 39600 Loss: 0.014022960900081111  PSNR: 21.719635009765625\n","[TRAIN] Iter: 39700 Loss: 0.014229904457805791  PSNR: 21.668033599853516\n","[TRAIN] Iter: 39800 Loss: 0.014190030312341294  PSNR: 21.6647891998291\n","[TRAIN] Iter: 39900 Loss: 0.013767423789430428  PSNR: 21.74623680114746\n"," 20% 39999/200000 [3:05:57<12:31:37,  3.55it/s]Saved checkpoints at ./logs/56leonard_test_st0/040000.tar\n","[TRAIN] Iter: 40000 Loss: 0.012733657165744414  PSNR: 22.133359909057617\n","[TRAIN] Iter: 40100 Loss: 0.013588106733086096  PSNR: 21.872142791748047\n","[TRAIN] Iter: 40200 Loss: 0.013965009942491693  PSNR: 21.741140365600586\n","[TRAIN] Iter: 40300 Loss: 0.014238728696727878  PSNR: 21.676179885864258\n","[TRAIN] Iter: 40400 Loss: 0.013123752787037628  PSNR: 21.968847274780273\n","[TRAIN] Iter: 40500 Loss: 0.012526607119191265  PSNR: 22.2418212890625\n","[TRAIN] Iter: 40600 Loss: 0.014481069351186007  PSNR: 21.55634880065918\n","[TRAIN] Iter: 40700 Loss: 0.01363111757092382  PSNR: 21.826725006103516\n","[TRAIN] Iter: 40800 Loss: 0.014690550571576776  PSNR: 21.540470123291016\n","[TRAIN] Iter: 40900 Loss: 0.013712812231120754  PSNR: 21.729598999023438\n","[TRAIN] Iter: 41000 Loss: 0.013292972686650882  PSNR: 21.961381912231445\n","[TRAIN] Iter: 41100 Loss: 0.013679359718383995  PSNR: 21.76740264892578\n","[TRAIN] Iter: 41200 Loss: 0.013762628719789033  PSNR: 21.82893943786621\n","[TRAIN] Iter: 41300 Loss: 0.013580723287317903  PSNR: 21.86641502380371\n","[TRAIN] Iter: 41400 Loss: 0.014231631042301537  PSNR: 21.691022872924805\n","[TRAIN] Iter: 41500 Loss: 0.013739448154632862  PSNR: 21.8062801361084\n","[TRAIN] Iter: 41600 Loss: 0.015003115716325798  PSNR: 21.393951416015625\n","[TRAIN] Iter: 41700 Loss: 0.015208279594054148  PSNR: 21.38459587097168\n","[TRAIN] Iter: 41800 Loss: 0.01318943840780645  PSNR: 21.901758193969727\n","[TRAIN] Iter: 41900 Loss: 0.014023884675824744  PSNR: 21.672056198120117\n","[TRAIN] Iter: 42000 Loss: 0.013803256852955805  PSNR: 21.79473114013672\n","[TRAIN] Iter: 42100 Loss: 0.013382216796838257  PSNR: 21.86819839477539\n","[TRAIN] Iter: 42200 Loss: 0.01337938875876779  PSNR: 21.938980102539062\n","[TRAIN] Iter: 42300 Loss: 0.014306194982401634  PSNR: 21.631929397583008\n","[TRAIN] Iter: 42400 Loss: 0.013345007634976463  PSNR: 21.89106559753418\n","[TRAIN] Iter: 42500 Loss: 0.013381059351360091  PSNR: 21.928489685058594\n","[TRAIN] Iter: 42600 Loss: 0.012139697889974745  PSNR: 22.398109436035156\n","[TRAIN] Iter: 42700 Loss: 0.013389284000716505  PSNR: 21.871070861816406\n","[TRAIN] Iter: 42800 Loss: 0.013803900469128123  PSNR: 21.805002212524414\n","[TRAIN] Iter: 42900 Loss: 0.013277031075150573  PSNR: 21.96463966369629\n","[TRAIN] Iter: 43000 Loss: 0.012842661892461714  PSNR: 22.106143951416016\n","[TRAIN] Iter: 43100 Loss: 0.01373620607395409  PSNR: 21.82357406616211\n","[TRAIN] Iter: 43200 Loss: 0.014012991306302404  PSNR: 21.740373611450195\n","[TRAIN] Iter: 43300 Loss: 0.013217981159746975  PSNR: 21.99028778076172\n","[TRAIN] Iter: 43400 Loss: 0.014416274212621195  PSNR: 21.61167335510254\n","[TRAIN] Iter: 43500 Loss: 0.012743184842082813  PSNR: 22.138708114624023\n","[TRAIN] Iter: 43600 Loss: 0.013392992039062667  PSNR: 21.955833435058594\n","[TRAIN] Iter: 43700 Loss: 0.013176509875230471  PSNR: 22.024744033813477\n","[TRAIN] Iter: 43800 Loss: 0.013111577710575615  PSNR: 21.992084503173828\n","[TRAIN] Iter: 43900 Loss: 0.013424804605225842  PSNR: 21.96222496032715\n","[TRAIN] Iter: 44000 Loss: 0.013078466944722096  PSNR: 22.023828506469727\n","[TRAIN] Iter: 44100 Loss: 0.014259911788297261  PSNR: 21.693159103393555\n","[TRAIN] Iter: 44200 Loss: 0.013586442096895635  PSNR: 21.83492660522461\n","[TRAIN] Iter: 44300 Loss: 0.014413225891598826  PSNR: 21.642871856689453\n","[TRAIN] Iter: 44400 Loss: 0.013979621101003235  PSNR: 21.75956153869629\n","[TRAIN] Iter: 44500 Loss: 0.013329466226950016  PSNR: 21.939090728759766\n","[TRAIN] Iter: 44600 Loss: 0.014695183066465703  PSNR: 21.494171142578125\n","[TRAIN] Iter: 44700 Loss: 0.013166076992887974  PSNR: 21.96737289428711\n","[TRAIN] Iter: 44800 Loss: 0.013049222627404423  PSNR: 22.073291778564453\n","[TRAIN] Iter: 44900 Loss: 0.01357504601603271  PSNR: 21.859140396118164\n","[TRAIN] Iter: 45000 Loss: 0.013813895387584228  PSNR: 21.725040435791016\n","[TRAIN] Iter: 45100 Loss: 0.012975737712516991  PSNR: 22.08139419555664\n","[TRAIN] Iter: 45200 Loss: 0.013291777499280163  PSNR: 21.928882598876953\n","[TRAIN] Iter: 45300 Loss: 0.014560781148665916  PSNR: 21.557233810424805\n","[TRAIN] Iter: 45400 Loss: 0.012834342276540393  PSNR: 22.123777389526367\n","[TRAIN] Iter: 45500 Loss: 0.013822794214421445  PSNR: 21.819381713867188\n","[TRAIN] Iter: 45600 Loss: 0.013174741997274829  PSNR: 21.981754302978516\n","[TRAIN] Iter: 45700 Loss: 0.014205293541342473  PSNR: 21.682910919189453\n","[TRAIN] Iter: 45800 Loss: 0.014875920957357737  PSNR: 21.44300651550293\n","[TRAIN] Iter: 45900 Loss: 0.013274112212667114  PSNR: 21.930889129638672\n","[TRAIN] Iter: 46000 Loss: 0.014504742137430549  PSNR: 21.46939468383789\n","[TRAIN] Iter: 46100 Loss: 0.014135216854575618  PSNR: 21.639328002929688\n","[TRAIN] Iter: 46200 Loss: 0.012728593974986347  PSNR: 22.114849090576172\n","[TRAIN] Iter: 46300 Loss: 0.014051843984815281  PSNR: 21.626380920410156\n","[TRAIN] Iter: 46400 Loss: 0.013993197634372281  PSNR: 21.684803009033203\n","[TRAIN] Iter: 46500 Loss: 0.012681562253118114  PSNR: 22.144041061401367\n","[TRAIN] Iter: 46600 Loss: 0.013246498240305795  PSNR: 21.936079025268555\n","[TRAIN] Iter: 46700 Loss: 0.013617042762732953  PSNR: 21.82770347595215\n","[TRAIN] Iter: 46800 Loss: 0.01268037150963272  PSNR: 22.13629913330078\n","[TRAIN] Iter: 46900 Loss: 0.012482536273516566  PSNR: 22.22337532043457\n","[TRAIN] Iter: 47000 Loss: 0.01312417054567832  PSNR: 22.06990623474121\n","[TRAIN] Iter: 47100 Loss: 0.013769785728635082  PSNR: 21.78103256225586\n","[TRAIN] Iter: 47200 Loss: 0.012006658822853906  PSNR: 22.36750030517578\n","[TRAIN] Iter: 47300 Loss: 0.013163262337317162  PSNR: 21.934894561767578\n","[TRAIN] Iter: 47400 Loss: 0.012571720036143728  PSNR: 22.214431762695312\n","[TRAIN] Iter: 47500 Loss: 0.013442744325895397  PSNR: 21.84874153137207\n","[TRAIN] Iter: 47600 Loss: 0.013404349690841913  PSNR: 21.894929885864258\n","[TRAIN] Iter: 47700 Loss: 0.012465634592601985  PSNR: 22.227012634277344\n","[TRAIN] Iter: 47800 Loss: 0.013334951561375218  PSNR: 21.904325485229492\n","[TRAIN] Iter: 47900 Loss: 0.013703343343590435  PSNR: 21.82822036743164\n","[TRAIN] Iter: 48000 Loss: 0.012479796625311563  PSNR: 22.33094024658203\n","[TRAIN] Iter: 48100 Loss: 0.012303193182463507  PSNR: 22.27789878845215\n","[TRAIN] Iter: 48200 Loss: 0.013610480959025954  PSNR: 21.884225845336914\n","[TRAIN] Iter: 48300 Loss: 0.014009341239212249  PSNR: 21.739614486694336\n","[TRAIN] Iter: 48400 Loss: 0.01391345557224208  PSNR: 21.774730682373047\n","[TRAIN] Iter: 48500 Loss: 0.013842395653119375  PSNR: 21.855606079101562\n","[TRAIN] Iter: 48600 Loss: 0.013061142262878571  PSNR: 22.044851303100586\n","[TRAIN] Iter: 48700 Loss: 0.012642726854261047  PSNR: 22.153316497802734\n","[TRAIN] Iter: 48800 Loss: 0.012918574327392157  PSNR: 22.076080322265625\n","[TRAIN] Iter: 48900 Loss: 0.01348596239226904  PSNR: 21.868417739868164\n","[TRAIN] Iter: 49000 Loss: 0.01423452111830567  PSNR: 21.64696502685547\n","[TRAIN] Iter: 49100 Loss: 0.0129662789323184  PSNR: 22.02167510986328\n","[TRAIN] Iter: 49200 Loss: 0.01361111784920678  PSNR: 21.770631790161133\n","[TRAIN] Iter: 49300 Loss: 0.014399408087344679  PSNR: 21.61574363708496\n","[TRAIN] Iter: 49400 Loss: 0.0128651845080962  PSNR: 22.1033935546875\n","[TRAIN] Iter: 49500 Loss: 0.013877025364237777  PSNR: 21.712852478027344\n","[TRAIN] Iter: 49600 Loss: 0.013768128788949167  PSNR: 21.80194664001465\n","[TRAIN] Iter: 49700 Loss: 0.014701909106089523  PSNR: 21.519372940063477\n","[TRAIN] Iter: 49800 Loss: 0.012522184009762925  PSNR: 22.227346420288086\n","[TRAIN] Iter: 49900 Loss: 0.013528060582467626  PSNR: 21.901988983154297\n"," 25% 49999/200000 [3:52:39<11:36:35,  3.59it/s]Saved checkpoints at ./logs/56leonard_test_st0/050000.tar\n","[TRAIN] Iter: 50000 Loss: 0.012463500950285287  PSNR: 22.18860626220703\n","[TRAIN] Iter: 50100 Loss: 0.014161540718848984  PSNR: 21.714906692504883\n","[TRAIN] Iter: 50200 Loss: 0.013011115902064815  PSNR: 22.033327102661133\n","[TRAIN] Iter: 50300 Loss: 0.013494490218649138  PSNR: 21.828157424926758\n","[TRAIN] Iter: 50400 Loss: 0.013093865976528566  PSNR: 22.05995750427246\n","[TRAIN] Iter: 50500 Loss: 0.012882504774947979  PSNR: 22.063289642333984\n","[TRAIN] Iter: 50600 Loss: 0.013288994321028213  PSNR: 21.95975685119629\n","[TRAIN] Iter: 50700 Loss: 0.012220306113245614  PSNR: 22.35150718688965\n","[TRAIN] Iter: 50800 Loss: 0.013018242929640513  PSNR: 22.07085609436035\n","[TRAIN] Iter: 50900 Loss: 0.012896471828082224  PSNR: 22.045141220092773\n","[TRAIN] Iter: 51000 Loss: 0.013716760310350653  PSNR: 21.830259323120117\n","[TRAIN] Iter: 51100 Loss: 0.013530091833949974  PSNR: 21.945175170898438\n","[TRAIN] Iter: 51200 Loss: 0.014106945514192426  PSNR: 21.654672622680664\n","[TRAIN] Iter: 51300 Loss: 0.013100002932256612  PSNR: 22.01042938232422\n","[TRAIN] Iter: 51400 Loss: 0.013831352974223627  PSNR: 21.73387336730957\n","[TRAIN] Iter: 51500 Loss: 0.013860425426139474  PSNR: 21.84693145751953\n","[TRAIN] Iter: 51600 Loss: 0.013169115754344912  PSNR: 21.987112045288086\n","[TRAIN] Iter: 51700 Loss: 0.013045849576483033  PSNR: 21.995269775390625\n","[TRAIN] Iter: 51800 Loss: 0.013122668581150227  PSNR: 21.978788375854492\n","[TRAIN] Iter: 51900 Loss: 0.013510232177955004  PSNR: 21.92981719970703\n","[TRAIN] Iter: 52000 Loss: 0.013899110410809868  PSNR: 21.79607582092285\n","[TRAIN] Iter: 52100 Loss: 0.012845031718533767  PSNR: 22.083133697509766\n","[TRAIN] Iter: 52200 Loss: 0.012430699972357198  PSNR: 22.279279708862305\n","[TRAIN] Iter: 52300 Loss: 0.012310044664954294  PSNR: 22.319061279296875\n","[TRAIN] Iter: 52400 Loss: 0.013317649097239884  PSNR: 21.914264678955078\n","[TRAIN] Iter: 52500 Loss: 0.014083377551389229  PSNR: 21.71058464050293\n","[TRAIN] Iter: 52600 Loss: 0.013897892768331956  PSNR: 21.745742797851562\n","[TRAIN] Iter: 52700 Loss: 0.013190568418247478  PSNR: 21.99301528930664\n","[TRAIN] Iter: 52800 Loss: 0.013653248923240763  PSNR: 21.834991455078125\n","[TRAIN] Iter: 52900 Loss: 0.013692854332552008  PSNR: 21.82465171813965\n","[TRAIN] Iter: 53000 Loss: 0.013994706587593934  PSNR: 21.725711822509766\n","[TRAIN] Iter: 53100 Loss: 0.012604842835374351  PSNR: 22.201953887939453\n","[TRAIN] Iter: 53200 Loss: 0.012471918647856987  PSNR: 22.23442840576172\n","[TRAIN] Iter: 53300 Loss: 0.0139588555616494  PSNR: 21.704442977905273\n","[TRAIN] Iter: 53400 Loss: 0.013146954410179473  PSNR: 22.024795532226562\n","[TRAIN] Iter: 53500 Loss: 0.012881954698921835  PSNR: 22.182308197021484\n","[TRAIN] Iter: 53600 Loss: 0.01325499914624828  PSNR: 22.007232666015625\n","[TRAIN] Iter: 53700 Loss: 0.012987074019757412  PSNR: 22.154617309570312\n","[TRAIN] Iter: 53800 Loss: 0.013631732257540293  PSNR: 21.824024200439453\n","[TRAIN] Iter: 53900 Loss: 0.012257175907047408  PSNR: 22.342208862304688\n","[TRAIN] Iter: 54000 Loss: 0.01308767770905902  PSNR: 22.026473999023438\n","[TRAIN] Iter: 54100 Loss: 0.012978929494862915  PSNR: 22.159025192260742\n","[TRAIN] Iter: 54200 Loss: 0.013737787275011144  PSNR: 21.854751586914062\n","[TRAIN] Iter: 54300 Loss: 0.012944970073373932  PSNR: 22.042945861816406\n","[TRAIN] Iter: 54400 Loss: 0.013184808511503984  PSNR: 21.92354393005371\n","[TRAIN] Iter: 54500 Loss: 0.013655516690069614  PSNR: 21.870412826538086\n","[TRAIN] Iter: 54600 Loss: 0.012688491387986015  PSNR: 22.13343048095703\n","[TRAIN] Iter: 54700 Loss: 0.013610603184865597  PSNR: 21.84842872619629\n","[TRAIN] Iter: 54800 Loss: 0.014479078748280458  PSNR: 21.610456466674805\n","[TRAIN] Iter: 54900 Loss: 0.01355615726748736  PSNR: 21.863536834716797\n","[TRAIN] Iter: 55000 Loss: 0.014461428408188913  PSNR: 21.58512306213379\n","[TRAIN] Iter: 55100 Loss: 0.012794184832792276  PSNR: 22.12841796875\n","[TRAIN] Iter: 55200 Loss: 0.012842678531679514  PSNR: 22.06744956970215\n","[TRAIN] Iter: 55300 Loss: 0.012750308435220854  PSNR: 22.088592529296875\n","[TRAIN] Iter: 55400 Loss: 0.013458978031055528  PSNR: 21.87898063659668\n","[TRAIN] Iter: 55500 Loss: 0.01523641937989926  PSNR: 21.32098960876465\n","[TRAIN] Iter: 55600 Loss: 0.012959910251942824  PSNR: 22.135459899902344\n","[TRAIN] Iter: 55700 Loss: 0.013238677075505162  PSNR: 21.951961517333984\n","[TRAIN] Iter: 55800 Loss: 0.013138756183200345  PSNR: 21.987977981567383\n","[TRAIN] Iter: 55900 Loss: 0.012962357169319748  PSNR: 22.105087280273438\n","[TRAIN] Iter: 56000 Loss: 0.012675764761180586  PSNR: 22.161087036132812\n","[TRAIN] Iter: 56100 Loss: 0.01264144462868496  PSNR: 22.150705337524414\n","[TRAIN] Iter: 56200 Loss: 0.01244144990577776  PSNR: 22.176136016845703\n","[TRAIN] Iter: 56300 Loss: 0.013173558427313364  PSNR: 22.028642654418945\n","[TRAIN] Iter: 56400 Loss: 0.013851317574579353  PSNR: 21.74981689453125\n","[TRAIN] Iter: 56500 Loss: 0.014183433300389634  PSNR: 21.67388343811035\n","[TRAIN] Iter: 56600 Loss: 0.013057312860141956  PSNR: 21.97979164123535\n","[TRAIN] Iter: 56700 Loss: 0.013446656674553567  PSNR: 22.000083923339844\n","[TRAIN] Iter: 56800 Loss: 0.011587611217455175  PSNR: 22.54654312133789\n","[TRAIN] Iter: 56900 Loss: 0.014094742421474037  PSNR: 21.738828659057617\n","[TRAIN] Iter: 57000 Loss: 0.013453546336517182  PSNR: 21.918861389160156\n","[TRAIN] Iter: 57100 Loss: 0.013741359051171985  PSNR: 21.73124885559082\n","[TRAIN] Iter: 57200 Loss: 0.013370924581846792  PSNR: 21.935606002807617\n","[TRAIN] Iter: 57300 Loss: 0.013937735042543382  PSNR: 21.741844177246094\n","[TRAIN] Iter: 57400 Loss: 0.013503336955492506  PSNR: 21.907367706298828\n","[TRAIN] Iter: 57500 Loss: 0.013308186170094913  PSNR: 21.9883975982666\n","[TRAIN] Iter: 57600 Loss: 0.01302953202224025  PSNR: 22.071380615234375\n","[TRAIN] Iter: 57700 Loss: 0.01288322985560773  PSNR: 22.11262321472168\n","[TRAIN] Iter: 57800 Loss: 0.012951481001287875  PSNR: 22.071208953857422\n","[TRAIN] Iter: 57900 Loss: 0.014056755208214592  PSNR: 21.616392135620117\n","[TRAIN] Iter: 58000 Loss: 0.01284447866608551  PSNR: 22.155197143554688\n","[TRAIN] Iter: 58100 Loss: 0.012200111441784076  PSNR: 22.389541625976562\n","[TRAIN] Iter: 58200 Loss: 0.013584506738683139  PSNR: 21.838224411010742\n","[TRAIN] Iter: 58300 Loss: 0.012419362350894032  PSNR: 22.264902114868164\n","[TRAIN] Iter: 58400 Loss: 0.012612599297202808  PSNR: 22.137117385864258\n","[TRAIN] Iter: 58500 Loss: 0.013292834885372723  PSNR: 21.98988151550293\n","[TRAIN] Iter: 58600 Loss: 0.01187251818058193  PSNR: 22.375688552856445\n","[TRAIN] Iter: 58700 Loss: 0.011523383808679429  PSNR: 22.592700958251953\n","[TRAIN] Iter: 58800 Loss: 0.014374416976872285  PSNR: 21.648948669433594\n","[TRAIN] Iter: 58900 Loss: 0.013159776872203821  PSNR: 22.02078628540039\n","[TRAIN] Iter: 59000 Loss: 0.013246581106295816  PSNR: 21.995098114013672\n","[TRAIN] Iter: 59100 Loss: 0.012766183490340888  PSNR: 22.09136199951172\n","[TRAIN] Iter: 59200 Loss: 0.012533689882784626  PSNR: 22.213254928588867\n","[TRAIN] Iter: 59300 Loss: 0.01351180943959023  PSNR: 21.952239990234375\n"," 30% 59399/200000 [4:36:32<10:49:56,  3.61it/s]Shuffle data after an epoch!\n","[TRAIN] Iter: 59400 Loss: 0.013004957886749057  PSNR: 22.072858810424805\n","[TRAIN] Iter: 59500 Loss: 0.012190987256508544  PSNR: 22.31962776184082\n","[TRAIN] Iter: 59600 Loss: 0.01232844762850092  PSNR: 22.35594940185547\n","[TRAIN] Iter: 59700 Loss: 0.012024933832891799  PSNR: 22.469087600708008\n","[TRAIN] Iter: 59800 Loss: 0.012938479194608657  PSNR: 22.0596923828125\n","[TRAIN] Iter: 59900 Loss: 0.013188909066166144  PSNR: 22.01917266845703\n"," 30% 59999/200000 [4:39:26<10:54:11,  3.57it/s]Saved checkpoints at ./logs/56leonard_test_st0/060000.tar\n","[TRAIN] Iter: 60000 Loss: 0.012303558296426199  PSNR: 22.37242889404297\n","[TRAIN] Iter: 60100 Loss: 0.013476406031875278  PSNR: 21.94565773010254\n","[TRAIN] Iter: 60200 Loss: 0.013233369502074795  PSNR: 22.01879119873047\n","[TRAIN] Iter: 60300 Loss: 0.012635870591886948  PSNR: 22.2015380859375\n","[TRAIN] Iter: 60400 Loss: 0.012705898199017858  PSNR: 22.209918975830078\n","[TRAIN] Iter: 60500 Loss: 0.012614010091785243  PSNR: 22.214832305908203\n","[TRAIN] Iter: 60600 Loss: 0.012391847484032797  PSNR: 22.291427612304688\n","[TRAIN] Iter: 60700 Loss: 0.01237583002017476  PSNR: 22.278491973876953\n","[TRAIN] Iter: 60800 Loss: 0.012267461481566183  PSNR: 22.370830535888672\n","[TRAIN] Iter: 60900 Loss: 0.012493082126250533  PSNR: 22.203754425048828\n","[TRAIN] Iter: 61000 Loss: 0.011826663864789993  PSNR: 22.46126365661621\n","[TRAIN] Iter: 61100 Loss: 0.012904626556943662  PSNR: 22.052448272705078\n","[TRAIN] Iter: 61200 Loss: 0.013107655301481828  PSNR: 22.053325653076172\n","[TRAIN] Iter: 61300 Loss: 0.01180248046299734  PSNR: 22.410932540893555\n","[TRAIN] Iter: 61400 Loss: 0.012979631700685433  PSNR: 22.035930633544922\n","[TRAIN] Iter: 61500 Loss: 0.012258004103169785  PSNR: 22.330350875854492\n","[TRAIN] Iter: 61600 Loss: 0.012669729308072945  PSNR: 22.102584838867188\n","[TRAIN] Iter: 61700 Loss: 0.013502753605017839  PSNR: 21.85907745361328\n","[TRAIN] Iter: 61800 Loss: 0.012508710237526897  PSNR: 22.234342575073242\n","[TRAIN] Iter: 61900 Loss: 0.013491509984354819  PSNR: 21.831378936767578\n","[TRAIN] Iter: 62000 Loss: 0.012525195043936501  PSNR: 22.269893646240234\n","[TRAIN] Iter: 62100 Loss: 0.012255009644667213  PSNR: 22.3472843170166\n","[TRAIN] Iter: 62200 Loss: 0.01321503383776395  PSNR: 21.999427795410156\n","[TRAIN] Iter: 62300 Loss: 0.01273555604665175  PSNR: 22.123149871826172\n","[TRAIN] Iter: 62400 Loss: 0.01294117969295945  PSNR: 22.091934204101562\n","[TRAIN] Iter: 62500 Loss: 0.013092363696117217  PSNR: 22.018217086791992\n","[TRAIN] Iter: 62600 Loss: 0.013432712988107722  PSNR: 21.929012298583984\n","[TRAIN] Iter: 62700 Loss: 0.01287243740962825  PSNR: 22.130332946777344\n","[TRAIN] Iter: 62800 Loss: 0.01255293271633813  PSNR: 22.19919776916504\n","[TRAIN] Iter: 62900 Loss: 0.012354900707769158  PSNR: 22.295133590698242\n","[TRAIN] Iter: 63000 Loss: 0.012781415312848163  PSNR: 22.13736343383789\n","[TRAIN] Iter: 63100 Loss: 0.013483496282464699  PSNR: 21.949146270751953\n","[TRAIN] Iter: 63200 Loss: 0.012927092813147228  PSNR: 22.094755172729492\n","[TRAIN] Iter: 63300 Loss: 0.012846722287398658  PSNR: 22.072004318237305\n","[TRAIN] Iter: 63400 Loss: 0.013759510021458168  PSNR: 21.82076644897461\n","[TRAIN] Iter: 63500 Loss: 0.012961539566268256  PSNR: 22.076982498168945\n","[TRAIN] Iter: 63600 Loss: 0.012951200341848046  PSNR: 22.09952163696289\n","[TRAIN] Iter: 63700 Loss: 0.013054167473441524  PSNR: 22.089014053344727\n","[TRAIN] Iter: 63800 Loss: 0.01250422816518551  PSNR: 22.24372100830078\n","[TRAIN] Iter: 63900 Loss: 0.01244549058741408  PSNR: 22.310867309570312\n","[TRAIN] Iter: 64000 Loss: 0.012514409286736652  PSNR: 22.24547576904297\n","[TRAIN] Iter: 64100 Loss: 0.012297220079638408  PSNR: 22.323841094970703\n","[TRAIN] Iter: 64200 Loss: 0.012084256731874797  PSNR: 22.411252975463867\n","[TRAIN] Iter: 64300 Loss: 0.01316871581742625  PSNR: 22.11281967163086\n","[TRAIN] Iter: 64400 Loss: 0.012924084774551103  PSNR: 22.155893325805664\n","[TRAIN] Iter: 64500 Loss: 0.011846838967391343  PSNR: 22.40587615966797\n","[TRAIN] Iter: 64600 Loss: 0.012656690119516749  PSNR: 22.14238929748535\n","[TRAIN] Iter: 64700 Loss: 0.012239795228386002  PSNR: 22.337556838989258\n","[TRAIN] Iter: 64800 Loss: 0.012423068482653581  PSNR: 22.260507583618164\n","[TRAIN] Iter: 64900 Loss: 0.01280633970635611  PSNR: 22.16633415222168\n","[TRAIN] Iter: 65000 Loss: 0.013032777671285242  PSNR: 22.043222427368164\n","[TRAIN] Iter: 65100 Loss: 0.012679736787438967  PSNR: 22.128215789794922\n","[TRAIN] Iter: 65200 Loss: 0.01187875972117445  PSNR: 22.40302848815918\n","[TRAIN] Iter: 65300 Loss: 0.01196663305279489  PSNR: 22.47402000427246\n","[TRAIN] Iter: 65400 Loss: 0.011832582787709413  PSNR: 22.453033447265625\n","[TRAIN] Iter: 65500 Loss: 0.01292639070553421  PSNR: 22.156780242919922\n","[TRAIN] Iter: 65600 Loss: 0.01322204177142763  PSNR: 21.957366943359375\n","[TRAIN] Iter: 65700 Loss: 0.012712977419529287  PSNR: 22.242948532104492\n","[TRAIN] Iter: 65800 Loss: 0.01313061979162171  PSNR: 22.060251235961914\n","[TRAIN] Iter: 65900 Loss: 0.012869891737754525  PSNR: 22.145322799682617\n","[TRAIN] Iter: 66000 Loss: 0.012684065569389155  PSNR: 22.15997886657715\n","[TRAIN] Iter: 66100 Loss: 0.012849450226152095  PSNR: 22.16811180114746\n","[TRAIN] Iter: 66200 Loss: 0.012858051304854314  PSNR: 22.07699203491211\n","[TRAIN] Iter: 66300 Loss: 0.011719985123058012  PSNR: 22.49375343322754\n","[TRAIN] Iter: 66400 Loss: 0.014087750383552077  PSNR: 21.716838836669922\n","[TRAIN] Iter: 66500 Loss: 0.013766607605856044  PSNR: 21.820354461669922\n","[TRAIN] Iter: 66600 Loss: 0.011929807457684401  PSNR: 22.471561431884766\n","[TRAIN] Iter: 66700 Loss: 0.013488383354190834  PSNR: 21.919353485107422\n","[TRAIN] Iter: 66800 Loss: 0.013119545017818828  PSNR: 22.02095603942871\n","[TRAIN] Iter: 66900 Loss: 0.01251870337027431  PSNR: 22.217744827270508\n","[TRAIN] Iter: 67000 Loss: 0.011831649341018845  PSNR: 22.468204498291016\n","[TRAIN] Iter: 67100 Loss: 0.012410869522003194  PSNR: 22.301082611083984\n","[TRAIN] Iter: 67200 Loss: 0.011904479708686164  PSNR: 22.44002342224121\n","[TRAIN] Iter: 67300 Loss: 0.012060009369750702  PSNR: 22.38326072692871\n","[TRAIN] Iter: 67400 Loss: 0.012101602843002511  PSNR: 22.392370223999023\n","[TRAIN] Iter: 67500 Loss: 0.01218489674771724  PSNR: 22.335437774658203\n","[TRAIN] Iter: 67600 Loss: 0.012910894661194536  PSNR: 22.154760360717773\n","[TRAIN] Iter: 67700 Loss: 0.013814850478535493  PSNR: 21.7645263671875\n","[TRAIN] Iter: 67800 Loss: 0.012946662005519574  PSNR: 22.094470977783203\n","[TRAIN] Iter: 67900 Loss: 0.011956752586270793  PSNR: 22.40654945373535\n","[TRAIN] Iter: 68000 Loss: 0.013053012638050273  PSNR: 22.047380447387695\n","[TRAIN] Iter: 68100 Loss: 0.012715325224537449  PSNR: 22.177642822265625\n","[TRAIN] Iter: 68200 Loss: 0.012470795726258312  PSNR: 22.237220764160156\n","[TRAIN] Iter: 68300 Loss: 0.012828020722602362  PSNR: 22.104814529418945\n","[TRAIN] Iter: 68400 Loss: 0.011442607021808928  PSNR: 22.623483657836914\n","[TRAIN] Iter: 68500 Loss: 0.012175677251902661  PSNR: 22.312711715698242\n","[TRAIN] Iter: 68600 Loss: 0.01263463008750877  PSNR: 22.23671531677246\n","[TRAIN] Iter: 68700 Loss: 0.012535408636798053  PSNR: 22.218263626098633\n","[TRAIN] Iter: 68800 Loss: 0.012493535054710774  PSNR: 22.211275100708008\n","[TRAIN] Iter: 68900 Loss: 0.012881656055276658  PSNR: 22.14023780822754\n","[TRAIN] Iter: 69000 Loss: 0.012714238759577282  PSNR: 22.208593368530273\n","[TRAIN] Iter: 69100 Loss: 0.011957362999319777  PSNR: 22.48636245727539\n","[TRAIN] Iter: 69200 Loss: 0.012827103895350248  PSNR: 22.216415405273438\n","[TRAIN] Iter: 69300 Loss: 0.013005361592781029  PSNR: 22.09744644165039\n","[TRAIN] Iter: 69400 Loss: 0.012768285154934918  PSNR: 22.109344482421875\n","[TRAIN] Iter: 69500 Loss: 0.012235733669015168  PSNR: 22.38027000427246\n","[TRAIN] Iter: 69600 Loss: 0.011895374523855019  PSNR: 22.473426818847656\n","[TRAIN] Iter: 69700 Loss: 0.013404844481785558  PSNR: 21.97492790222168\n","[TRAIN] Iter: 69800 Loss: 0.011676649622139459  PSNR: 22.496652603149414\n","[TRAIN] Iter: 69900 Loss: 0.011740652067738291  PSNR: 22.579252243041992\n"," 35% 69999/200000 [5:26:05<10:08:05,  3.56it/s]Saved checkpoints at ./logs/56leonard_test_st0/070000.tar\n","[TRAIN] Iter: 70000 Loss: 0.013049133166115171  PSNR: 21.99203872680664\n","[TRAIN] Iter: 70100 Loss: 0.013292194450437382  PSNR: 21.953771591186523\n","[TRAIN] Iter: 70200 Loss: 0.011493206376144424  PSNR: 22.582181930541992\n","[TRAIN] Iter: 70300 Loss: 0.01218149688271231  PSNR: 22.34708595275879\n","[TRAIN] Iter: 70400 Loss: 0.012764107513239876  PSNR: 22.140789031982422\n","[TRAIN] Iter: 70500 Loss: 0.012074979973121212  PSNR: 22.35814666748047\n","[TRAIN] Iter: 70600 Loss: 0.012870492906847924  PSNR: 22.090496063232422\n","[TRAIN] Iter: 70700 Loss: 0.012667566484286158  PSNR: 22.16342544555664\n","[TRAIN] Iter: 70800 Loss: 0.011858230792639049  PSNR: 22.44649314880371\n","[TRAIN] Iter: 70900 Loss: 0.012775026411572719  PSNR: 22.220434188842773\n","[TRAIN] Iter: 71000 Loss: 0.01403091008543004  PSNR: 21.732685089111328\n","[TRAIN] Iter: 71100 Loss: 0.012729821039046869  PSNR: 22.157024383544922\n","[TRAIN] Iter: 71200 Loss: 0.01253953451853345  PSNR: 22.297374725341797\n","[TRAIN] Iter: 71300 Loss: 0.01284957704450488  PSNR: 22.137418746948242\n","[TRAIN] Iter: 71400 Loss: 0.01313536489980942  PSNR: 22.037925720214844\n","[TRAIN] Iter: 71500 Loss: 0.012201918759561886  PSNR: 22.308958053588867\n","[TRAIN] Iter: 71600 Loss: 0.012793604150841005  PSNR: 22.140283584594727\n","[TRAIN] Iter: 71700 Loss: 0.011171844192265402  PSNR: 22.712862014770508\n","[TRAIN] Iter: 71800 Loss: 0.013222333835370817  PSNR: 21.958942413330078\n","[TRAIN] Iter: 71900 Loss: 0.012239183936724799  PSNR: 22.380569458007812\n","[TRAIN] Iter: 72000 Loss: 0.011824322220603244  PSNR: 22.530569076538086\n","[TRAIN] Iter: 72100 Loss: 0.012469177348201658  PSNR: 22.28079605102539\n","[TRAIN] Iter: 72200 Loss: 0.012665414930896696  PSNR: 22.165302276611328\n","[TRAIN] Iter: 72300 Loss: 0.013576843372432975  PSNR: 21.905086517333984\n","[TRAIN] Iter: 72400 Loss: 0.012057996872167808  PSNR: 22.404239654541016\n","[TRAIN] Iter: 72500 Loss: 0.011810026101933675  PSNR: 22.47698974609375\n","[TRAIN] Iter: 72600 Loss: 0.011795084223901554  PSNR: 22.472496032714844\n","[TRAIN] Iter: 72700 Loss: 0.013307611344674203  PSNR: 21.947404861450195\n","[TRAIN] Iter: 72800 Loss: 0.012657987373090575  PSNR: 22.15334129333496\n","[TRAIN] Iter: 72900 Loss: 0.011799652920335075  PSNR: 22.464824676513672\n","[TRAIN] Iter: 73000 Loss: 0.012273276965574089  PSNR: 22.31403160095215\n","[TRAIN] Iter: 73100 Loss: 0.012433453235996306  PSNR: 22.289018630981445\n","[TRAIN] Iter: 73200 Loss: 0.01336153754499768  PSNR: 21.93476676940918\n","[TRAIN] Iter: 73300 Loss: 0.011787435582184395  PSNR: 22.509784698486328\n","[TRAIN] Iter: 73400 Loss: 0.012558188043364473  PSNR: 22.197479248046875\n","[TRAIN] Iter: 73500 Loss: 0.01186003545503991  PSNR: 22.491416931152344\n","[TRAIN] Iter: 73600 Loss: 0.012983970785120575  PSNR: 22.153003692626953\n","[TRAIN] Iter: 73700 Loss: 0.012536029464062426  PSNR: 22.258777618408203\n","[TRAIN] Iter: 73800 Loss: 0.013833769491724438  PSNR: 21.780527114868164\n","[TRAIN] Iter: 73900 Loss: 0.0129452821131811  PSNR: 22.119070053100586\n","[TRAIN] Iter: 74000 Loss: 0.012376991522915073  PSNR: 22.289047241210938\n","[TRAIN] Iter: 74100 Loss: 0.011969200684859335  PSNR: 22.44576644897461\n","[TRAIN] Iter: 74200 Loss: 0.012426739813349161  PSNR: 22.291337966918945\n","[TRAIN] Iter: 74300 Loss: 0.012342825206473569  PSNR: 22.289451599121094\n","[TRAIN] Iter: 74400 Loss: 0.011845800848414703  PSNR: 22.385990142822266\n","[TRAIN] Iter: 74500 Loss: 0.01223152302387016  PSNR: 22.36342430114746\n","[TRAIN] Iter: 74600 Loss: 0.012835909011734419  PSNR: 22.13172721862793\n","[TRAIN] Iter: 74700 Loss: 0.012264165697352305  PSNR: 22.253833770751953\n","[TRAIN] Iter: 74800 Loss: 0.011680137146651986  PSNR: 22.56973648071289\n","[TRAIN] Iter: 74900 Loss: 0.011406772372600543  PSNR: 22.65705680847168\n","[TRAIN] Iter: 75000 Loss: 0.012376051930114982  PSNR: 22.31269645690918\n","[TRAIN] Iter: 75100 Loss: 0.012593064275391943  PSNR: 22.186555862426758\n","[TRAIN] Iter: 75200 Loss: 0.013255423125309487  PSNR: 21.924903869628906\n","[TRAIN] Iter: 75300 Loss: 0.012382878916785127  PSNR: 22.27182388305664\n","[TRAIN] Iter: 75400 Loss: 0.012430931550490746  PSNR: 22.301233291625977\n","[TRAIN] Iter: 75500 Loss: 0.012889589106302477  PSNR: 22.065967559814453\n","[TRAIN] Iter: 75600 Loss: 0.012684549769944669  PSNR: 22.189167022705078\n","[TRAIN] Iter: 75700 Loss: 0.013063254995730515  PSNR: 22.09649085998535\n","[TRAIN] Iter: 75800 Loss: 0.01201952150620201  PSNR: 22.488819122314453\n","[TRAIN] Iter: 75900 Loss: 0.013063742722002893  PSNR: 22.109079360961914\n","[TRAIN] Iter: 76000 Loss: 0.012268889547565642  PSNR: 22.405750274658203\n","[TRAIN] Iter: 76100 Loss: 0.012242896526892935  PSNR: 22.29000473022461\n","[TRAIN] Iter: 76200 Loss: 0.01226548438506074  PSNR: 22.38108253479004\n","[TRAIN] Iter: 76300 Loss: 0.012326163304527678  PSNR: 22.268577575683594\n","[TRAIN] Iter: 76400 Loss: 0.011579240299561524  PSNR: 22.559112548828125\n","[TRAIN] Iter: 76500 Loss: 0.012789183430734374  PSNR: 22.053279876708984\n","[TRAIN] Iter: 76600 Loss: 0.012938300720829977  PSNR: 22.099401473999023\n","[TRAIN] Iter: 76700 Loss: 0.012597385386450924  PSNR: 22.132402420043945\n","[TRAIN] Iter: 76800 Loss: 0.012705564355129796  PSNR: 22.20201301574707\n","[TRAIN] Iter: 76900 Loss: 0.011696968551021106  PSNR: 22.558286666870117\n","[TRAIN] Iter: 77000 Loss: 0.013271240862675171  PSNR: 22.004653930664062\n","[TRAIN] Iter: 77100 Loss: 0.012164892849457865  PSNR: 22.421236038208008\n","[TRAIN] Iter: 77200 Loss: 0.01214839154763009  PSNR: 22.3475284576416\n","[TRAIN] Iter: 77300 Loss: 0.013091264539231626  PSNR: 22.031356811523438\n","[TRAIN] Iter: 77400 Loss: 0.012633522505256522  PSNR: 22.19924545288086\n","[TRAIN] Iter: 77500 Loss: 0.011867964970392771  PSNR: 22.45988655090332\n","[TRAIN] Iter: 77600 Loss: 0.012330253580243838  PSNR: 22.333467483520508\n","[TRAIN] Iter: 77700 Loss: 0.01340018689319248  PSNR: 22.04401397705078\n","[TRAIN] Iter: 77800 Loss: 0.012128695951223508  PSNR: 22.364913940429688\n","[TRAIN] Iter: 77900 Loss: 0.01237758399028616  PSNR: 22.190996170043945\n","[TRAIN] Iter: 78000 Loss: 0.011714672884824797  PSNR: 22.48859977722168\n","[TRAIN] Iter: 78100 Loss: 0.012369653830162522  PSNR: 22.38945770263672\n","[TRAIN] Iter: 78200 Loss: 0.01314820467190812  PSNR: 22.03302001953125\n","[TRAIN] Iter: 78300 Loss: 0.011910055353788602  PSNR: 22.49761962890625\n","[TRAIN] Iter: 78400 Loss: 0.011555862798802436  PSNR: 22.67736053466797\n","[TRAIN] Iter: 78500 Loss: 0.01239937921797409  PSNR: 22.218629837036133\n","[TRAIN] Iter: 78600 Loss: 0.011607613543240143  PSNR: 22.507841110229492\n","[TRAIN] Iter: 78700 Loss: 0.011925573862501582  PSNR: 22.515348434448242\n","[TRAIN] Iter: 78800 Loss: 0.011325454411222784  PSNR: 22.718114852905273\n","[TRAIN] Iter: 78900 Loss: 0.01276682128027318  PSNR: 22.109115600585938\n","[TRAIN] Iter: 79000 Loss: 0.011773028788603361  PSNR: 22.50199317932129\n","[TRAIN] Iter: 79100 Loss: 0.01117149216930777  PSNR: 22.722026824951172\n"," 40% 79199/200000 [6:08:53<9:32:18,  3.52it/s]Shuffle data after an epoch!\n","[TRAIN] Iter: 79200 Loss: 0.011363470599259474  PSNR: 22.726659774780273\n","[TRAIN] Iter: 79300 Loss: 0.012523829330166773  PSNR: 22.31672477722168\n","[TRAIN] Iter: 79400 Loss: 0.011227227432898935  PSNR: 22.727081298828125\n","[TRAIN] Iter: 79500 Loss: 0.011514899853041948  PSNR: 22.588491439819336\n","[TRAIN] Iter: 79600 Loss: 0.01181705557277302  PSNR: 22.550884246826172\n","[TRAIN] Iter: 79700 Loss: 0.01288951495640039  PSNR: 22.20772361755371\n","[TRAIN] Iter: 79800 Loss: 0.012048068542996216  PSNR: 22.48479652404785\n","[TRAIN] Iter: 79900 Loss: 0.012041018697275377  PSNR: 22.51042938232422\n"," 40% 79999/200000 [6:12:42<9:17:32,  3.59it/s]Saved checkpoints at ./logs/56leonard_test_st0/080000.tar\n","[TRAIN] Iter: 80000 Loss: 0.011063378656837632  PSNR: 22.814937591552734\n","[TRAIN] Iter: 80100 Loss: 0.011839336047498879  PSNR: 22.466285705566406\n","[TRAIN] Iter: 80200 Loss: 0.011885317721028426  PSNR: 22.400758743286133\n","[TRAIN] Iter: 80300 Loss: 0.01249828869998338  PSNR: 22.281862258911133\n","[TRAIN] Iter: 80400 Loss: 0.012668705026692074  PSNR: 22.239715576171875\n","[TRAIN] Iter: 80500 Loss: 0.011525014383846224  PSNR: 22.6992130279541\n","[TRAIN] Iter: 80600 Loss: 0.012776992663311046  PSNR: 22.163352966308594\n","[TRAIN] Iter: 80700 Loss: 0.011266060716138936  PSNR: 22.651206970214844\n","[TRAIN] Iter: 80800 Loss: 0.010519091246474937  PSNR: 22.986330032348633\n","[TRAIN] Iter: 80900 Loss: 0.012629893386956541  PSNR: 22.307741165161133\n","[TRAIN] Iter: 81000 Loss: 0.013320052226878414  PSNR: 21.965526580810547\n","[TRAIN] Iter: 81100 Loss: 0.011447723576914676  PSNR: 22.623056411743164\n","[TRAIN] Iter: 81200 Loss: 0.011497153209766423  PSNR: 22.623233795166016\n","[TRAIN] Iter: 81300 Loss: 0.012762471325745574  PSNR: 22.16393280029297\n","[TRAIN] Iter: 81400 Loss: 0.012466198085152678  PSNR: 22.263673782348633\n","[TRAIN] Iter: 81500 Loss: 0.012619358038147082  PSNR: 22.22899627685547\n","[TRAIN] Iter: 81600 Loss: 0.012525306045961143  PSNR: 22.29310417175293\n","[TRAIN] Iter: 81700 Loss: 0.012450674888326959  PSNR: 22.26778793334961\n","[TRAIN] Iter: 81800 Loss: 0.011904790383593213  PSNR: 22.42057991027832\n","[TRAIN] Iter: 81900 Loss: 0.012545261894013001  PSNR: 22.223312377929688\n","[TRAIN] Iter: 82000 Loss: 0.011625937507043868  PSNR: 22.754392623901367\n","[TRAIN] Iter: 82100 Loss: 0.011635124326263648  PSNR: 22.530681610107422\n","[TRAIN] Iter: 82200 Loss: 0.011895197017067628  PSNR: 22.40976333618164\n","[TRAIN] Iter: 82300 Loss: 0.01247189895995857  PSNR: 22.215497970581055\n","[TRAIN] Iter: 82400 Loss: 0.01305931001966646  PSNR: 22.022836685180664\n","[TRAIN] Iter: 82500 Loss: 0.012900673152678972  PSNR: 22.05734634399414\n","[TRAIN] Iter: 82600 Loss: 0.012601128542324282  PSNR: 22.240345001220703\n","[TRAIN] Iter: 82700 Loss: 0.012263662483848073  PSNR: 22.329668045043945\n","[TRAIN] Iter: 82800 Loss: 0.012444788466827564  PSNR: 22.264158248901367\n","[TRAIN] Iter: 82900 Loss: 0.012634564304881244  PSNR: 22.28451156616211\n","[TRAIN] Iter: 83000 Loss: 0.011699025605346875  PSNR: 22.532649993896484\n","[TRAIN] Iter: 83100 Loss: 0.012137411145302293  PSNR: 22.38968276977539\n","[TRAIN] Iter: 83200 Loss: 0.012534899105072448  PSNR: 22.20810890197754\n","[TRAIN] Iter: 83300 Loss: 0.0119067434515734  PSNR: 22.551105499267578\n","[TRAIN] Iter: 83400 Loss: 0.011973646524630585  PSNR: 22.443273544311523\n","[TRAIN] Iter: 83500 Loss: 0.011797169869437647  PSNR: 22.57925033569336\n","[TRAIN] Iter: 83600 Loss: 0.012287962268333615  PSNR: 22.37571907043457\n","[TRAIN] Iter: 83700 Loss: 0.012612390329641437  PSNR: 22.230693817138672\n","[TRAIN] Iter: 83800 Loss: 0.013085756462977972  PSNR: 21.990060806274414\n","[TRAIN] Iter: 83900 Loss: 0.012380529493441216  PSNR: 22.29825210571289\n","[TRAIN] Iter: 84000 Loss: 0.012933682439610347  PSNR: 22.124662399291992\n","[TRAIN] Iter: 84100 Loss: 0.01169774072251389  PSNR: 22.482032775878906\n","[TRAIN] Iter: 84200 Loss: 0.011517200180914596  PSNR: 22.62777328491211\n","[TRAIN] Iter: 84300 Loss: 0.012881040389118623  PSNR: 22.141813278198242\n","[TRAIN] Iter: 84400 Loss: 0.011399940767137448  PSNR: 22.587154388427734\n","[TRAIN] Iter: 84500 Loss: 0.011971938909971825  PSNR: 22.429561614990234\n","[TRAIN] Iter: 84600 Loss: 0.012880947214208759  PSNR: 22.12572479248047\n","[TRAIN] Iter: 84700 Loss: 0.012457055966594008  PSNR: 22.310033798217773\n","[TRAIN] Iter: 84800 Loss: 0.012002302481284453  PSNR: 22.479686737060547\n","[TRAIN] Iter: 84900 Loss: 0.012272613002578758  PSNR: 22.316755294799805\n","[TRAIN] Iter: 85000 Loss: 0.011728523232316348  PSNR: 22.505800247192383\n","[TRAIN] Iter: 85100 Loss: 0.011747742423638036  PSNR: 22.585308074951172\n","[TRAIN] Iter: 85200 Loss: 0.012625824635181205  PSNR: 22.278650283813477\n","[TRAIN] Iter: 85300 Loss: 0.011988856221310308  PSNR: 22.48594093322754\n","[TRAIN] Iter: 85400 Loss: 0.011514229848219367  PSNR: 22.559947967529297\n","[TRAIN] Iter: 85500 Loss: 0.011805428048997065  PSNR: 22.543176651000977\n","[TRAIN] Iter: 85600 Loss: 0.01332941671121398  PSNR: 21.998043060302734\n","[TRAIN] Iter: 85700 Loss: 0.012278223361150047  PSNR: 22.287216186523438\n","[TRAIN] Iter: 85800 Loss: 0.011548332881074643  PSNR: 22.582792282104492\n","[TRAIN] Iter: 85900 Loss: 0.011707952477440582  PSNR: 22.580469131469727\n","[TRAIN] Iter: 86000 Loss: 0.012528562817830902  PSNR: 22.207096099853516\n","[TRAIN] Iter: 86100 Loss: 0.012495207540720563  PSNR: 22.298463821411133\n","[TRAIN] Iter: 86200 Loss: 0.011416801106014892  PSNR: 22.657684326171875\n","[TRAIN] Iter: 86300 Loss: 0.01164889940844926  PSNR: 22.57231330871582\n","[TRAIN] Iter: 86400 Loss: 0.011557281419198326  PSNR: 22.610092163085938\n","[TRAIN] Iter: 86500 Loss: 0.012800412629836793  PSNR: 22.187545776367188\n","[TRAIN] Iter: 86600 Loss: 0.012427530912693809  PSNR: 22.262611389160156\n","[TRAIN] Iter: 86700 Loss: 0.011156675255026238  PSNR: 22.70033073425293\n","[TRAIN] Iter: 86800 Loss: 0.012348440969305414  PSNR: 22.284286499023438\n","[TRAIN] Iter: 86900 Loss: 0.012268077560464243  PSNR: 22.371734619140625\n","[TRAIN] Iter: 87000 Loss: 0.011514231763632265  PSNR: 22.67850112915039\n","[TRAIN] Iter: 87100 Loss: 0.012700773495166393  PSNR: 22.194791793823242\n","[TRAIN] Iter: 87200 Loss: 0.01267575925507668  PSNR: 22.155746459960938\n","[TRAIN] Iter: 87300 Loss: 0.01211837978596407  PSNR: 22.411380767822266\n","[TRAIN] Iter: 87400 Loss: 0.011256183573928363  PSNR: 22.653533935546875\n","[TRAIN] Iter: 87500 Loss: 0.0109318780716397  PSNR: 22.88893699645996\n","[TRAIN] Iter: 87600 Loss: 0.012464031307657601  PSNR: 22.299474716186523\n","[TRAIN] Iter: 87700 Loss: 0.013393814765848288  PSNR: 21.991867065429688\n","[TRAIN] Iter: 87800 Loss: 0.011954415489238326  PSNR: 22.41326141357422\n","[TRAIN] Iter: 87900 Loss: 0.011839022098591969  PSNR: 22.513586044311523\n","[TRAIN] Iter: 88000 Loss: 0.013040189158232466  PSNR: 22.125932693481445\n","[TRAIN] Iter: 88100 Loss: 0.012940890374410523  PSNR: 22.133928298950195\n","[TRAIN] Iter: 88200 Loss: 0.011595242305370144  PSNR: 22.557310104370117\n","[TRAIN] Iter: 88300 Loss: 0.013290229685670459  PSNR: 22.03009796142578\n","[TRAIN] Iter: 88400 Loss: 0.01230717893701197  PSNR: 22.362903594970703\n","[TRAIN] Iter: 88500 Loss: 0.0114090982942767  PSNR: 22.6803035736084\n","[TRAIN] Iter: 88600 Loss: 0.01184828624958557  PSNR: 22.552366256713867\n","[TRAIN] Iter: 88700 Loss: 0.011605703611174346  PSNR: 22.637500762939453\n","[TRAIN] Iter: 88800 Loss: 0.01185061504480077  PSNR: 22.551380157470703\n","[TRAIN] Iter: 88900 Loss: 0.01194249676429814  PSNR: 22.459047317504883\n","[TRAIN] Iter: 89000 Loss: 0.012202336791539665  PSNR: 22.376317977905273\n","[TRAIN] Iter: 89100 Loss: 0.011449585284238596  PSNR: 22.647356033325195\n","[TRAIN] Iter: 89200 Loss: 0.012727404035895964  PSNR: 22.21126365661621\n","[TRAIN] Iter: 89300 Loss: 0.012375264323758267  PSNR: 22.277437210083008\n","[TRAIN] Iter: 89400 Loss: 0.01175265084489996  PSNR: 22.537382125854492\n","[TRAIN] Iter: 89500 Loss: 0.012000432373911529  PSNR: 22.542085647583008\n","[TRAIN] Iter: 89600 Loss: 0.012018839038804132  PSNR: 22.44635009765625\n","[TRAIN] Iter: 89700 Loss: 0.012192863026629912  PSNR: 22.364112854003906\n","[TRAIN] Iter: 89800 Loss: 0.012506191076525855  PSNR: 22.174030303955078\n","[TRAIN] Iter: 89900 Loss: 0.012883736987655792  PSNR: 22.209312438964844\n"," 45% 89999/200000 [6:59:10<8:35:37,  3.56it/s]Saved checkpoints at ./logs/56leonard_test_st0/090000.tar\n","[TRAIN] Iter: 90000 Loss: 0.012711009279146823  PSNR: 22.30867576599121\n","[TRAIN] Iter: 90100 Loss: 0.011506834510666104  PSNR: 22.647512435913086\n","[TRAIN] Iter: 90200 Loss: 0.011677167004302056  PSNR: 22.544721603393555\n","[TRAIN] Iter: 90300 Loss: 0.011277736842060945  PSNR: 22.67990493774414\n","[TRAIN] Iter: 90400 Loss: 0.011570616009466357  PSNR: 22.619966506958008\n","[TRAIN] Iter: 90500 Loss: 0.012152685975013375  PSNR: 22.500648498535156\n","[TRAIN] Iter: 90600 Loss: 0.011839662596551406  PSNR: 22.45990753173828\n","[TRAIN] Iter: 90700 Loss: 0.011017913618263538  PSNR: 22.847763061523438\n","[TRAIN] Iter: 90800 Loss: 0.011201530808200171  PSNR: 22.777021408081055\n","[TRAIN] Iter: 90900 Loss: 0.012571264044228096  PSNR: 22.248411178588867\n","[TRAIN] Iter: 91000 Loss: 0.01247038948014052  PSNR: 22.27637481689453\n","[TRAIN] Iter: 91100 Loss: 0.012198224787125497  PSNR: 22.362388610839844\n","[TRAIN] Iter: 91200 Loss: 0.01104740977382052  PSNR: 22.77848243713379\n","[TRAIN] Iter: 91300 Loss: 0.011765307826428244  PSNR: 22.528329849243164\n","[TRAIN] Iter: 91400 Loss: 0.011741750741957161  PSNR: 22.58888816833496\n","[TRAIN] Iter: 91500 Loss: 0.011093298387935591  PSNR: 22.81635856628418\n","[TRAIN] Iter: 91600 Loss: 0.012246950566568458  PSNR: 22.349607467651367\n","[TRAIN] Iter: 91700 Loss: 0.011273230730651236  PSNR: 22.752126693725586\n","[TRAIN] Iter: 91800 Loss: 0.012320741431643006  PSNR: 22.29361915588379\n","[TRAIN] Iter: 91900 Loss: 0.011912620257386819  PSNR: 22.50792121887207\n","[TRAIN] Iter: 92000 Loss: 0.012519103185632069  PSNR: 22.27946662902832\n","[TRAIN] Iter: 92100 Loss: 0.012236918401058741  PSNR: 22.273847579956055\n","[TRAIN] Iter: 92200 Loss: 0.011864662379613738  PSNR: 22.445222854614258\n","[TRAIN] Iter: 92300 Loss: 0.012155529843219774  PSNR: 22.3952579498291\n","[TRAIN] Iter: 92400 Loss: 0.012264469165459101  PSNR: 22.331462860107422\n","[TRAIN] Iter: 92500 Loss: 0.012060329227732064  PSNR: 22.31699562072754\n","[TRAIN] Iter: 92600 Loss: 0.011415224384637301  PSNR: 22.744382858276367\n","[TRAIN] Iter: 92700 Loss: 0.011260071577218249  PSNR: 22.744691848754883\n","[TRAIN] Iter: 92800 Loss: 0.011352519700246191  PSNR: 22.70138931274414\n","[TRAIN] Iter: 92900 Loss: 0.010978449659518347  PSNR: 23.002538681030273\n","[TRAIN] Iter: 93000 Loss: 0.012000373269395133  PSNR: 22.411462783813477\n","[TRAIN] Iter: 93100 Loss: 0.011461433310470276  PSNR: 22.54264259338379\n","[TRAIN] Iter: 93200 Loss: 0.012097134329417456  PSNR: 22.389469146728516\n","[TRAIN] Iter: 93300 Loss: 0.011776797164728566  PSNR: 22.565170288085938\n","[TRAIN] Iter: 93400 Loss: 0.011952820544580852  PSNR: 22.47287940979004\n","[TRAIN] Iter: 93500 Loss: 0.012388974625849066  PSNR: 22.237184524536133\n","[TRAIN] Iter: 93600 Loss: 0.012185223936804834  PSNR: 22.31405258178711\n","[TRAIN] Iter: 93700 Loss: 0.011763818348242729  PSNR: 22.586694717407227\n","[TRAIN] Iter: 93800 Loss: 0.011707026481259307  PSNR: 22.59813117980957\n","[TRAIN] Iter: 93900 Loss: 0.011710670125799641  PSNR: 22.57677459716797\n","[TRAIN] Iter: 94000 Loss: 0.01164343762577437  PSNR: 22.541976928710938\n","[TRAIN] Iter: 94100 Loss: 0.012440296028042233  PSNR: 22.23517417907715\n","[TRAIN] Iter: 94200 Loss: 0.011659905096730556  PSNR: 22.59239387512207\n","[TRAIN] Iter: 94300 Loss: 0.011669260656865414  PSNR: 22.497865676879883\n","[TRAIN] Iter: 94400 Loss: 0.01209104329564202  PSNR: 22.448333740234375\n","[TRAIN] Iter: 94500 Loss: 0.012237323624894516  PSNR: 22.299270629882812\n","[TRAIN] Iter: 94600 Loss: 0.011209954325588372  PSNR: 22.79619026184082\n","[TRAIN] Iter: 94700 Loss: 0.012949908290791082  PSNR: 22.079816818237305\n","[TRAIN] Iter: 94800 Loss: 0.011651313127060501  PSNR: 22.63890266418457\n","[TRAIN] Iter: 94900 Loss: 0.012093765888985367  PSNR: 22.40958023071289\n","[TRAIN] Iter: 95000 Loss: 0.012032342722057915  PSNR: 22.413665771484375\n","[TRAIN] Iter: 95100 Loss: 0.012504716548046878  PSNR: 22.283645629882812\n","[TRAIN] Iter: 95200 Loss: 0.01248559162725509  PSNR: 22.197772979736328\n","[TRAIN] Iter: 95300 Loss: 0.01311493815279587  PSNR: 21.99028968811035\n","[TRAIN] Iter: 95400 Loss: 0.012190725486668552  PSNR: 22.364736557006836\n","[TRAIN] Iter: 95500 Loss: 0.011496924477513128  PSNR: 22.710824966430664\n","[TRAIN] Iter: 95600 Loss: 0.011975770788071383  PSNR: 22.44689178466797\n","[TRAIN] Iter: 95700 Loss: 0.011943596199383023  PSNR: 22.480449676513672\n","[TRAIN] Iter: 95800 Loss: 0.012013734315442238  PSNR: 22.353275299072266\n","[TRAIN] Iter: 95900 Loss: 0.011798889279292556  PSNR: 22.47661781311035\n","[TRAIN] Iter: 96000 Loss: 0.011674683572196226  PSNR: 22.59320068359375\n","[TRAIN] Iter: 96100 Loss: 0.011994813175939572  PSNR: 22.441265106201172\n","[TRAIN] Iter: 96200 Loss: 0.012063534100128412  PSNR: 22.462726593017578\n","[TRAIN] Iter: 96300 Loss: 0.01225455436512794  PSNR: 22.438386917114258\n","[TRAIN] Iter: 96400 Loss: 0.013036448090825922  PSNR: 22.082427978515625\n","[TRAIN] Iter: 96500 Loss: 0.011787963939790754  PSNR: 22.55373191833496\n","[TRAIN] Iter: 96600 Loss: 0.01183277926485773  PSNR: 22.45477867126465\n","[TRAIN] Iter: 96700 Loss: 0.012045142752738427  PSNR: 22.486663818359375\n","[TRAIN] Iter: 96800 Loss: 0.01122966456099659  PSNR: 22.672748565673828\n","[TRAIN] Iter: 96900 Loss: 0.011726759636152426  PSNR: 22.659198760986328\n","[TRAIN] Iter: 97000 Loss: 0.013279997425097615  PSNR: 21.964134216308594\n","[TRAIN] Iter: 97100 Loss: 0.012887636698975587  PSNR: 22.146650314331055\n","[TRAIN] Iter: 97200 Loss: 0.011831966363552427  PSNR: 22.43458366394043\n","[TRAIN] Iter: 97300 Loss: 0.01249006642970618  PSNR: 22.32132911682129\n","[TRAIN] Iter: 97400 Loss: 0.011861486868850834  PSNR: 22.495607376098633\n","[TRAIN] Iter: 97500 Loss: 0.012895342834528822  PSNR: 22.189910888671875\n","[TRAIN] Iter: 97600 Loss: 0.01056430900147087  PSNR: 22.9825439453125\n","[TRAIN] Iter: 97700 Loss: 0.011502532596719362  PSNR: 22.679100036621094\n","[TRAIN] Iter: 97800 Loss: 0.012805335208401417  PSNR: 22.08942413330078\n","[TRAIN] Iter: 97900 Loss: 0.011998363952442276  PSNR: 22.390167236328125\n","[TRAIN] Iter: 98000 Loss: 0.01144028580620817  PSNR: 22.701129913330078\n","[TRAIN] Iter: 98100 Loss: 0.01225867737063793  PSNR: 22.31975746154785\n","[TRAIN] Iter: 98200 Loss: 0.012456902610372371  PSNR: 22.31939125061035\n","[TRAIN] Iter: 98300 Loss: 0.011846527666463883  PSNR: 22.485109329223633\n","[TRAIN] Iter: 98400 Loss: 0.011217892243147216  PSNR: 22.759613037109375\n","[TRAIN] Iter: 98500 Loss: 0.01188738464771927  PSNR: 22.502010345458984\n","[TRAIN] Iter: 98600 Loss: 0.012107460558401856  PSNR: 22.343921661376953\n","[TRAIN] Iter: 98700 Loss: 0.010907804235120971  PSNR: 22.941802978515625\n","[TRAIN] Iter: 98800 Loss: 0.011522139568952237  PSNR: 22.647600173950195\n","[TRAIN] Iter: 98900 Loss: 0.012118143553138293  PSNR: 22.389751434326172\n"," 49% 98999/200000 [7:40:58<7:43:34,  3.63it/s]Shuffle data after an epoch!\n","[TRAIN] Iter: 99000 Loss: 0.011867912288094033  PSNR: 22.43968963623047\n","[TRAIN] Iter: 99100 Loss: 0.01188193370218293  PSNR: 22.461740493774414\n","[TRAIN] Iter: 99200 Loss: 0.011280031330425978  PSNR: 22.75819969177246\n","[TRAIN] Iter: 99300 Loss: 0.011922633157505047  PSNR: 22.47161102294922\n","[TRAIN] Iter: 99400 Loss: 0.011717502646085159  PSNR: 22.49801254272461\n","[TRAIN] Iter: 99500 Loss: 0.01258038374563369  PSNR: 22.309703826904297\n","[TRAIN] Iter: 99600 Loss: 0.012486637342066183  PSNR: 22.362089157104492\n","[TRAIN] Iter: 99700 Loss: 0.011635721495300802  PSNR: 22.55913734436035\n","[TRAIN] Iter: 99800 Loss: 0.011575628641912025  PSNR: 22.66433334350586\n","[TRAIN] Iter: 99900 Loss: 0.012394730958156377  PSNR: 22.290990829467773\n"," 50% 99999/200000 [7:45:42<7:47:22,  3.57it/s]Saved checkpoints at ./logs/56leonard_test_st0/100000.tar\n","[TRAIN] Iter: 100000 Loss: 0.011806464085580174  PSNR: 22.53505516052246\n","[TRAIN] Iter: 100100 Loss: 0.011542721877841205  PSNR: 22.618085861206055\n","[TRAIN] Iter: 100200 Loss: 0.011757102896091078  PSNR: 22.478363037109375\n","[TRAIN] Iter: 100300 Loss: 0.011243153198525528  PSNR: 22.7502384185791\n","[TRAIN] Iter: 100400 Loss: 0.011014439079614583  PSNR: 22.884145736694336\n","[TRAIN] Iter: 100500 Loss: 0.011837175635974482  PSNR: 22.509035110473633\n","[TRAIN] Iter: 100600 Loss: 0.011344638268888218  PSNR: 22.69251823425293\n","[TRAIN] Iter: 100700 Loss: 0.012844233312467944  PSNR: 22.0675106048584\n","[TRAIN] Iter: 100800 Loss: 0.011463715763498993  PSNR: 22.658126831054688\n","[TRAIN] Iter: 100900 Loss: 0.011729484199747595  PSNR: 22.57386589050293\n","[TRAIN] Iter: 101000 Loss: 0.011013793881989864  PSNR: 22.661317825317383\n","[TRAIN] Iter: 101100 Loss: 0.01069198638097982  PSNR: 22.93659210205078\n","[TRAIN] Iter: 101200 Loss: 0.011829046610854498  PSNR: 22.444360733032227\n","[TRAIN] Iter: 101300 Loss: 0.011242407533350825  PSNR: 22.78440284729004\n","[TRAIN] Iter: 101400 Loss: 0.011800913955259167  PSNR: 22.535415649414062\n","[TRAIN] Iter: 101500 Loss: 0.011809191185917384  PSNR: 22.574155807495117\n","[TRAIN] Iter: 101600 Loss: 0.012351340557965898  PSNR: 22.294979095458984\n","[TRAIN] Iter: 101700 Loss: 0.010678831672920473  PSNR: 22.992704391479492\n","[TRAIN] Iter: 101800 Loss: 0.012067934009410017  PSNR: 22.400516510009766\n","[TRAIN] Iter: 101900 Loss: 0.012230130194245313  PSNR: 22.362016677856445\n","[TRAIN] Iter: 102000 Loss: 0.011496579221472958  PSNR: 22.587936401367188\n","[TRAIN] Iter: 102100 Loss: 0.010182064984228736  PSNR: 23.178407669067383\n","[TRAIN] Iter: 102200 Loss: 0.011587277309603556  PSNR: 22.55096435546875\n","[TRAIN] Iter: 102300 Loss: 0.01289546570136693  PSNR: 22.16656494140625\n","[TRAIN] Iter: 102400 Loss: 0.011421443149606931  PSNR: 22.656740188598633\n","[TRAIN] Iter: 102500 Loss: 0.011612177479629457  PSNR: 22.66151237487793\n","[TRAIN] Iter: 102600 Loss: 0.0112757339939254  PSNR: 22.694896697998047\n","[TRAIN] Iter: 102700 Loss: 0.011256893572755281  PSNR: 22.779022216796875\n","[TRAIN] Iter: 102800 Loss: 0.012080537684397484  PSNR: 22.424102783203125\n","[TRAIN] Iter: 102900 Loss: 0.012041189981631305  PSNR: 22.45082664489746\n","[TRAIN] Iter: 103000 Loss: 0.011578028060557717  PSNR: 22.495662689208984\n","[TRAIN] Iter: 103100 Loss: 0.011700016214512714  PSNR: 22.62737464904785\n","[TRAIN] Iter: 103200 Loss: 0.011852950345293434  PSNR: 22.44873809814453\n","[TRAIN] Iter: 103300 Loss: 0.012044680627909289  PSNR: 22.377737045288086\n","[TRAIN] Iter: 103400 Loss: 0.011511638496816185  PSNR: 22.69983673095703\n","[TRAIN] Iter: 103500 Loss: 0.011676966182911454  PSNR: 22.5492000579834\n","[TRAIN] Iter: 103600 Loss: 0.01181165210772147  PSNR: 22.501556396484375\n","[TRAIN] Iter: 103700 Loss: 0.011960377962615554  PSNR: 22.505586624145508\n","[TRAIN] Iter: 103800 Loss: 0.011126258593050484  PSNR: 22.672563552856445\n","[TRAIN] Iter: 103900 Loss: 0.011125724171598949  PSNR: 22.741558074951172\n","[TRAIN] Iter: 104000 Loss: 0.011418157455354088  PSNR: 22.80641746520996\n","[TRAIN] Iter: 104100 Loss: 0.012239603479112637  PSNR: 22.407550811767578\n","[TRAIN] Iter: 104200 Loss: 0.011740720011896206  PSNR: 22.509550094604492\n","[TRAIN] Iter: 104300 Loss: 0.01102531837468179  PSNR: 22.796566009521484\n","[TRAIN] Iter: 104400 Loss: 0.01128736866418007  PSNR: 22.717884063720703\n","[TRAIN] Iter: 104500 Loss: 0.012318591143433547  PSNR: 22.410667419433594\n","[TRAIN] Iter: 104600 Loss: 0.0122134132142794  PSNR: 22.449016571044922\n","[TRAIN] Iter: 104700 Loss: 0.011071175172344214  PSNR: 22.80649185180664\n","[TRAIN] Iter: 104800 Loss: 0.011628619821728105  PSNR: 22.668325424194336\n","[TRAIN] Iter: 104900 Loss: 0.011349950338082147  PSNR: 22.695343017578125\n","[TRAIN] Iter: 105000 Loss: 0.01293171232820199  PSNR: 22.112911224365234\n","[TRAIN] Iter: 105100 Loss: 0.011942265997449464  PSNR: 22.43343734741211\n","[TRAIN] Iter: 105200 Loss: 0.011302422147043842  PSNR: 22.708219528198242\n","[TRAIN] Iter: 105300 Loss: 0.01250813656701661  PSNR: 22.24544334411621\n","[TRAIN] Iter: 105400 Loss: 0.011625619918449976  PSNR: 22.582372665405273\n","[TRAIN] Iter: 105500 Loss: 0.011501692057968168  PSNR: 22.60018539428711\n","[TRAIN] Iter: 105600 Loss: 0.011305177419221631  PSNR: 22.75762367248535\n","[TRAIN] Iter: 105700 Loss: 0.011792333119363285  PSNR: 22.536657333374023\n","[TRAIN] Iter: 105800 Loss: 0.011483016495805394  PSNR: 22.71409797668457\n","[TRAIN] Iter: 105900 Loss: 0.010952065441937865  PSNR: 22.843507766723633\n","[TRAIN] Iter: 106000 Loss: 0.011945765037507517  PSNR: 22.447010040283203\n","[TRAIN] Iter: 106100 Loss: 0.01342216950647318  PSNR: 21.8619327545166\n","[TRAIN] Iter: 106200 Loss: 0.011571214218502246  PSNR: 22.509437561035156\n","[TRAIN] Iter: 106300 Loss: 0.011617397328827068  PSNR: 22.638717651367188\n","[TRAIN] Iter: 106400 Loss: 0.01202590459707235  PSNR: 22.343460083007812\n","[TRAIN] Iter: 106500 Loss: 0.012267277420139226  PSNR: 22.376245498657227\n","[TRAIN] Iter: 106600 Loss: 0.011725429473805678  PSNR: 22.620067596435547\n","[TRAIN] Iter: 106700 Loss: 0.010878931052719297  PSNR: 22.847116470336914\n","[TRAIN] Iter: 106800 Loss: 0.010849265262385976  PSNR: 22.855619430541992\n","[TRAIN] Iter: 106900 Loss: 0.01106281315912817  PSNR: 22.864654541015625\n","[TRAIN] Iter: 107000 Loss: 0.013020182795985807  PSNR: 22.077360153198242\n","[TRAIN] Iter: 107100 Loss: 0.011494864247989968  PSNR: 22.580570220947266\n","[TRAIN] Iter: 107200 Loss: 0.011550374064582956  PSNR: 22.671031951904297\n","[TRAIN] Iter: 107300 Loss: 0.011753324613402193  PSNR: 22.54928207397461\n","[TRAIN] Iter: 107400 Loss: 0.010845921202345444  PSNR: 22.8811092376709\n","[TRAIN] Iter: 107500 Loss: 0.011751203364943341  PSNR: 22.531414031982422\n","[TRAIN] Iter: 107600 Loss: 0.011418743178734793  PSNR: 22.576507568359375\n","[TRAIN] Iter: 107700 Loss: 0.011364374001069075  PSNR: 22.613672256469727\n","[TRAIN] Iter: 107800 Loss: 0.011461246342685229  PSNR: 22.70696449279785\n","[TRAIN] Iter: 107900 Loss: 0.011176422661723605  PSNR: 22.82170867919922\n","[TRAIN] Iter: 108000 Loss: 0.011217774333749271  PSNR: 22.870628356933594\n","[TRAIN] Iter: 108100 Loss: 0.01121991774901928  PSNR: 22.729503631591797\n","[TRAIN] Iter: 108200 Loss: 0.012610905268160107  PSNR: 22.221397399902344\n","[TRAIN] Iter: 108300 Loss: 0.011137167911181745  PSNR: 22.82347869873047\n","[TRAIN] Iter: 108400 Loss: 0.01165624252076826  PSNR: 22.558197021484375\n","[TRAIN] Iter: 108500 Loss: 0.0125602961438252  PSNR: 22.26772689819336\n","[TRAIN] Iter: 108600 Loss: 0.010923753363480193  PSNR: 22.82071304321289\n","[TRAIN] Iter: 108700 Loss: 0.01181893135300832  PSNR: 22.551239013671875\n","[TRAIN] Iter: 108800 Loss: 0.011103005534324872  PSNR: 22.785518646240234\n","[TRAIN] Iter: 108900 Loss: 0.012635244886392377  PSNR: 22.290340423583984\n","[TRAIN] Iter: 109000 Loss: 0.011394617872524972  PSNR: 22.817649841308594\n","[TRAIN] Iter: 109100 Loss: 0.01190970464086381  PSNR: 22.438739776611328\n","[TRAIN] Iter: 109200 Loss: 0.010913407253639363  PSNR: 22.857423782348633\n","[TRAIN] Iter: 109300 Loss: 0.01110203917882949  PSNR: 22.84927749633789\n","[TRAIN] Iter: 109400 Loss: 0.010648736747560002  PSNR: 23.066356658935547\n","[TRAIN] Iter: 109500 Loss: 0.0114305678068519  PSNR: 22.651615142822266\n","[TRAIN] Iter: 109600 Loss: 0.012518241344864709  PSNR: 22.242639541625977\n","[TRAIN] Iter: 109700 Loss: 0.011749753134680026  PSNR: 22.515228271484375\n","[TRAIN] Iter: 109800 Loss: 0.011013143387692785  PSNR: 22.848876953125\n","[TRAIN] Iter: 109900 Loss: 0.011261145164686875  PSNR: 22.658979415893555\n"," 55% 109999/200000 [8:32:24<6:55:54,  3.61it/s]Saved checkpoints at ./logs/56leonard_test_st0/110000.tar\n","[TRAIN] Iter: 110000 Loss: 0.010332373422469062  PSNR: 23.147659301757812\n","[TRAIN] Iter: 110100 Loss: 0.012494035564604905  PSNR: 22.268875122070312\n","[TRAIN] Iter: 110200 Loss: 0.012079082070400347  PSNR: 22.4278564453125\n","[TRAIN] Iter: 110300 Loss: 0.011408971010310642  PSNR: 22.635540008544922\n","[TRAIN] Iter: 110400 Loss: 0.011151250399512293  PSNR: 22.841453552246094\n","[TRAIN] Iter: 110500 Loss: 0.01191916238809179  PSNR: 22.52501678466797\n","[TRAIN] Iter: 110600 Loss: 0.011295721915862356  PSNR: 22.667919158935547\n","[TRAIN] Iter: 110700 Loss: 0.01169114894023042  PSNR: 22.6391544342041\n","[TRAIN] Iter: 110800 Loss: 0.01156191076475245  PSNR: 22.65141487121582\n","[TRAIN] Iter: 110900 Loss: 0.01146904772994441  PSNR: 22.681575775146484\n","[TRAIN] Iter: 111000 Loss: 0.010756278287248303  PSNR: 22.957077026367188\n","[TRAIN] Iter: 111100 Loss: 0.011900930252917063  PSNR: 22.42778205871582\n","[TRAIN] Iter: 111200 Loss: 0.011678449189106422  PSNR: 22.620954513549805\n","[TRAIN] Iter: 111300 Loss: 0.011949840715213024  PSNR: 22.507471084594727\n","[TRAIN] Iter: 111400 Loss: 0.011368788695148858  PSNR: 22.728614807128906\n","[TRAIN] Iter: 111500 Loss: 0.011892187848373835  PSNR: 22.554101943969727\n","[TRAIN] Iter: 111600 Loss: 0.011519107241491858  PSNR: 22.60911750793457\n","[TRAIN] Iter: 111700 Loss: 0.01230546766519521  PSNR: 22.224708557128906\n","[TRAIN] Iter: 111800 Loss: 0.012483932782431947  PSNR: 22.216049194335938\n","[TRAIN] Iter: 111900 Loss: 0.011498620339967543  PSNR: 22.615806579589844\n","[TRAIN] Iter: 112000 Loss: 0.011990006506411865  PSNR: 22.493633270263672\n","[TRAIN] Iter: 112100 Loss: 0.011676317529473332  PSNR: 22.548200607299805\n","[TRAIN] Iter: 112200 Loss: 0.011401463865222203  PSNR: 22.730669021606445\n","[TRAIN] Iter: 112300 Loss: 0.011204259668786853  PSNR: 22.781551361083984\n","[TRAIN] Iter: 112400 Loss: 0.011576202698261673  PSNR: 22.617637634277344\n","[TRAIN] Iter: 112500 Loss: 0.010587057866284545  PSNR: 22.954456329345703\n","[TRAIN] Iter: 112600 Loss: 0.011297712152561595  PSNR: 22.75246810913086\n","[TRAIN] Iter: 112700 Loss: 0.012011326041035934  PSNR: 22.506145477294922\n","[TRAIN] Iter: 112800 Loss: 0.011193954103828868  PSNR: 22.71712875366211\n","[TRAIN] Iter: 112900 Loss: 0.01248429932760882  PSNR: 22.308210372924805\n","[TRAIN] Iter: 113000 Loss: 0.011130040014660234  PSNR: 22.83417320251465\n","[TRAIN] Iter: 113100 Loss: 0.011603066702356809  PSNR: 22.604053497314453\n","[TRAIN] Iter: 113200 Loss: 0.011238443827664374  PSNR: 22.728612899780273\n","[TRAIN] Iter: 113300 Loss: 0.010962797952569547  PSNR: 22.936199188232422\n","[TRAIN] Iter: 113400 Loss: 0.011457647116634887  PSNR: 22.641218185424805\n","[TRAIN] Iter: 113500 Loss: 0.011520929984744127  PSNR: 22.711721420288086\n","[TRAIN] Iter: 113600 Loss: 0.01051446413811306  PSNR: 23.06923484802246\n","[TRAIN] Iter: 113700 Loss: 0.011618626709011884  PSNR: 22.580732345581055\n","[TRAIN] Iter: 113800 Loss: 0.011927899504100989  PSNR: 22.451030731201172\n","[TRAIN] Iter: 113900 Loss: 0.0110085226619606  PSNR: 22.840726852416992\n","[TRAIN] Iter: 114000 Loss: 0.012055315929803266  PSNR: 22.434402465820312\n","[TRAIN] Iter: 114100 Loss: 0.011516617146375255  PSNR: 22.68489646911621\n","[TRAIN] Iter: 114200 Loss: 0.010989717268880445  PSNR: 22.831363677978516\n","[TRAIN] Iter: 114300 Loss: 0.0115030375236113  PSNR: 22.66603660583496\n","[TRAIN] Iter: 114400 Loss: 0.01162278972725049  PSNR: 22.583253860473633\n","[TRAIN] Iter: 114500 Loss: 0.011723514700655338  PSNR: 22.57307243347168\n","[TRAIN] Iter: 114600 Loss: 0.010793204944433882  PSNR: 22.956974029541016\n","[TRAIN] Iter: 114700 Loss: 0.010538055800940633  PSNR: 22.944364547729492\n","[TRAIN] Iter: 114800 Loss: 0.011802659892074463  PSNR: 22.541717529296875\n","[TRAIN] Iter: 114900 Loss: 0.011105449151336688  PSNR: 22.76185417175293\n","[TRAIN] Iter: 115000 Loss: 0.010584832970883582  PSNR: 22.96906852722168\n","[TRAIN] Iter: 115100 Loss: 0.01151272602372057  PSNR: 22.722436904907227\n","[TRAIN] Iter: 115200 Loss: 0.011381607313081321  PSNR: 22.765949249267578\n","[TRAIN] Iter: 115300 Loss: 0.011277614491818862  PSNR: 22.79998779296875\n","[TRAIN] Iter: 115400 Loss: 0.011526570175277326  PSNR: 22.565868377685547\n","[TRAIN] Iter: 115500 Loss: 0.011236870744993646  PSNR: 22.75847053527832\n","[TRAIN] Iter: 115600 Loss: 0.011711890974858306  PSNR: 22.618520736694336\n","[TRAIN] Iter: 115700 Loss: 0.011464393185918666  PSNR: 22.62726593017578\n","[TRAIN] Iter: 115800 Loss: 0.010682489449403101  PSNR: 23.00232696533203\n","[TRAIN] Iter: 115900 Loss: 0.012106738276992747  PSNR: 22.38409423828125\n","[TRAIN] Iter: 116000 Loss: 0.011732663835912097  PSNR: 22.58199691772461\n","[TRAIN] Iter: 116100 Loss: 0.011731754022939874  PSNR: 22.594825744628906\n","[TRAIN] Iter: 116200 Loss: 0.012271947289833026  PSNR: 22.38011932373047\n","[TRAIN] Iter: 116300 Loss: 0.012695627027722341  PSNR: 22.186065673828125\n","[TRAIN] Iter: 116400 Loss: 0.011466680208046217  PSNR: 22.643918991088867\n","[TRAIN] Iter: 116500 Loss: 0.011980535569889886  PSNR: 22.51766014099121\n","[TRAIN] Iter: 116600 Loss: 0.010866035894525328  PSNR: 22.859939575195312\n","[TRAIN] Iter: 116700 Loss: 0.012712779687830064  PSNR: 22.190034866333008\n","[TRAIN] Iter: 116800 Loss: 0.01141605645184383  PSNR: 22.596525192260742\n","[TRAIN] Iter: 116900 Loss: 0.011269706998377517  PSNR: 22.741291046142578\n","[TRAIN] Iter: 117000 Loss: 0.012629090734459756  PSNR: 22.237411499023438\n","[TRAIN] Iter: 117100 Loss: 0.011984966801995258  PSNR: 22.43714714050293\n","[TRAIN] Iter: 117200 Loss: 0.012260626380081267  PSNR: 22.409250259399414\n","[TRAIN] Iter: 117300 Loss: 0.011854928969914549  PSNR: 22.56029510498047\n","[TRAIN] Iter: 117400 Loss: 0.012349983180525378  PSNR: 22.294315338134766\n","[TRAIN] Iter: 117500 Loss: 0.011365602902508778  PSNR: 22.79129981994629\n","[TRAIN] Iter: 117600 Loss: 0.011546818276456774  PSNR: 22.663528442382812\n","[TRAIN] Iter: 117700 Loss: 0.012049236699230163  PSNR: 22.435503005981445\n","[TRAIN] Iter: 117800 Loss: 0.011736776626954376  PSNR: 22.58538246154785\n","[TRAIN] Iter: 117900 Loss: 0.010426599874399048  PSNR: 23.112491607666016\n","[TRAIN] Iter: 118000 Loss: 0.011459055966022341  PSNR: 22.691648483276367\n","[TRAIN] Iter: 118100 Loss: 0.011233777932214476  PSNR: 22.73535919189453\n","[TRAIN] Iter: 118200 Loss: 0.010813307291380681  PSNR: 22.862363815307617\n","[TRAIN] Iter: 118300 Loss: 0.012022784177774961  PSNR: 22.401500701904297\n","[TRAIN] Iter: 118400 Loss: 0.011748828159494514  PSNR: 22.549530029296875\n","[TRAIN] Iter: 118500 Loss: 0.011533491277583453  PSNR: 22.69731903076172\n","[TRAIN] Iter: 118600 Loss: 0.011811078673959384  PSNR: 22.517637252807617\n","[TRAIN] Iter: 118700 Loss: 0.011304701185269052  PSNR: 22.696348190307617\n"," 59% 118799/200000 [9:13:24<6:21:30,  3.55it/s]Shuffle data after an epoch!\n","[TRAIN] Iter: 118800 Loss: 0.011643504284596132  PSNR: 22.535762786865234\n","[TRAIN] Iter: 118900 Loss: 0.011566317226524967  PSNR: 22.622608184814453\n","[TRAIN] Iter: 119000 Loss: 0.011753992392832176  PSNR: 22.568161010742188\n","[TRAIN] Iter: 119100 Loss: 0.01135120786393872  PSNR: 22.742122650146484\n","[TRAIN] Iter: 119200 Loss: 0.012122131528534807  PSNR: 22.445341110229492\n","[TRAIN] Iter: 119300 Loss: 0.010373062029334515  PSNR: 23.0557804107666\n","[TRAIN] Iter: 119400 Loss: 0.011849380226903723  PSNR: 22.465991973876953\n","[TRAIN] Iter: 119500 Loss: 0.011313290027407585  PSNR: 22.78084373474121\n","[TRAIN] Iter: 119600 Loss: 0.011204241249032692  PSNR: 22.858917236328125\n","[TRAIN] Iter: 119700 Loss: 0.011777040118784228  PSNR: 22.490432739257812\n","[TRAIN] Iter: 119800 Loss: 0.011331916126962516  PSNR: 22.71843719482422\n","[TRAIN] Iter: 119900 Loss: 0.010962084944131607  PSNR: 22.914751052856445\n"," 60% 119999/200000 [9:19:05<6:12:29,  3.58it/s]Saved checkpoints at ./logs/56leonard_test_st0/120000.tar\n","[TRAIN] Iter: 120000 Loss: 0.010734199380157246  PSNR: 23.03377342224121\n","[TRAIN] Iter: 120100 Loss: 0.011585238062929132  PSNR: 22.629308700561523\n","[TRAIN] Iter: 120200 Loss: 0.011688683910896565  PSNR: 22.570375442504883\n","[TRAIN] Iter: 120300 Loss: 0.011399105593509103  PSNR: 22.59754753112793\n","[TRAIN] Iter: 120400 Loss: 0.012027545532074076  PSNR: 22.439298629760742\n","[TRAIN] Iter: 120500 Loss: 0.011252657639316594  PSNR: 22.778423309326172\n","[TRAIN] Iter: 120600 Loss: 0.01254364623088482  PSNR: 22.320274353027344\n","[TRAIN] Iter: 120700 Loss: 0.011351226854252952  PSNR: 22.656517028808594\n","[TRAIN] Iter: 120800 Loss: 0.013130065019817285  PSNR: 22.106107711791992\n","[TRAIN] Iter: 120900 Loss: 0.011244215045984308  PSNR: 22.772624969482422\n","[TRAIN] Iter: 121000 Loss: 0.011646227280733178  PSNR: 22.528573989868164\n","[TRAIN] Iter: 121100 Loss: 0.011842081902739705  PSNR: 22.49043846130371\n","[TRAIN] Iter: 121200 Loss: 0.010924855331716072  PSNR: 22.88352394104004\n","[TRAIN] Iter: 121300 Loss: 0.012065905387151158  PSNR: 22.395404815673828\n","[TRAIN] Iter: 121400 Loss: 0.011869792160342204  PSNR: 22.51633644104004\n","[TRAIN] Iter: 121500 Loss: 0.01184954789614062  PSNR: 22.471588134765625\n","[TRAIN] Iter: 121600 Loss: 0.011370319355844486  PSNR: 22.67420196533203\n","[TRAIN] Iter: 121700 Loss: 0.01114235957606232  PSNR: 22.74185562133789\n","[TRAIN] Iter: 121800 Loss: 0.011389723854512434  PSNR: 22.710912704467773\n","[TRAIN] Iter: 121900 Loss: 0.011143105195239953  PSNR: 22.778024673461914\n","[TRAIN] Iter: 122000 Loss: 0.011199943789978257  PSNR: 22.762746810913086\n","[TRAIN] Iter: 122100 Loss: 0.012394052205640091  PSNR: 22.313753128051758\n","[TRAIN] Iter: 122200 Loss: 0.011345924547081526  PSNR: 22.714670181274414\n","[TRAIN] Iter: 122300 Loss: 0.01192132949579149  PSNR: 22.515783309936523\n","[TRAIN] Iter: 122400 Loss: 0.010888130520541016  PSNR: 22.949161529541016\n","[TRAIN] Iter: 122500 Loss: 0.011014074027701555  PSNR: 22.787385940551758\n","[TRAIN] Iter: 122600 Loss: 0.010548558143772157  PSNR: 23.055763244628906\n","[TRAIN] Iter: 122700 Loss: 0.010409913739037555  PSNR: 23.116378784179688\n","[TRAIN] Iter: 122800 Loss: 0.01021741424324203  PSNR: 23.157695770263672\n","[TRAIN] Iter: 122900 Loss: 0.0101171348676014  PSNR: 23.180789947509766\n","[TRAIN] Iter: 123000 Loss: 0.010349060684219667  PSNR: 23.15756607055664\n","[TRAIN] Iter: 123100 Loss: 0.010953844933440943  PSNR: 22.940689086914062\n","[TRAIN] Iter: 123200 Loss: 0.011819095290551885  PSNR: 22.542512893676758\n","[TRAIN] Iter: 123300 Loss: 0.010655858023417394  PSNR: 23.032386779785156\n","[TRAIN] Iter: 123400 Loss: 0.011585775874957575  PSNR: 22.651962280273438\n","[TRAIN] Iter: 123500 Loss: 0.01181909095428404  PSNR: 22.55364418029785\n","[TRAIN] Iter: 123600 Loss: 0.011138919796605988  PSNR: 22.80562973022461\n","[TRAIN] Iter: 123700 Loss: 0.011515943009092006  PSNR: 22.647428512573242\n","[TRAIN] Iter: 123800 Loss: 0.011754977064403406  PSNR: 22.536230087280273\n","[TRAIN] Iter: 123900 Loss: 0.011549944104820851  PSNR: 22.645238876342773\n","[TRAIN] Iter: 124000 Loss: 0.011915814890582025  PSNR: 22.460952758789062\n","[TRAIN] Iter: 124100 Loss: 0.011198900854581792  PSNR: 22.724031448364258\n","[TRAIN] Iter: 124200 Loss: 0.012027363479811783  PSNR: 22.4591007232666\n","[TRAIN] Iter: 124300 Loss: 0.011333581233108096  PSNR: 22.66313934326172\n","[TRAIN] Iter: 124400 Loss: 0.011267611764883445  PSNR: 22.753612518310547\n","[TRAIN] Iter: 124500 Loss: 0.01033804104378602  PSNR: 23.0661563873291\n","[TRAIN] Iter: 124600 Loss: 0.010962848786859209  PSNR: 22.874584197998047\n","[TRAIN] Iter: 124700 Loss: 0.011488672098924117  PSNR: 22.626005172729492\n","[TRAIN] Iter: 124800 Loss: 0.011029570611144339  PSNR: 22.916013717651367\n","[TRAIN] Iter: 124900 Loss: 0.010984153159885594  PSNR: 22.916807174682617\n","[TRAIN] Iter: 125000 Loss: 0.01154678409860439  PSNR: 22.68160629272461\n","[TRAIN] Iter: 125100 Loss: 0.011997172925865694  PSNR: 22.525163650512695\n","[TRAIN] Iter: 125200 Loss: 0.011423842123580838  PSNR: 22.701175689697266\n","[TRAIN] Iter: 125300 Loss: 0.012239485990589426  PSNR: 22.475427627563477\n","[TRAIN] Iter: 125400 Loss: 0.01198142100631356  PSNR: 22.443622589111328\n","[TRAIN] Iter: 125500 Loss: 0.011628692564637064  PSNR: 22.59292221069336\n","[TRAIN] Iter: 125600 Loss: 0.011133949198200738  PSNR: 22.759456634521484\n","[TRAIN] Iter: 125700 Loss: 0.012171109526274116  PSNR: 22.41681671142578\n","[TRAIN] Iter: 125800 Loss: 0.011670337796617507  PSNR: 22.626550674438477\n","[TRAIN] Iter: 125900 Loss: 0.011929576216547647  PSNR: 22.572389602661133\n","[TRAIN] Iter: 126000 Loss: 0.011047086725124344  PSNR: 22.900672912597656\n","[TRAIN] Iter: 126100 Loss: 0.012525702119483189  PSNR: 22.318201065063477\n","[TRAIN] Iter: 126200 Loss: 0.012150509066161483  PSNR: 22.457462310791016\n","[TRAIN] Iter: 126300 Loss: 0.011532860058253967  PSNR: 22.64362907409668\n","[TRAIN] Iter: 126400 Loss: 0.010920925195850539  PSNR: 22.8664608001709\n","[TRAIN] Iter: 126500 Loss: 0.011553737018207427  PSNR: 22.675708770751953\n","[TRAIN] Iter: 126600 Loss: 0.010651827055594514  PSNR: 22.9715518951416\n","[TRAIN] Iter: 126700 Loss: 0.01201787163112422  PSNR: 22.490970611572266\n","[TRAIN] Iter: 126800 Loss: 0.012551038232272077  PSNR: 22.198139190673828\n","[TRAIN] Iter: 126900 Loss: 0.011706838411778048  PSNR: 22.554855346679688\n","[TRAIN] Iter: 127000 Loss: 0.011824975052307793  PSNR: 22.544872283935547\n","[TRAIN] Iter: 127100 Loss: 0.011689307596143035  PSNR: 22.535245895385742\n","[TRAIN] Iter: 127200 Loss: 0.011114245133920085  PSNR: 22.872812271118164\n","[TRAIN] Iter: 127300 Loss: 0.011317446841443462  PSNR: 22.722246170043945\n","[TRAIN] Iter: 127400 Loss: 0.011486100466192199  PSNR: 22.642070770263672\n","[TRAIN] Iter: 127500 Loss: 0.012150868786334335  PSNR: 22.383508682250977\n","[TRAIN] Iter: 127600 Loss: 0.011237315201269634  PSNR: 22.777446746826172\n","[TRAIN] Iter: 127700 Loss: 0.011057117913458155  PSNR: 22.895681381225586\n","[TRAIN] Iter: 127800 Loss: 0.011452543423859235  PSNR: 22.65860366821289\n","[TRAIN] Iter: 127900 Loss: 0.011495488677406466  PSNR: 22.646907806396484\n","[TRAIN] Iter: 128000 Loss: 0.011530820284619376  PSNR: 22.61276626586914\n","[TRAIN] Iter: 128100 Loss: 0.011617763964566439  PSNR: 22.768312454223633\n","[TRAIN] Iter: 128200 Loss: 0.011965287463940877  PSNR: 22.52460479736328\n","[TRAIN] Iter: 128300 Loss: 0.01141289016322753  PSNR: 22.670446395874023\n","[TRAIN] Iter: 128400 Loss: 0.011592604281419203  PSNR: 22.67215347290039\n","[TRAIN] Iter: 128500 Loss: 0.01115831708827302  PSNR: 22.797298431396484\n","[TRAIN] Iter: 128600 Loss: 0.011885187401383945  PSNR: 22.44910430908203\n","[TRAIN] Iter: 128700 Loss: 0.012017566968469081  PSNR: 22.471498489379883\n","[TRAIN] Iter: 128800 Loss: 0.012476505535801194  PSNR: 22.25713539123535\n","[TRAIN] Iter: 128900 Loss: 0.01191203634166823  PSNR: 22.591888427734375\n","[TRAIN] Iter: 129000 Loss: 0.012023765064418162  PSNR: 22.515960693359375\n","[TRAIN] Iter: 129100 Loss: 0.011401383155093321  PSNR: 22.728740692138672\n","[TRAIN] Iter: 129200 Loss: 0.011556934234723587  PSNR: 22.603282928466797\n","[TRAIN] Iter: 129300 Loss: 0.011336003423174874  PSNR: 22.720294952392578\n","[TRAIN] Iter: 129400 Loss: 0.01133950424731276  PSNR: 22.700801849365234\n","[TRAIN] Iter: 129500 Loss: 0.011133918202509757  PSNR: 22.807815551757812\n","[TRAIN] Iter: 129600 Loss: 0.011718114495101305  PSNR: 22.568422317504883\n","[TRAIN] Iter: 129700 Loss: 0.011179678088321984  PSNR: 22.83184051513672\n","[TRAIN] Iter: 129800 Loss: 0.010631529237432115  PSNR: 23.001737594604492\n","[TRAIN] Iter: 129900 Loss: 0.010974395560183635  PSNR: 22.82459831237793\n"," 65% 129999/200000 [10:05:47<5:26:39,  3.57it/s]Saved checkpoints at ./logs/56leonard_test_st0/130000.tar\n","[TRAIN] Iter: 130000 Loss: 0.01217302912759456  PSNR: 22.421245574951172\n","[TRAIN] Iter: 130100 Loss: 0.010866947686527746  PSNR: 22.91221046447754\n","[TRAIN] Iter: 130200 Loss: 0.01184147541540709  PSNR: 22.471721649169922\n","[TRAIN] Iter: 130300 Loss: 0.011091175273790204  PSNR: 22.89919662475586\n","[TRAIN] Iter: 130400 Loss: 0.011480582011325246  PSNR: 22.678821563720703\n","[TRAIN] Iter: 130500 Loss: 0.01072456716583282  PSNR: 22.989276885986328\n","[TRAIN] Iter: 130600 Loss: 0.01151003745529783  PSNR: 22.672645568847656\n","[TRAIN] Iter: 130700 Loss: 0.011944538218025259  PSNR: 22.488311767578125\n","[TRAIN] Iter: 130800 Loss: 0.011306988173654369  PSNR: 22.716686248779297\n","[TRAIN] Iter: 130900 Loss: 0.01115440053423054  PSNR: 22.760452270507812\n","[TRAIN] Iter: 131000 Loss: 0.012295198986216043  PSNR: 22.36396026611328\n","[TRAIN] Iter: 131100 Loss: 0.01106398540701197  PSNR: 22.816612243652344\n","[TRAIN] Iter: 131200 Loss: 0.010758078292583972  PSNR: 22.904834747314453\n","[TRAIN] Iter: 131300 Loss: 0.01212991939173825  PSNR: 22.38706398010254\n","[TRAIN] Iter: 131400 Loss: 0.011687334731227478  PSNR: 22.641714096069336\n","[TRAIN] Iter: 131500 Loss: 0.010402122113027375  PSNR: 23.05730438232422\n","[TRAIN] Iter: 131600 Loss: 0.011232729783774037  PSNR: 22.831159591674805\n","[TRAIN] Iter: 131700 Loss: 0.011521290013147873  PSNR: 22.647531509399414\n","[TRAIN] Iter: 131800 Loss: 0.010529571244747266  PSNR: 23.114717483520508\n","[TRAIN] Iter: 131900 Loss: 0.011471880686346404  PSNR: 22.647289276123047\n","[TRAIN] Iter: 132000 Loss: 0.011470156205246319  PSNR: 22.604877471923828\n","[TRAIN] Iter: 132100 Loss: 0.011305968940938285  PSNR: 22.756044387817383\n","[TRAIN] Iter: 132200 Loss: 0.01133982889985992  PSNR: 22.67521858215332\n","[TRAIN] Iter: 132300 Loss: 0.011258781578050929  PSNR: 22.795631408691406\n","[TRAIN] Iter: 132400 Loss: 0.011354722569713031  PSNR: 22.813798904418945\n","[TRAIN] Iter: 132500 Loss: 0.011278397739795155  PSNR: 22.672941207885742\n","[TRAIN] Iter: 132600 Loss: 0.010632744599412755  PSNR: 22.97978973388672\n","[TRAIN] Iter: 132700 Loss: 0.011634813251387684  PSNR: 22.49381446838379\n","[TRAIN] Iter: 132800 Loss: 0.011849141655762147  PSNR: 22.51454734802246\n","[TRAIN] Iter: 132900 Loss: 0.011118129126298261  PSNR: 22.739152908325195\n","[TRAIN] Iter: 133000 Loss: 0.01115223657044524  PSNR: 22.693912506103516\n","[TRAIN] Iter: 133100 Loss: 0.011594621851116411  PSNR: 22.673992156982422\n","[TRAIN] Iter: 133200 Loss: 0.014061248350996711  PSNR: 21.81655502319336\n","[TRAIN] Iter: 133300 Loss: 0.011041451709062896  PSNR: 22.86341094970703\n","[TRAIN] Iter: 133400 Loss: 0.011643346053637534  PSNR: 22.589773178100586\n","[TRAIN] Iter: 133500 Loss: 0.011120125852983956  PSNR: 22.780990600585938\n","[TRAIN] Iter: 133600 Loss: 0.011405231818579631  PSNR: 22.631481170654297\n","[TRAIN] Iter: 133700 Loss: 0.011647782943090748  PSNR: 22.659343719482422\n","[TRAIN] Iter: 133800 Loss: 0.011525793061773433  PSNR: 22.704017639160156\n","[TRAIN] Iter: 133900 Loss: 0.012933203318070139  PSNR: 22.109241485595703\n","[TRAIN] Iter: 134000 Loss: 0.010335797637191686  PSNR: 23.16689109802246\n","[TRAIN] Iter: 134100 Loss: 0.011358923943429192  PSNR: 22.754257202148438\n","[TRAIN] Iter: 134200 Loss: 0.011268438542803062  PSNR: 22.765417098999023\n","[TRAIN] Iter: 134300 Loss: 0.009661235687839248  PSNR: 23.46586799621582\n","[TRAIN] Iter: 134400 Loss: 0.010883714560209097  PSNR: 22.88858413696289\n","[TRAIN] Iter: 134500 Loss: 0.012250810037737786  PSNR: 22.33411979675293\n","[TRAIN] Iter: 134600 Loss: 0.012174537246599598  PSNR: 22.326929092407227\n","[TRAIN] Iter: 134700 Loss: 0.012383557838127723  PSNR: 22.316741943359375\n","[TRAIN] Iter: 134800 Loss: 0.01142330027404882  PSNR: 22.6990909576416\n","[TRAIN] Iter: 134900 Loss: 0.011396438453174514  PSNR: 22.744958877563477\n","[TRAIN] Iter: 135000 Loss: 0.011731927940483763  PSNR: 22.627689361572266\n","[TRAIN] Iter: 135100 Loss: 0.011659904947696462  PSNR: 22.589754104614258\n","[TRAIN] Iter: 135200 Loss: 0.010582639785470799  PSNR: 23.019704818725586\n","[TRAIN] Iter: 135300 Loss: 0.011009102895761362  PSNR: 22.872112274169922\n","[TRAIN] Iter: 135400 Loss: 0.010624887676919027  PSNR: 22.97954559326172\n","[TRAIN] Iter: 135500 Loss: 0.010732727843778125  PSNR: 22.911638259887695\n","[TRAIN] Iter: 135600 Loss: 0.011824987983543256  PSNR: 22.57379913330078\n","[TRAIN] Iter: 135700 Loss: 0.010691960056309673  PSNR: 23.0167236328125\n","[TRAIN] Iter: 135800 Loss: 0.011410500728604701  PSNR: 22.70378303527832\n","[TRAIN] Iter: 135900 Loss: 0.010810178437456963  PSNR: 22.989057540893555\n","[TRAIN] Iter: 136000 Loss: 0.01099740158317137  PSNR: 22.82638168334961\n","[TRAIN] Iter: 136100 Loss: 0.011145665929252955  PSNR: 22.780195236206055\n","[TRAIN] Iter: 136200 Loss: 0.01170798212115162  PSNR: 22.5316104888916\n","[TRAIN] Iter: 136300 Loss: 0.011558861092141659  PSNR: 22.645143508911133\n","[TRAIN] Iter: 136400 Loss: 0.011684592512093974  PSNR: 22.60959243774414\n","[TRAIN] Iter: 136500 Loss: 0.01100726852523313  PSNR: 23.02035140991211\n","[TRAIN] Iter: 136600 Loss: 0.011939454054508369  PSNR: 22.49165916442871\n","[TRAIN] Iter: 136700 Loss: 0.010317348126067827  PSNR: 23.145082473754883\n","[TRAIN] Iter: 136800 Loss: 0.011601589636140401  PSNR: 22.573131561279297\n","[TRAIN] Iter: 136900 Loss: 0.010941954673463167  PSNR: 22.881473541259766\n","[TRAIN] Iter: 137000 Loss: 0.010708930490163977  PSNR: 22.885793685913086\n","[TRAIN] Iter: 137100 Loss: 0.010246208861653524  PSNR: 23.11491584777832\n","[TRAIN] Iter: 137200 Loss: 0.010900926924753253  PSNR: 22.824541091918945\n","[TRAIN] Iter: 137300 Loss: 0.010989563645054467  PSNR: 22.871381759643555\n","[TRAIN] Iter: 137400 Loss: 0.011333488499940172  PSNR: 22.772449493408203\n","[TRAIN] Iter: 137500 Loss: 0.01221560410271125  PSNR: 22.43804359436035\n","[TRAIN] Iter: 137600 Loss: 0.010849670113948589  PSNR: 22.969038009643555\n","[TRAIN] Iter: 137700 Loss: 0.01142447582895097  PSNR: 22.6554012298584\n","[TRAIN] Iter: 137800 Loss: 0.01103566904849455  PSNR: 22.771692276000977\n","[TRAIN] Iter: 137900 Loss: 0.011628513672378115  PSNR: 22.650997161865234\n","[TRAIN] Iter: 138000 Loss: 0.010422309812674741  PSNR: 23.179912567138672\n","[TRAIN] Iter: 138100 Loss: 0.011410341321799399  PSNR: 22.612056732177734\n","[TRAIN] Iter: 138200 Loss: 0.011437929844884152  PSNR: 22.724618911743164\n","[TRAIN] Iter: 138300 Loss: 0.011227321459985153  PSNR: 22.725318908691406\n","[TRAIN] Iter: 138400 Loss: 0.011561013112305656  PSNR: 22.725765228271484\n","[TRAIN] Iter: 138500 Loss: 0.010508078404303522  PSNR: 23.093276977539062\n"," 69% 138599/200000 [10:46:21<4:49:10,  3.54it/s]Shuffle data after an epoch!\n","[TRAIN] Iter: 138600 Loss: 0.01233450042055288  PSNR: 22.427654266357422\n","[TRAIN] Iter: 138700 Loss: 0.010572557315631718  PSNR: 23.14289093017578\n","[TRAIN] Iter: 138800 Loss: 0.01177869561225265  PSNR: 22.636369705200195\n","[TRAIN] Iter: 138900 Loss: 0.011119468343012903  PSNR: 22.831186294555664\n","[TRAIN] Iter: 139000 Loss: 0.01198090193086086  PSNR: 22.499725341796875\n","[TRAIN] Iter: 139100 Loss: 0.01097212183629196  PSNR: 22.821014404296875\n","[TRAIN] Iter: 139200 Loss: 0.011642318456046644  PSNR: 22.64326286315918\n","[TRAIN] Iter: 139300 Loss: 0.01161867210740304  PSNR: 22.667198181152344\n","[TRAIN] Iter: 139400 Loss: 0.01151843573847423  PSNR: 22.710325241088867\n","[TRAIN] Iter: 139500 Loss: 0.011228844146037125  PSNR: 22.863927841186523\n","[TRAIN] Iter: 139600 Loss: 0.011366076702336246  PSNR: 22.72077751159668\n","[TRAIN] Iter: 139700 Loss: 0.011158461939156943  PSNR: 22.68519401550293\n","[TRAIN] Iter: 139800 Loss: 0.010674210108661871  PSNR: 23.112226486206055\n","[TRAIN] Iter: 139900 Loss: 0.011298682118837586  PSNR: 22.724220275878906\n"," 70% 139999/200000 [10:53:07<4:39:49,  3.57it/s]Saved checkpoints at ./logs/56leonard_test_st0/140000.tar\n","[TRAIN] Iter: 140000 Loss: 0.010757097705793788  PSNR: 22.952329635620117\n","[TRAIN] Iter: 140100 Loss: 0.010412413177545408  PSNR: 23.083778381347656\n","[TRAIN] Iter: 140200 Loss: 0.010576913932932584  PSNR: 23.01957130432129\n","[TRAIN] Iter: 140300 Loss: 0.010659520055952571  PSNR: 23.008432388305664\n","[TRAIN] Iter: 140400 Loss: 0.011461764581883084  PSNR: 22.67647933959961\n","[TRAIN] Iter: 140500 Loss: 0.011164303729840432  PSNR: 22.81110191345215\n","[TRAIN] Iter: 140600 Loss: 0.011680889644224576  PSNR: 22.61819076538086\n","[TRAIN] Iter: 140700 Loss: 0.01130055552677963  PSNR: 22.729665756225586\n","[TRAIN] Iter: 140800 Loss: 0.010455569840322502  PSNR: 23.096555709838867\n","[TRAIN] Iter: 140900 Loss: 0.010337189351545605  PSNR: 23.106895446777344\n","[TRAIN] Iter: 141000 Loss: 0.011053474223674614  PSNR: 22.8255615234375\n","[TRAIN] Iter: 141100 Loss: 0.012336807436030392  PSNR: 22.33545684814453\n","[TRAIN] Iter: 141200 Loss: 0.01100065110276931  PSNR: 22.880155563354492\n","[TRAIN] Iter: 141300 Loss: 0.010733549588804868  PSNR: 23.025285720825195\n","[TRAIN] Iter: 141400 Loss: 0.01152551478504778  PSNR: 22.62496566772461\n","[TRAIN] Iter: 141500 Loss: 0.010841750648785875  PSNR: 22.94424819946289\n","[TRAIN] Iter: 141600 Loss: 0.0120075667857506  PSNR: 22.523357391357422\n","[TRAIN] Iter: 141700 Loss: 0.011282380806121223  PSNR: 22.705028533935547\n","[TRAIN] Iter: 141800 Loss: 0.011155620006186206  PSNR: 22.81372833251953\n","[TRAIN] Iter: 141900 Loss: 0.011028468552180683  PSNR: 22.947772979736328\n","[TRAIN] Iter: 142000 Loss: 0.011752600048877754  PSNR: 22.649681091308594\n","[TRAIN] Iter: 142100 Loss: 0.011500995905232654  PSNR: 22.691408157348633\n","[TRAIN] Iter: 142200 Loss: 0.010044471391097029  PSNR: 23.31938362121582\n","[TRAIN] Iter: 142300 Loss: 0.01183080585191985  PSNR: 22.58587646484375\n","[TRAIN] Iter: 142400 Loss: 0.011090634080866479  PSNR: 22.82263946533203\n","[TRAIN] Iter: 142500 Loss: 0.011177458201597453  PSNR: 22.749452590942383\n","[TRAIN] Iter: 142600 Loss: 0.011375658072033742  PSNR: 22.687524795532227\n","[TRAIN] Iter: 142700 Loss: 0.010457682291252829  PSNR: 23.113313674926758\n","[TRAIN] Iter: 142800 Loss: 0.010630027977535225  PSNR: 23.087366104125977\n","[TRAIN] Iter: 142900 Loss: 0.011017019829538946  PSNR: 22.933368682861328\n","[TRAIN] Iter: 143000 Loss: 0.011609928173366597  PSNR: 22.6905517578125\n","[TRAIN] Iter: 143100 Loss: 0.011084296238730142  PSNR: 22.810701370239258\n","[TRAIN] Iter: 143200 Loss: 0.01248494022678379  PSNR: 22.28078842163086\n","[TRAIN] Iter: 143300 Loss: 0.010681509001922884  PSNR: 23.029888153076172\n","[TRAIN] Iter: 143400 Loss: 0.01145891821352446  PSNR: 22.709449768066406\n","[TRAIN] Iter: 143500 Loss: 0.010360191758014734  PSNR: 23.226408004760742\n","[TRAIN] Iter: 143600 Loss: 0.011098584363104357  PSNR: 22.888492584228516\n","[TRAIN] Iter: 143700 Loss: 0.011431249264259463  PSNR: 22.73689842224121\n","[TRAIN] Iter: 143800 Loss: 0.01091226115205592  PSNR: 22.846214294433594\n","[TRAIN] Iter: 143900 Loss: 0.010964012539030293  PSNR: 22.83088493347168\n","[TRAIN] Iter: 144000 Loss: 0.011887085456816781  PSNR: 22.43964195251465\n","[TRAIN] Iter: 144100 Loss: 0.010992222666025817  PSNR: 22.885129928588867\n","[TRAIN] Iter: 144200 Loss: 0.011012529182331305  PSNR: 22.902841567993164\n","[TRAIN] Iter: 144300 Loss: 0.011873034591806178  PSNR: 22.51283836364746\n","[TRAIN] Iter: 144400 Loss: 0.010816458432549655  PSNR: 22.845531463623047\n","[TRAIN] Iter: 144500 Loss: 0.011252530087115029  PSNR: 22.803226470947266\n","[TRAIN] Iter: 144600 Loss: 0.01174325147739675  PSNR: 22.568750381469727\n","[TRAIN] Iter: 144700 Loss: 0.011671949167924285  PSNR: 22.517885208129883\n","[TRAIN] Iter: 144800 Loss: 0.010554605185352636  PSNR: 22.93914031982422\n","[TRAIN] Iter: 144900 Loss: 0.010200208500001222  PSNR: 23.227115631103516\n","[TRAIN] Iter: 145000 Loss: 0.011901145073065549  PSNR: 22.523923873901367\n","[TRAIN] Iter: 145100 Loss: 0.011007221624832537  PSNR: 22.835893630981445\n","[TRAIN] Iter: 145200 Loss: 0.011779045844663548  PSNR: 22.454143524169922\n","[TRAIN] Iter: 145300 Loss: 0.010773116049584142  PSNR: 22.944644927978516\n","[TRAIN] Iter: 145400 Loss: 0.01188456882563549  PSNR: 22.556427001953125\n","[TRAIN] Iter: 145500 Loss: 0.01119173877007635  PSNR: 22.78069496154785\n","[TRAIN] Iter: 145600 Loss: 0.010825358692893331  PSNR: 23.00236701965332\n","[TRAIN] Iter: 145700 Loss: 0.011924505489160422  PSNR: 22.476049423217773\n","[TRAIN] Iter: 145800 Loss: 0.01168249495288079  PSNR: 22.622901916503906\n","[TRAIN] Iter: 145900 Loss: 0.010573145602155193  PSNR: 22.97743797302246\n","[TRAIN] Iter: 146000 Loss: 0.011043956248982129  PSNR: 22.89412498474121\n","[TRAIN] Iter: 146100 Loss: 0.010688860794341115  PSNR: 22.863582611083984\n","[TRAIN] Iter: 146200 Loss: 0.01195313240073778  PSNR: 22.538955688476562\n","[TRAIN] Iter: 146300 Loss: 0.012169178619883297  PSNR: 22.440994262695312\n","[TRAIN] Iter: 146400 Loss: 0.012033161739482299  PSNR: 22.456628799438477\n","[TRAIN] Iter: 146500 Loss: 0.010749687908524819  PSNR: 22.983381271362305\n","[TRAIN] Iter: 146600 Loss: 0.011592791758683083  PSNR: 22.684307098388672\n","[TRAIN] Iter: 146700 Loss: 0.01110524268769432  PSNR: 22.707841873168945\n","[TRAIN] Iter: 146800 Loss: 0.011504828070146259  PSNR: 22.692766189575195\n","[TRAIN] Iter: 146900 Loss: 0.010523119932754425  PSNR: 23.121232986450195\n","[TRAIN] Iter: 147000 Loss: 0.010389163048276737  PSNR: 23.085460662841797\n","[TRAIN] Iter: 147100 Loss: 0.01181377814412151  PSNR: 22.517562866210938\n","[TRAIN] Iter: 147200 Loss: 0.01241349754017217  PSNR: 22.39185333251953\n","[TRAIN] Iter: 147300 Loss: 0.010501429204008646  PSNR: 23.090761184692383\n","[TRAIN] Iter: 147400 Loss: 0.011097184604485699  PSNR: 22.913381576538086\n","[TRAIN] Iter: 147500 Loss: 0.010768910234645839  PSNR: 23.018112182617188\n","[TRAIN] Iter: 147600 Loss: 0.01126806216454798  PSNR: 22.672809600830078\n","[TRAIN] Iter: 147700 Loss: 0.011319659531350612  PSNR: 22.74976348876953\n","[TRAIN] Iter: 147800 Loss: 0.010756710889687307  PSNR: 22.897693634033203\n","[TRAIN] Iter: 147900 Loss: 0.009795355675395266  PSNR: 23.422449111938477\n","[TRAIN] Iter: 148000 Loss: 0.010474493513862843  PSNR: 23.019672393798828\n","[TRAIN] Iter: 148100 Loss: 0.011893602851510789  PSNR: 22.54549217224121\n","[TRAIN] Iter: 148200 Loss: 0.011179250310821098  PSNR: 22.878084182739258\n","[TRAIN] Iter: 148300 Loss: 0.010733976310006076  PSNR: 22.97606658935547\n","[TRAIN] Iter: 148400 Loss: 0.0108319515092015  PSNR: 22.865955352783203\n","[TRAIN] Iter: 148500 Loss: 0.010978357784662046  PSNR: 22.778860092163086\n","[TRAIN] Iter: 148600 Loss: 0.010981285594445524  PSNR: 22.75553321838379\n","[TRAIN] Iter: 148700 Loss: 0.011248114361909709  PSNR: 22.687089920043945\n","[TRAIN] Iter: 148800 Loss: 0.012304096474038868  PSNR: 22.42556381225586\n","[TRAIN] Iter: 148900 Loss: 0.010232589515602804  PSNR: 23.164260864257812\n","[TRAIN] Iter: 149000 Loss: 0.010627121891438825  PSNR: 23.025348663330078\n","[TRAIN] Iter: 149100 Loss: 0.011096592853171064  PSNR: 22.720670700073242\n","[TRAIN] Iter: 149200 Loss: 0.011296858466180568  PSNR: 22.73147201538086\n","[TRAIN] Iter: 149300 Loss: 0.0105353520539847  PSNR: 22.99004554748535\n","[TRAIN] Iter: 149400 Loss: 0.011953077936453425  PSNR: 22.54887580871582\n","[TRAIN] Iter: 149500 Loss: 0.011866297306061263  PSNR: 22.593273162841797\n","[TRAIN] Iter: 149600 Loss: 0.011412272109517079  PSNR: 22.630874633789062\n","[TRAIN] Iter: 149700 Loss: 0.010695241489431056  PSNR: 22.95606231689453\n","[TRAIN] Iter: 149800 Loss: 0.011460211596458748  PSNR: 22.67131996154785\n","[TRAIN] Iter: 149900 Loss: 0.01103816599042744  PSNR: 22.822153091430664\n"," 75% 149999/200000 [11:40:54<3:54:25,  3.55it/s]Saved checkpoints at ./logs/56leonard_test_st0/150000.tar\n","[TRAIN] Iter: 150000 Loss: 0.01068081578523672  PSNR: 22.94921875\n","[TRAIN] Iter: 150100 Loss: 0.010838856868407878  PSNR: 22.9776611328125\n","[TRAIN] Iter: 150200 Loss: 0.011369351064589531  PSNR: 22.726102828979492\n","[TRAIN] Iter: 150300 Loss: 0.011418510055180941  PSNR: 22.687414169311523\n","[TRAIN] Iter: 150400 Loss: 0.01057189518480187  PSNR: 23.008420944213867\n","[TRAIN] Iter: 150500 Loss: 0.01060342736143165  PSNR: 23.105783462524414\n","[TRAIN] Iter: 150600 Loss: 0.011069902173987699  PSNR: 22.940515518188477\n","[TRAIN] Iter: 150700 Loss: 0.01038669568802339  PSNR: 23.158571243286133\n","[TRAIN] Iter: 150800 Loss: 0.010700257349623404  PSNR: 23.006826400756836\n","[TRAIN] Iter: 150900 Loss: 0.01131828427624349  PSNR: 22.755651473999023\n","[TRAIN] Iter: 151000 Loss: 0.011090054933904395  PSNR: 22.859403610229492\n","[TRAIN] Iter: 151100 Loss: 0.010331996942322789  PSNR: 23.118467330932617\n","[TRAIN] Iter: 151200 Loss: 0.01164237621771206  PSNR: 22.566822052001953\n","[TRAIN] Iter: 151300 Loss: 0.010758892509217886  PSNR: 22.962499618530273\n","[TRAIN] Iter: 151400 Loss: 0.01062821671892055  PSNR: 22.97776222229004\n","[TRAIN] Iter: 151500 Loss: 0.011201031928464243  PSNR: 22.762165069580078\n","[TRAIN] Iter: 151600 Loss: 0.010830242256031622  PSNR: 22.94318962097168\n","[TRAIN] Iter: 151700 Loss: 0.011309471734498748  PSNR: 22.75700569152832\n","[TRAIN] Iter: 151800 Loss: 0.01146499293296498  PSNR: 22.659942626953125\n","[TRAIN] Iter: 151900 Loss: 0.011906138127381753  PSNR: 22.434314727783203\n","[TRAIN] Iter: 152000 Loss: 0.011500391057143466  PSNR: 22.730445861816406\n","[TRAIN] Iter: 152100 Loss: 0.010310665474210234  PSNR: 23.186235427856445\n","[TRAIN] Iter: 152200 Loss: 0.011422097915006198  PSNR: 22.59292221069336\n","[TRAIN] Iter: 152300 Loss: 0.011080384768075975  PSNR: 22.851593017578125\n","[TRAIN] Iter: 152400 Loss: 0.011416984597742033  PSNR: 22.710323333740234\n","[TRAIN] Iter: 152500 Loss: 0.012398724838227139  PSNR: 22.356876373291016\n","[TRAIN] Iter: 152600 Loss: 0.011062621901008212  PSNR: 22.862897872924805\n","[TRAIN] Iter: 152700 Loss: 0.01035580629101823  PSNR: 23.207561492919922\n","[TRAIN] Iter: 152800 Loss: 0.010844418801706343  PSNR: 22.96095085144043\n","[TRAIN] Iter: 152900 Loss: 0.011944752342885251  PSNR: 22.49150276184082\n","[TRAIN] Iter: 153000 Loss: 0.010206935941109706  PSNR: 23.22691535949707\n","[TRAIN] Iter: 153100 Loss: 0.010086015860514112  PSNR: 23.22177505493164\n","[TRAIN] Iter: 153200 Loss: 0.011342019079586759  PSNR: 22.734312057495117\n","[TRAIN] Iter: 153300 Loss: 0.011147587241770906  PSNR: 22.899721145629883\n","[TRAIN] Iter: 153400 Loss: 0.011128615567092253  PSNR: 22.787206649780273\n","[TRAIN] Iter: 153500 Loss: 0.01085350602567891  PSNR: 22.9600830078125\n","[TRAIN] Iter: 153600 Loss: 0.010255937265513066  PSNR: 23.226652145385742\n","[TRAIN] Iter: 153700 Loss: 0.010798055493958339  PSNR: 22.902341842651367\n","[TRAIN] Iter: 153800 Loss: 0.011180456356296522  PSNR: 22.829383850097656\n","[TRAIN] Iter: 153900 Loss: 0.01073605868544589  PSNR: 22.99822235107422\n","[TRAIN] Iter: 154000 Loss: 0.011549312496741019  PSNR: 22.659311294555664\n","[TRAIN] Iter: 154100 Loss: 0.011187538349798118  PSNR: 22.75737953186035\n","[TRAIN] Iter: 154200 Loss: 0.010253739595189677  PSNR: 23.16919708251953\n","[TRAIN] Iter: 154300 Loss: 0.011384205054271849  PSNR: 22.653474807739258\n","[TRAIN] Iter: 154400 Loss: 0.01084935262943591  PSNR: 22.932518005371094\n","[TRAIN] Iter: 154500 Loss: 0.011502878095851016  PSNR: 22.657320022583008\n","[TRAIN] Iter: 154600 Loss: 0.0121776284342022  PSNR: 22.394941329956055\n","[TRAIN] Iter: 154700 Loss: 0.010687511924734655  PSNR: 22.992210388183594\n","[TRAIN] Iter: 154800 Loss: 0.01180596044727818  PSNR: 22.439739227294922\n","[TRAIN] Iter: 154900 Loss: 0.01175335803297545  PSNR: 22.520383834838867\n","[TRAIN] Iter: 155000 Loss: 0.011473553197802789  PSNR: 22.693401336669922\n","[TRAIN] Iter: 155100 Loss: 0.011016414626398095  PSNR: 22.811603546142578\n","[TRAIN] Iter: 155200 Loss: 0.0111670703678685  PSNR: 22.880590438842773\n","[TRAIN] Iter: 155300 Loss: 0.011398040415561441  PSNR: 22.729928970336914\n","[TRAIN] Iter: 155400 Loss: 0.010939463026998187  PSNR: 22.958904266357422\n","[TRAIN] Iter: 155500 Loss: 0.009902884396803426  PSNR: 23.283781051635742\n","[TRAIN] Iter: 155600 Loss: 0.010265887884867066  PSNR: 23.15880012512207\n","[TRAIN] Iter: 155700 Loss: 0.011987558311808748  PSNR: 22.472274780273438\n","[TRAIN] Iter: 155800 Loss: 0.011829155846653217  PSNR: 22.590946197509766\n","[TRAIN] Iter: 155900 Loss: 0.011076178474483691  PSNR: 22.8359375\n","[TRAIN] Iter: 156000 Loss: 0.010835431927786927  PSNR: 23.05917739868164\n","[TRAIN] Iter: 156100 Loss: 0.01130744275436775  PSNR: 22.87704086303711\n","[TRAIN] Iter: 156200 Loss: 0.011156218399371191  PSNR: 22.869205474853516\n","[TRAIN] Iter: 156300 Loss: 0.011094379449494651  PSNR: 22.809669494628906\n","[TRAIN] Iter: 156400 Loss: 0.011042812447411347  PSNR: 22.868619918823242\n","[TRAIN] Iter: 156500 Loss: 0.01027288229758171  PSNR: 23.225847244262695\n","[TRAIN] Iter: 156600 Loss: 0.01205144018944663  PSNR: 22.474878311157227\n","[TRAIN] Iter: 156700 Loss: 0.01070584188476754  PSNR: 22.952829360961914\n","[TRAIN] Iter: 156800 Loss: 0.010683109681544317  PSNR: 23.04620361328125\n","[TRAIN] Iter: 156900 Loss: 0.010500138481807886  PSNR: 23.057645797729492\n","[TRAIN] Iter: 157000 Loss: 0.011348976171446017  PSNR: 22.72207260131836\n","[TRAIN] Iter: 157100 Loss: 0.011051464015872698  PSNR: 22.861730575561523\n","[TRAIN] Iter: 157200 Loss: 0.010824281582359017  PSNR: 22.9295597076416\n","[TRAIN] Iter: 157300 Loss: 0.01108361075349697  PSNR: 22.850566864013672\n","[TRAIN] Iter: 157400 Loss: 0.011684609460721591  PSNR: 22.626094818115234\n","[TRAIN] Iter: 157500 Loss: 0.01115713263247552  PSNR: 22.909244537353516\n","[TRAIN] Iter: 157600 Loss: 0.011331442427078078  PSNR: 22.75385284423828\n","[TRAIN] Iter: 157700 Loss: 0.009848588870132168  PSNR: 23.357210159301758\n","[TRAIN] Iter: 157800 Loss: 0.010420652151858717  PSNR: 23.10948371887207\n","[TRAIN] Iter: 157900 Loss: 0.010156040925196862  PSNR: 23.266220092773438\n","[TRAIN] Iter: 158000 Loss: 0.010833397904073786  PSNR: 22.929595947265625\n","[TRAIN] Iter: 158100 Loss: 0.01037297532220156  PSNR: 23.108112335205078\n","[TRAIN] Iter: 158200 Loss: 0.0103432140043741  PSNR: 23.183677673339844\n","[TRAIN] Iter: 158300 Loss: 0.010679033974965519  PSNR: 22.96754264831543\n"," 79% 158399/200000 [12:21:02<3:20:45,  3.45it/s]Shuffle data after an epoch!\n","[TRAIN] Iter: 158400 Loss: 0.011565824089334934  PSNR: 22.65384864807129\n","[TRAIN] Iter: 158500 Loss: 0.011040009981446487  PSNR: 22.83855628967285\n","[TRAIN] Iter: 158600 Loss: 0.011420013897457088  PSNR: 22.736454010009766\n","[TRAIN] Iter: 158700 Loss: 0.011256264929954459  PSNR: 22.783510208129883\n","[TRAIN] Iter: 158800 Loss: 0.010659621500604599  PSNR: 23.003374099731445\n","[TRAIN] Iter: 158900 Loss: 0.011273388609343599  PSNR: 22.75235366821289\n","[TRAIN] Iter: 159000 Loss: 0.01175831082692125  PSNR: 22.499004364013672\n","[TRAIN] Iter: 159100 Loss: 0.01097879474630339  PSNR: 22.830204010009766\n","[TRAIN] Iter: 159200 Loss: 0.011152409552953345  PSNR: 22.7639217376709\n","[TRAIN] Iter: 159300 Loss: 0.010269226170713705  PSNR: 23.18485450744629\n","[TRAIN] Iter: 159400 Loss: 0.01010713704139855  PSNR: 23.25741958618164\n","[TRAIN] Iter: 159500 Loss: 0.011222174087730857  PSNR: 22.784147262573242\n","[TRAIN] Iter: 159600 Loss: 0.01164691866423242  PSNR: 22.56694221496582\n","[TRAIN] Iter: 159700 Loss: 0.01201595136496195  PSNR: 22.470134735107422\n","[TRAIN] Iter: 159800 Loss: 0.010857312805482102  PSNR: 22.879640579223633\n","[TRAIN] Iter: 159900 Loss: 0.011158387087303122  PSNR: 22.80187225341797\n"," 80% 159999/200000 [12:28:47<3:09:40,  3.51it/s]Saved checkpoints at ./logs/56leonard_test_st0/160000.tar\n","[TRAIN] Iter: 160000 Loss: 0.012467879201492619  PSNR: 22.251859664916992\n","[TRAIN] Iter: 160100 Loss: 0.011527935776662755  PSNR: 22.685094833374023\n","[TRAIN] Iter: 160200 Loss: 0.01125435367255204  PSNR: 22.731687545776367\n","[TRAIN] Iter: 160300 Loss: 0.010129715481257635  PSNR: 23.157705307006836\n","[TRAIN] Iter: 160400 Loss: 0.011187277738011502  PSNR: 22.768327713012695\n","[TRAIN] Iter: 160500 Loss: 0.010544625123216128  PSNR: 23.12862777709961\n","[TRAIN] Iter: 160600 Loss: 0.011178593620854953  PSNR: 22.768630981445312\n","[TRAIN] Iter: 160700 Loss: 0.011221562390349684  PSNR: 22.872867584228516\n","[TRAIN] Iter: 160800 Loss: 0.010823031929419122  PSNR: 22.980131149291992\n","[TRAIN] Iter: 160900 Loss: 0.010766818391168066  PSNR: 22.971336364746094\n","[TRAIN] Iter: 161000 Loss: 0.01066926040531311  PSNR: 23.045452117919922\n","[TRAIN] Iter: 161100 Loss: 0.010959611477159986  PSNR: 22.94719886779785\n","[TRAIN] Iter: 161200 Loss: 0.011683024775446221  PSNR: 22.58091926574707\n","[TRAIN] Iter: 161300 Loss: 0.010634529344397604  PSNR: 22.962772369384766\n","[TRAIN] Iter: 161400 Loss: 0.011520089302367913  PSNR: 22.593067169189453\n","[TRAIN] Iter: 161500 Loss: 0.011215000001190648  PSNR: 22.759550094604492\n","[TRAIN] Iter: 161600 Loss: 0.011498811260323701  PSNR: 22.66225242614746\n","[TRAIN] Iter: 161700 Loss: 0.011146036615927252  PSNR: 22.740476608276367\n","[TRAIN] Iter: 161800 Loss: 0.011150039548478083  PSNR: 22.68064308166504\n","[TRAIN] Iter: 161900 Loss: 0.010829119379126392  PSNR: 22.91191864013672\n","[TRAIN] Iter: 162000 Loss: 0.011084358178227116  PSNR: 22.85908317565918\n","[TRAIN] Iter: 162100 Loss: 0.010192791302734691  PSNR: 23.29738998413086\n","[TRAIN] Iter: 162200 Loss: 0.010797955109135901  PSNR: 22.95857048034668\n","[TRAIN] Iter: 162300 Loss: 0.010556794692248395  PSNR: 23.054048538208008\n","[TRAIN] Iter: 162400 Loss: 0.010924768324285794  PSNR: 22.849456787109375\n","[TRAIN] Iter: 162500 Loss: 0.011291564179685083  PSNR: 22.705562591552734\n","[TRAIN] Iter: 162600 Loss: 0.010287300283155157  PSNR: 23.216297149658203\n","[TRAIN] Iter: 162700 Loss: 0.010159016518689121  PSNR: 23.24615478515625\n","[TRAIN] Iter: 162800 Loss: 0.011209954140500055  PSNR: 22.744611740112305\n","[TRAIN] Iter: 162900 Loss: 0.010356892962083083  PSNR: 23.09940528869629\n","[TRAIN] Iter: 163000 Loss: 0.01058307060423943  PSNR: 23.017892837524414\n","[TRAIN] Iter: 163100 Loss: 0.010795064946898338  PSNR: 22.950952529907227\n","[TRAIN] Iter: 163200 Loss: 0.010437860775652938  PSNR: 23.080163955688477\n","[TRAIN] Iter: 163300 Loss: 0.012213910873829687  PSNR: 22.327899932861328\n","[TRAIN] Iter: 163400 Loss: 0.011535882076091216  PSNR: 22.5943603515625\n","[TRAIN] Iter: 163500 Loss: 0.011816917403571717  PSNR: 22.513023376464844\n","[TRAIN] Iter: 163600 Loss: 0.011402848484756561  PSNR: 22.691925048828125\n","[TRAIN] Iter: 163700 Loss: 0.01107394787405984  PSNR: 22.849401473999023\n","[TRAIN] Iter: 163800 Loss: 0.011602299938999668  PSNR: 22.702491760253906\n","[TRAIN] Iter: 163900 Loss: 0.011471606031196534  PSNR: 22.662761688232422\n","[TRAIN] Iter: 164000 Loss: 0.0104031947708896  PSNR: 23.11636734008789\n","[TRAIN] Iter: 164100 Loss: 0.010798744302573587  PSNR: 22.969606399536133\n","[TRAIN] Iter: 164200 Loss: 0.011041019212322978  PSNR: 22.86215591430664\n","[TRAIN] Iter: 164300 Loss: 0.010337621438044078  PSNR: 23.07851791381836\n","[TRAIN] Iter: 164400 Loss: 0.011984534303732041  PSNR: 22.55539321899414\n","[TRAIN] Iter: 164500 Loss: 0.01221412739572418  PSNR: 22.3707332611084\n","[TRAIN] Iter: 164600 Loss: 0.011612376008386771  PSNR: 22.645748138427734\n","[TRAIN] Iter: 164700 Loss: 0.011461521546163766  PSNR: 22.771299362182617\n","[TRAIN] Iter: 164800 Loss: 0.011996353979068466  PSNR: 22.589244842529297\n","[TRAIN] Iter: 164900 Loss: 0.010856658355861411  PSNR: 22.933237075805664\n","[TRAIN] Iter: 165000 Loss: 0.012074420190578614  PSNR: 22.417879104614258\n","[TRAIN] Iter: 165100 Loss: 0.010091228547483577  PSNR: 23.289533615112305\n","[TRAIN] Iter: 165200 Loss: 0.010922649497811888  PSNR: 22.8839054107666\n","[TRAIN] Iter: 165300 Loss: 0.011616581962960023  PSNR: 22.583702087402344\n","[TRAIN] Iter: 165400 Loss: 0.010600627559571175  PSNR: 22.925844192504883\n","[TRAIN] Iter: 165500 Loss: 0.010044095422709209  PSNR: 23.1903076171875\n","[TRAIN] Iter: 165600 Loss: 0.011586741361086311  PSNR: 22.594085693359375\n","[TRAIN] Iter: 165700 Loss: 0.012000876655052218  PSNR: 22.554405212402344\n","[TRAIN] Iter: 165800 Loss: 0.011508662844561776  PSNR: 22.691360473632812\n","[TRAIN] Iter: 165900 Loss: 0.01106126417997317  PSNR: 22.912338256835938\n","[TRAIN] Iter: 166000 Loss: 0.01107704534950247  PSNR: 22.792163848876953\n","[TRAIN] Iter: 166100 Loss: 0.011945379301104722  PSNR: 22.480791091918945\n","[TRAIN] Iter: 166200 Loss: 0.01088348262344798  PSNR: 22.925893783569336\n","[TRAIN] Iter: 166300 Loss: 0.010331924711807768  PSNR: 23.190824508666992\n","[TRAIN] Iter: 166400 Loss: 0.010259627532644176  PSNR: 23.160846710205078\n","[TRAIN] Iter: 166500 Loss: 0.011401658397455626  PSNR: 22.6934757232666\n","[TRAIN] Iter: 166600 Loss: 0.011572005563343084  PSNR: 22.616352081298828\n","[TRAIN] Iter: 166700 Loss: 0.011649067676872737  PSNR: 22.674190521240234\n","[TRAIN] Iter: 166800 Loss: 0.010794692712641218  PSNR: 22.958663940429688\n","[TRAIN] Iter: 166900 Loss: 0.011178632082904273  PSNR: 22.81940460205078\n","[TRAIN] Iter: 167000 Loss: 0.010991368305830406  PSNR: 22.9311580657959\n","[TRAIN] Iter: 167100 Loss: 0.011048246432160777  PSNR: 22.820783615112305\n","[TRAIN] Iter: 167200 Loss: 0.011555197612288818  PSNR: 22.6712646484375\n","[TRAIN] Iter: 167300 Loss: 0.010490355737254578  PSNR: 23.040525436401367\n","[TRAIN] Iter: 167400 Loss: 0.011083073490572868  PSNR: 22.830392837524414\n","[TRAIN] Iter: 167500 Loss: 0.010061786695556074  PSNR: 23.219850540161133\n","[TRAIN] Iter: 167600 Loss: 0.010438361990947435  PSNR: 23.147750854492188\n","[TRAIN] Iter: 167700 Loss: 0.01030401382249944  PSNR: 23.194469451904297\n","[TRAIN] Iter: 167800 Loss: 0.010562161775094482  PSNR: 23.033842086791992\n","[TRAIN] Iter: 167900 Loss: 0.01059214136308664  PSNR: 23.02828598022461\n","[TRAIN] Iter: 168000 Loss: 0.010173466202907511  PSNR: 23.282808303833008\n","[TRAIN] Iter: 168100 Loss: 0.010678390856131599  PSNR: 22.964818954467773\n","[TRAIN] Iter: 168200 Loss: 0.01137516054384952  PSNR: 22.75419044494629\n","[TRAIN] Iter: 168300 Loss: 0.010776622768884287  PSNR: 22.931255340576172\n","[TRAIN] Iter: 168400 Loss: 0.011399923882632477  PSNR: 22.756017684936523\n","[TRAIN] Iter: 168500 Loss: 0.010877181664429301  PSNR: 22.824071884155273\n","[TRAIN] Iter: 168600 Loss: 0.011710780062245442  PSNR: 22.464786529541016\n","[TRAIN] Iter: 168700 Loss: 0.011078988806734337  PSNR: 22.802780151367188\n","[TRAIN] Iter: 168800 Loss: 0.011099541312736622  PSNR: 22.858116149902344\n","[TRAIN] Iter: 168900 Loss: 0.011050497248084106  PSNR: 22.842987060546875\n","[TRAIN] Iter: 169000 Loss: 0.010719268193664559  PSNR: 22.96670913696289\n","[TRAIN] Iter: 169100 Loss: 0.01139179259024747  PSNR: 22.773645401000977\n","[TRAIN] Iter: 169200 Loss: 0.011190489622108345  PSNR: 22.81885528564453\n","[TRAIN] Iter: 169300 Loss: 0.011040983514375682  PSNR: 22.840084075927734\n","[TRAIN] Iter: 169400 Loss: 0.010888238995419686  PSNR: 22.946950912475586\n","[TRAIN] Iter: 169500 Loss: 0.010723788246491645  PSNR: 22.97606086730957\n","[TRAIN] Iter: 169600 Loss: 0.0112232456921298  PSNR: 22.789857864379883\n","[TRAIN] Iter: 169700 Loss: 0.011518898366731627  PSNR: 22.648962020874023\n","[TRAIN] Iter: 169800 Loss: 0.0114704949448576  PSNR: 22.618213653564453\n","[TRAIN] Iter: 169900 Loss: 0.011604465059155612  PSNR: 22.66536521911621\n"," 85% 169999/200000 [13:16:12<2:25:20,  3.44it/s]Saved checkpoints at ./logs/56leonard_test_st0/170000.tar\n","[TRAIN] Iter: 170000 Loss: 0.01109990964209565  PSNR: 22.8150577545166\n","[TRAIN] Iter: 170100 Loss: 0.011469947875784283  PSNR: 22.657724380493164\n","[TRAIN] Iter: 170200 Loss: 0.010960305341009703  PSNR: 22.924928665161133\n","[TRAIN] Iter: 170300 Loss: 0.011478999753686301  PSNR: 22.76007843017578\n","[TRAIN] Iter: 170400 Loss: 0.010315987146397  PSNR: 23.23011589050293\n","[TRAIN] Iter: 170500 Loss: 0.010363673241646335  PSNR: 23.117900848388672\n","[TRAIN] Iter: 170600 Loss: 0.012004044938780566  PSNR: 22.531003952026367\n","[TRAIN] Iter: 170700 Loss: 0.010466407201743178  PSNR: 23.10100555419922\n","[TRAIN] Iter: 170800 Loss: 0.011111448298741031  PSNR: 22.82902717590332\n","[TRAIN] Iter: 170900 Loss: 0.011716250906185192  PSNR: 22.572917938232422\n","[TRAIN] Iter: 171000 Loss: 0.010360637260170405  PSNR: 23.056344985961914\n","[TRAIN] Iter: 171100 Loss: 0.010840512623890481  PSNR: 22.988672256469727\n","[TRAIN] Iter: 171200 Loss: 0.010676209671967434  PSNR: 22.98505973815918\n","[TRAIN] Iter: 171300 Loss: 0.010040808441716995  PSNR: 23.28580665588379\n","[TRAIN] Iter: 171400 Loss: 0.01077759345739259  PSNR: 22.879186630249023\n","[TRAIN] Iter: 171500 Loss: 0.011379398963187761  PSNR: 22.72016716003418\n","[TRAIN] Iter: 171600 Loss: 0.011436172960530944  PSNR: 22.64449691772461\n","[TRAIN] Iter: 171700 Loss: 0.010764000699043373  PSNR: 22.90888786315918\n","[TRAIN] Iter: 171800 Loss: 0.011050840469543192  PSNR: 22.828737258911133\n","[TRAIN] Iter: 171900 Loss: 0.012114595842079676  PSNR: 22.4390926361084\n","[TRAIN] Iter: 172000 Loss: 0.011193461796523857  PSNR: 22.748722076416016\n","[TRAIN] Iter: 172100 Loss: 0.011503935620625466  PSNR: 22.679059982299805\n","[TRAIN] Iter: 172200 Loss: 0.010771032690644614  PSNR: 23.064804077148438\n","[TRAIN] Iter: 172300 Loss: 0.010744633915782646  PSNR: 23.015241622924805\n","[TRAIN] Iter: 172400 Loss: 0.011056991878968431  PSNR: 22.798912048339844\n","[TRAIN] Iter: 172500 Loss: 0.010942965203044809  PSNR: 22.817869186401367\n","[TRAIN] Iter: 172600 Loss: 0.010709330126738869  PSNR: 22.999780654907227\n","[TRAIN] Iter: 172700 Loss: 0.01075969799439086  PSNR: 23.040132522583008\n","[TRAIN] Iter: 172800 Loss: 0.01044375329761855  PSNR: 23.05537223815918\n","[TRAIN] Iter: 172900 Loss: 0.011098525013383462  PSNR: 22.92011260986328\n","[TRAIN] Iter: 173000 Loss: 0.011406639971835802  PSNR: 22.652177810668945\n","[TRAIN] Iter: 173100 Loss: 0.011926514466416089  PSNR: 22.57817840576172\n","[TRAIN] Iter: 173200 Loss: 0.011518340141714968  PSNR: 22.71600341796875\n","[TRAIN] Iter: 173300 Loss: 0.011425042884064512  PSNR: 22.69721794128418\n","[TRAIN] Iter: 173400 Loss: 0.011201476159828444  PSNR: 22.770631790161133\n","[TRAIN] Iter: 173500 Loss: 0.011611843227319753  PSNR: 22.576398849487305\n","[TRAIN] Iter: 173600 Loss: 0.01014625079999284  PSNR: 23.208972930908203\n","[TRAIN] Iter: 173700 Loss: 0.010791219889435323  PSNR: 22.839391708374023\n","[TRAIN] Iter: 173800 Loss: 0.011388160467905055  PSNR: 22.735395431518555\n","[TRAIN] Iter: 173900 Loss: 0.010220961653169736  PSNR: 23.152511596679688\n","[TRAIN] Iter: 174000 Loss: 0.010740716275192581  PSNR: 22.896709442138672\n","[TRAIN] Iter: 174100 Loss: 0.012390623907644453  PSNR: 22.387935638427734\n","[TRAIN] Iter: 174200 Loss: 0.010961977302528802  PSNR: 22.917390823364258\n","[TRAIN] Iter: 174300 Loss: 0.011021042484192061  PSNR: 22.870237350463867\n","[TRAIN] Iter: 174400 Loss: 0.011123702983663358  PSNR: 22.7871036529541\n","[TRAIN] Iter: 174500 Loss: 0.010376973046163356  PSNR: 23.232595443725586\n","[TRAIN] Iter: 174600 Loss: 0.010095106631751736  PSNR: 23.1380558013916\n","[TRAIN] Iter: 174700 Loss: 0.011209383618964725  PSNR: 22.712499618530273\n","[TRAIN] Iter: 174800 Loss: 0.010455927821645745  PSNR: 23.113506317138672\n","[TRAIN] Iter: 174900 Loss: 0.010368976221422655  PSNR: 23.088808059692383\n","[TRAIN] Iter: 175000 Loss: 0.01094782099379536  PSNR: 22.838233947753906\n","[TRAIN] Iter: 175100 Loss: 0.01155234076165366  PSNR: 22.64967155456543\n","[TRAIN] Iter: 175200 Loss: 0.011455771610477016  PSNR: 22.75131607055664\n","[TRAIN] Iter: 175300 Loss: 0.010934181192702223  PSNR: 22.854808807373047\n","[TRAIN] Iter: 175400 Loss: 0.01104230364268331  PSNR: 22.86104393005371\n","[TRAIN] Iter: 175500 Loss: 0.010428972611801063  PSNR: 23.133136749267578\n","[TRAIN] Iter: 175600 Loss: 0.011053950295671426  PSNR: 22.816730499267578\n","[TRAIN] Iter: 175700 Loss: 0.010933620885710822  PSNR: 22.91000747680664\n","[TRAIN] Iter: 175800 Loss: 0.011403335431808513  PSNR: 22.66389274597168\n","[TRAIN] Iter: 175900 Loss: 0.01075689927091225  PSNR: 22.966629028320312\n","[TRAIN] Iter: 176000 Loss: 0.01083776471241078  PSNR: 23.0887393951416\n","[TRAIN] Iter: 176100 Loss: 0.01129816564954031  PSNR: 22.74637222290039\n","[TRAIN] Iter: 176200 Loss: 0.011335759953233254  PSNR: 22.787580490112305\n","[TRAIN] Iter: 176300 Loss: 0.010717143018924315  PSNR: 22.971437454223633\n","[TRAIN] Iter: 176400 Loss: 0.010411421453405605  PSNR: 23.1436710357666\n","[TRAIN] Iter: 176500 Loss: 0.010409082999806234  PSNR: 23.145652770996094\n","[TRAIN] Iter: 176600 Loss: 0.011165476444506488  PSNR: 22.74725341796875\n","[TRAIN] Iter: 176700 Loss: 0.011670943544997495  PSNR: 22.596874237060547\n","[TRAIN] Iter: 176800 Loss: 0.011470887003653143  PSNR: 22.66872787475586\n","[TRAIN] Iter: 176900 Loss: 0.01037920869074134  PSNR: 23.146242141723633\n","[TRAIN] Iter: 177000 Loss: 0.010698520093781156  PSNR: 22.971376419067383\n","[TRAIN] Iter: 177100 Loss: 0.01042562120512842  PSNR: 23.017717361450195\n","[TRAIN] Iter: 177200 Loss: 0.01099588107622309  PSNR: 22.817174911499023\n","[TRAIN] Iter: 177300 Loss: 0.010819013978865226  PSNR: 22.957490921020508\n","[TRAIN] Iter: 177400 Loss: 0.010842461386636568  PSNR: 22.925460815429688\n","[TRAIN] Iter: 177500 Loss: 0.010758074449307747  PSNR: 23.024478912353516\n","[TRAIN] Iter: 177600 Loss: 0.011041567687968337  PSNR: 22.874412536621094\n","[TRAIN] Iter: 177700 Loss: 0.011017255125038842  PSNR: 22.790498733520508\n","[TRAIN] Iter: 177800 Loss: 0.01164914426087644  PSNR: 22.70609474182129\n","[TRAIN] Iter: 177900 Loss: 0.01050891774013742  PSNR: 23.034589767456055\n","[TRAIN] Iter: 178000 Loss: 0.010375950961937485  PSNR: 23.157453536987305\n","[TRAIN] Iter: 178100 Loss: 0.01100478412059569  PSNR: 22.93500328063965\n"," 89% 178199/200000 [13:55:03<1:43:10,  3.52it/s]Shuffle data after an epoch!\n","[TRAIN] Iter: 178200 Loss: 0.009483970644337156  PSNR: 23.484485626220703\n","[TRAIN] Iter: 178300 Loss: 0.01068001613245538  PSNR: 22.954538345336914\n","[TRAIN] Iter: 178400 Loss: 0.011675632301225407  PSNR: 22.58587646484375\n","[TRAIN] Iter: 178500 Loss: 0.01080732711662006  PSNR: 22.978731155395508\n","[TRAIN] Iter: 178600 Loss: 0.01012051187275501  PSNR: 23.327425003051758\n","[TRAIN] Iter: 178700 Loss: 0.010152104940583025  PSNR: 23.232486724853516\n","[TRAIN] Iter: 178800 Loss: 0.010591307821467572  PSNR: 23.048137664794922\n","[TRAIN] Iter: 178900 Loss: 0.011270676420522228  PSNR: 22.8066463470459\n","[TRAIN] Iter: 179000 Loss: 0.011409764816078756  PSNR: 22.665334701538086\n","[TRAIN] Iter: 179100 Loss: 0.01030267395674819  PSNR: 23.10756492614746\n","[TRAIN] Iter: 179200 Loss: 0.011316006517518355  PSNR: 22.77276611328125\n","[TRAIN] Iter: 179300 Loss: 0.010696769352887749  PSNR: 23.03689193725586\n","[TRAIN] Iter: 179400 Loss: 0.010475216117964186  PSNR: 23.04384994506836\n","[TRAIN] Iter: 179500 Loss: 0.010533825229370443  PSNR: 23.07732391357422\n","[TRAIN] Iter: 179600 Loss: 0.01046416270864927  PSNR: 23.058292388916016\n","[TRAIN] Iter: 179700 Loss: 0.01138800809740656  PSNR: 22.68243408203125\n","[TRAIN] Iter: 179800 Loss: 0.011164532314653489  PSNR: 22.7724609375\n","[TRAIN] Iter: 179900 Loss: 0.011065273819187153  PSNR: 22.905363082885742\n"," 90% 179999/200000 [14:03:40<1:33:53,  3.55it/s]Saved checkpoints at ./logs/56leonard_test_st0/180000.tar\n","[TRAIN] Iter: 180000 Loss: 0.011016517919752876  PSNR: 23.01087188720703\n","[TRAIN] Iter: 180100 Loss: 0.01034378368212881  PSNR: 23.22188949584961\n","[TRAIN] Iter: 180200 Loss: 0.01078735385392512  PSNR: 22.949405670166016\n","[TRAIN] Iter: 180300 Loss: 0.010383955891108083  PSNR: 23.10968017578125\n","[TRAIN] Iter: 180400 Loss: 0.010879533945357168  PSNR: 22.870433807373047\n","[TRAIN] Iter: 180500 Loss: 0.010617041226315103  PSNR: 23.047937393188477\n","[TRAIN] Iter: 180600 Loss: 0.010554702300905128  PSNR: 23.10511016845703\n","[TRAIN] Iter: 180700 Loss: 0.011688356129436053  PSNR: 22.61878204345703\n","[TRAIN] Iter: 180800 Loss: 0.010426295060202952  PSNR: 23.022296905517578\n","[TRAIN] Iter: 180900 Loss: 0.010236589707215107  PSNR: 23.230253219604492\n","[TRAIN] Iter: 181000 Loss: 0.010132704279213034  PSNR: 23.20205307006836\n","[TRAIN] Iter: 181100 Loss: 0.009995537528287936  PSNR: 23.301332473754883\n","[TRAIN] Iter: 181200 Loss: 0.011083436198036228  PSNR: 22.820127487182617\n","[TRAIN] Iter: 181300 Loss: 0.010961952123396864  PSNR: 22.91234588623047\n","[TRAIN] Iter: 181400 Loss: 0.011337994844426878  PSNR: 22.803735733032227\n","[TRAIN] Iter: 181500 Loss: 0.010724847683985373  PSNR: 23.027481079101562\n","[TRAIN] Iter: 181600 Loss: 0.010897878087229949  PSNR: 22.904003143310547\n","[TRAIN] Iter: 181700 Loss: 0.010381723978587117  PSNR: 23.154340744018555\n","[TRAIN] Iter: 181800 Loss: 0.010856601509091873  PSNR: 22.904956817626953\n","[TRAIN] Iter: 181900 Loss: 0.010847161283518657  PSNR: 22.95435905456543\n","[TRAIN] Iter: 182000 Loss: 0.010721436869055419  PSNR: 23.14227294921875\n","[TRAIN] Iter: 182100 Loss: 0.010552142918410183  PSNR: 23.04941177368164\n","[TRAIN] Iter: 182200 Loss: 0.010271230194666556  PSNR: 23.218896865844727\n","[TRAIN] Iter: 182300 Loss: 0.011783072289324473  PSNR: 22.568038940429688\n","[TRAIN] Iter: 182400 Loss: 0.01131192292287498  PSNR: 22.792362213134766\n","[TRAIN] Iter: 182500 Loss: 0.011567822121899694  PSNR: 22.587114334106445\n","[TRAIN] Iter: 182600 Loss: 0.010360195657674244  PSNR: 23.169076919555664\n","[TRAIN] Iter: 182700 Loss: 0.012119401135570028  PSNR: 22.436697006225586\n","[TRAIN] Iter: 182800 Loss: 0.012364608991497961  PSNR: 22.410985946655273\n","[TRAIN] Iter: 182900 Loss: 0.010193315488556214  PSNR: 23.143091201782227\n","[TRAIN] Iter: 183000 Loss: 0.009978182601430026  PSNR: 23.3251895904541\n","[TRAIN] Iter: 183100 Loss: 0.010372112606949195  PSNR: 23.12134552001953\n","[TRAIN] Iter: 183200 Loss: 0.01065033843199076  PSNR: 23.03703498840332\n","[TRAIN] Iter: 183300 Loss: 0.010567439704364582  PSNR: 22.96059799194336\n","[TRAIN] Iter: 183400 Loss: 0.010828447012027034  PSNR: 22.86947250366211\n","[TRAIN] Iter: 183500 Loss: 0.010572943000339327  PSNR: 23.021127700805664\n","[TRAIN] Iter: 183600 Loss: 0.011226448696611172  PSNR: 22.816017150878906\n","[TRAIN] Iter: 183700 Loss: 0.010366041834678165  PSNR: 23.12926483154297\n","[TRAIN] Iter: 183800 Loss: 0.011351437675498031  PSNR: 22.84941291809082\n","[TRAIN] Iter: 183900 Loss: 0.01039400003535063  PSNR: 23.11485481262207\n","[TRAIN] Iter: 184000 Loss: 0.010672451830809847  PSNR: 23.080406188964844\n","[TRAIN] Iter: 184100 Loss: 0.011952712110858372  PSNR: 22.54142951965332\n","[TRAIN] Iter: 184200 Loss: 0.009853838525971836  PSNR: 23.311464309692383\n","[TRAIN] Iter: 184300 Loss: 0.010991932719922371  PSNR: 22.881750106811523\n","[TRAIN] Iter: 184400 Loss: 0.011716727255858182  PSNR: 22.60786247253418\n","[TRAIN] Iter: 184500 Loss: 0.010563700997111593  PSNR: 23.052331924438477\n","[TRAIN] Iter: 184600 Loss: 0.009698953057716298  PSNR: 23.432296752929688\n","[TRAIN] Iter: 184700 Loss: 0.010824516083238391  PSNR: 22.93714714050293\n","[TRAIN] Iter: 184800 Loss: 0.010468572034984646  PSNR: 23.09720230102539\n","[TRAIN] Iter: 184900 Loss: 0.011069947593559216  PSNR: 22.810697555541992\n","[TRAIN] Iter: 185000 Loss: 0.011177063234031314  PSNR: 22.807222366333008\n","[TRAIN] Iter: 185100 Loss: 0.011375844286645658  PSNR: 22.697654724121094\n"," 93% 185181/200000 [14:28:09<1:10:09,  3.52it/s]"]}],"source":["!python run_bungee.py --config 'configs/56Leonard.txt'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q4EgylHszyps"},"outputs":[],"source":["!python run_bungee.py --config 'configs/56Leonard.txt' --render_test"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyMqK4IS0ziZjI4V27UAasxj","gpuType":"A100","machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
